{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'FLIR_groups1and2_train.csv'\n",
    "test_path = 'FLIR_groups1and2_test.csv'\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Round 1:</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 112</th>\n",
       "      <th>Unnamed: 113</th>\n",
       "      <th>Other parameters:</th>\n",
       "      <th>Unnamed: 115</th>\n",
       "      <th>Unnamed: 116</th>\n",
       "      <th>Unnamed: 117</th>\n",
       "      <th>Unnamed: 118</th>\n",
       "      <th>Unnamed: 119</th>\n",
       "      <th>Unnamed: 120</th>\n",
       "      <th>Target:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_CRmax</td>\n",
       "      <td>T_CLmax</td>\n",
       "      <td>T̅_CR</td>\n",
       "      <td>T̅_CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T_Mmax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SubjectID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_offset_1</td>\n",
       "      <td>Max1R13_1</td>\n",
       "      <td>Max1L13_1</td>\n",
       "      <td>aveAllR13_1</td>\n",
       "      <td>aveAllL13_1</td>\n",
       "      <td>T_RC_1</td>\n",
       "      <td>T_RC_Dry_1</td>\n",
       "      <td>T_RC_Wet_1</td>\n",
       "      <td>...</td>\n",
       "      <td>T_OR_Max_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gender</td>\n",
       "      <td>Age</td>\n",
       "      <td>Ethnicity</td>\n",
       "      <td>T_atm</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Distance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aveOralM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161117-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.58</td>\n",
       "      <td>34.98</td>\n",
       "      <td>35.36</td>\n",
       "      <td>34.44</td>\n",
       "      <td>34.85</td>\n",
       "      <td>34.91</td>\n",
       "      <td>34.91</td>\n",
       "      <td>34.6</td>\n",
       "      <td>...</td>\n",
       "      <td>36.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>41-50</td>\n",
       "      <td>White</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161117-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34.71</td>\n",
       "      <td>34.51</td>\n",
       "      <td>34.46</td>\n",
       "      <td>34.24</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.44</td>\n",
       "      <td>...</td>\n",
       "      <td>35.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>31-40</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161117-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85</td>\n",
       "      <td>35.7</td>\n",
       "      <td>35.44</td>\n",
       "      <td>35</td>\n",
       "      <td>34.78</td>\n",
       "      <td>35.67</td>\n",
       "      <td>35.67</td>\n",
       "      <td>35.46</td>\n",
       "      <td>...</td>\n",
       "      <td>36.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>21-30</td>\n",
       "      <td>White</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  Unnamed: 1    Round 1: Unnamed: 3 Unnamed: 4   Unnamed: 5  \\\n",
       "0        NaN         NaN         NaN    T_CRmax    T_CLmax        T̅_CR   \n",
       "1  SubjectID         NaN  T_offset_1  Max1R13_1  Max1L13_1  aveAllR13_1   \n",
       "2   161117-1         NaN        0.58      34.98      35.36        34.44   \n",
       "3   161117-2         NaN        0.83      34.71      34.51        34.46   \n",
       "4   161117-3         NaN        0.85       35.7      35.44           35   \n",
       "\n",
       "    Unnamed: 6 Unnamed: 7  Unnamed: 8  Unnamed: 9  ... Unnamed: 112  \\\n",
       "0        T̅_CL        NaN         NaN         NaN  ...       T_Mmax   \n",
       "1  aveAllL13_1     T_RC_1  T_RC_Dry_1  T_RC_Wet_1  ...   T_OR_Max_4   \n",
       "2        34.85      34.91       34.91        34.6  ...        36.39   \n",
       "3        34.24      34.68       34.68       34.44  ...        35.84   \n",
       "4        34.78      35.67       35.67       35.46  ...         36.4   \n",
       "\n",
       "  Unnamed: 113 Other parameters: Unnamed: 115               Unnamed: 116  \\\n",
       "0          NaN               NaN          NaN                        NaN   \n",
       "1          NaN            Gender          Age                  Ethnicity   \n",
       "2          NaN              Male        41-50                      White   \n",
       "3          NaN            Female        31-40  Black or African-American   \n",
       "4          NaN            Female        21-30                      White   \n",
       "\n",
       "  Unnamed: 117 Unnamed: 118 Unnamed: 119 Unnamed: 120   Target:  \n",
       "0          NaN          NaN          NaN          NaN       NaN  \n",
       "1        T_atm     Humidity     Distance          NaN  aveOralM  \n",
       "2           24           28          0.8          NaN     36.59  \n",
       "3           24           26          0.8          NaN     37.19  \n",
       "4           24           26          0.8          NaN     37.34  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Round 1:</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 112</th>\n",
       "      <th>Unnamed: 113</th>\n",
       "      <th>Other parameters:</th>\n",
       "      <th>Unnamed: 115</th>\n",
       "      <th>Unnamed: 116</th>\n",
       "      <th>Unnamed: 117</th>\n",
       "      <th>Unnamed: 118</th>\n",
       "      <th>Unnamed: 119</th>\n",
       "      <th>Unnamed: 120</th>\n",
       "      <th>Target:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_CRmax</td>\n",
       "      <td>T_CLmax</td>\n",
       "      <td>T̅_CR</td>\n",
       "      <td>T̅_CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T_Mmax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SubjectID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_offset_1</td>\n",
       "      <td>Max1R13_1</td>\n",
       "      <td>Max1L13_1</td>\n",
       "      <td>aveAllR13_1</td>\n",
       "      <td>aveAllL13_1</td>\n",
       "      <td>T_RC_1</td>\n",
       "      <td>T_RC_Dry_1</td>\n",
       "      <td>T_RC_Wet_1</td>\n",
       "      <td>...</td>\n",
       "      <td>T_OR_Max_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gender</td>\n",
       "      <td>Age</td>\n",
       "      <td>Ethnicity</td>\n",
       "      <td>T_atm</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Distance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aveOralM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180208-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.39</td>\n",
       "      <td>35.33</td>\n",
       "      <td>35.07</td>\n",
       "      <td>35.59</td>\n",
       "      <td>35.59</td>\n",
       "      <td>35.44</td>\n",
       "      <td>...</td>\n",
       "      <td>36.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-20</td>\n",
       "      <td>Asian</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180209-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44</td>\n",
       "      <td>36.21</td>\n",
       "      <td>36.27</td>\n",
       "      <td>35.47</td>\n",
       "      <td>35.86</td>\n",
       "      <td>36.19</td>\n",
       "      <td>36.19</td>\n",
       "      <td>35.87</td>\n",
       "      <td>...</td>\n",
       "      <td>37.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180209-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>37.68</td>\n",
       "      <td>37.16</td>\n",
       "      <td>37.06</td>\n",
       "      <td>36.52</td>\n",
       "      <td>37.71</td>\n",
       "      <td>37.6</td>\n",
       "      <td>37.69</td>\n",
       "      <td>...</td>\n",
       "      <td>38.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-20</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  Unnamed: 1    Round 1: Unnamed: 3 Unnamed: 4   Unnamed: 5  \\\n",
       "0        NaN         NaN         NaN    T_CRmax    T_CLmax        T̅_CR   \n",
       "1  SubjectID         NaN  T_offset_1  Max1R13_1  Max1L13_1  aveAllR13_1   \n",
       "2  180208-10         NaN           1      35.62      35.39        35.33   \n",
       "3  180209-01         NaN        0.44      36.21      36.27        35.47   \n",
       "4  180209-02         NaN        0.65      37.68      37.16        37.06   \n",
       "\n",
       "    Unnamed: 6 Unnamed: 7  Unnamed: 8  Unnamed: 9  ... Unnamed: 112  \\\n",
       "0        T̅_CL        NaN         NaN         NaN  ...       T_Mmax   \n",
       "1  aveAllL13_1     T_RC_1  T_RC_Dry_1  T_RC_Wet_1  ...   T_OR_Max_4   \n",
       "2        35.07      35.59       35.59       35.44  ...        36.62   \n",
       "3        35.86      36.19       36.19       35.87  ...        37.21   \n",
       "4        36.52      37.71        37.6       37.69  ...        38.72   \n",
       "\n",
       "  Unnamed: 113 Other parameters: Unnamed: 115 Unnamed: 116 Unnamed: 117  \\\n",
       "0          NaN               NaN          NaN          NaN          NaN   \n",
       "1          NaN            Gender          Age    Ethnicity        T_atm   \n",
       "2          NaN            Female        18-20        Asian           22   \n",
       "3          NaN            Female        18-20        White         24.1   \n",
       "4          NaN              Male        18-20        Asian         24.1   \n",
       "\n",
       "  Unnamed: 118 Unnamed: 119 Unnamed: 120   Target:  \n",
       "0          NaN          NaN          NaN       NaN  \n",
       "1     Humidity     Distance          NaN  aveOralM  \n",
       "2           30          0.6          NaN     36.74  \n",
       "3         15.6         0.62          NaN     37.69  \n",
       "4         15.6         0.62          NaN     39.34  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=712, step=1)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 122)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        1\n",
       "Unnamed: 1      712\n",
       "Round 1:         12\n",
       "Unnamed: 3       11\n",
       "Unnamed: 4       11\n",
       "               ... \n",
       "Unnamed: 117      1\n",
       "Unnamed: 118      1\n",
       "Unnamed: 119      1\n",
       "Unnamed: 120    712\n",
       " Target:          1\n",
       "Length: 122, dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = train_data.isnull().sum()\n",
    "\n",
    "columns_to_drop = null_counts[null_counts == len(train_data)].index\n",
    "data = train_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       1\n",
       "Round 1:        12\n",
       "Unnamed: 3      11\n",
       "Unnamed: 4      11\n",
       "Unnamed: 5      11\n",
       "                ..\n",
       "Unnamed: 116     1\n",
       "Unnamed: 117     1\n",
       "Unnamed: 118     1\n",
       "Unnamed: 119     1\n",
       " Target:         1\n",
       "Length: 116, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Round 1:</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 110</th>\n",
       "      <th>Unnamed: 111</th>\n",
       "      <th>Unnamed: 112</th>\n",
       "      <th>Other parameters:</th>\n",
       "      <th>Unnamed: 115</th>\n",
       "      <th>Unnamed: 116</th>\n",
       "      <th>Unnamed: 117</th>\n",
       "      <th>Unnamed: 118</th>\n",
       "      <th>Unnamed: 119</th>\n",
       "      <th>Target:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>711</td>\n",
       "      <td>700</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>688</td>\n",
       "      <td>689</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>711</td>\n",
       "      <td>165</td>\n",
       "      <td>222</td>\n",
       "      <td>214</td>\n",
       "      <td>272</td>\n",
       "      <td>260</td>\n",
       "      <td>209</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>211</td>\n",
       "      <td>...</td>\n",
       "      <td>210</td>\n",
       "      <td>217</td>\n",
       "      <td>214</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "      <td>319</td>\n",
       "      <td>34</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>SubjectID</td>\n",
       "      <td>0.78</td>\n",
       "      <td>35.39</td>\n",
       "      <td>35.53</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.75</td>\n",
       "      <td>35.39</td>\n",
       "      <td>35.66</td>\n",
       "      <td>35.39</td>\n",
       "      <td>35.39</td>\n",
       "      <td>...</td>\n",
       "      <td>36.76</td>\n",
       "      <td>36.66</td>\n",
       "      <td>36.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>36.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>420</td>\n",
       "      <td>371</td>\n",
       "      <td>354</td>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "      <td>287</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Round 1: Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6  \\\n",
       "count         711      700        701        701        701        701   \n",
       "unique        711      165        222        214        272        260   \n",
       "top     SubjectID     0.78      35.39      35.53      34.68      34.75   \n",
       "freq            1       18         13         14         10         10   \n",
       "\n",
       "       Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10  ... Unnamed: 110  \\\n",
       "count         700        700        700         700  ...          689   \n",
       "unique        209        215        215         211  ...          210   \n",
       "top         35.39      35.66      35.39       35.39  ...        36.76   \n",
       "freq           15         11         12          17  ...           11   \n",
       "\n",
       "       Unnamed: 111 Unnamed: 112 Other parameters: Unnamed: 115 Unnamed: 116  \\\n",
       "count           688          689               711          711          711   \n",
       "unique          217          214                 3            9            7   \n",
       "top           36.66        36.61            Female        18-20        White   \n",
       "freq             13           10               420          371          354   \n",
       "\n",
       "       Unnamed: 117 Unnamed: 118 Unnamed: 119  Target:  \n",
       "count           711          711          711      711  \n",
       "unique           73          319           34       64  \n",
       "top              24           30          0.6    36.89  \n",
       "freq             51           17          287       57  \n",
       "\n",
       "[4 rows x 116 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns.tolist()\n",
    "\n",
    "round2_index = columns.index([col for col in columns if \"Round 2\" in col][0])\n",
    "round_1_columns = columns[:round2_index]\n",
    "constant_features = columns[-7:]\n",
    "\n",
    "df_round_1 = data[round_1_columns + constant_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Round 1:</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 26</th>\n",
       "      <th>Unnamed: 27</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Other parameters:</th>\n",
       "      <th>Unnamed: 115</th>\n",
       "      <th>Unnamed: 116</th>\n",
       "      <th>Unnamed: 117</th>\n",
       "      <th>Unnamed: 118</th>\n",
       "      <th>Unnamed: 119</th>\n",
       "      <th>Target:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_CRmax</td>\n",
       "      <td>T_CLmax</td>\n",
       "      <td>T̅_CR</td>\n",
       "      <td>T̅_CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T_max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_Mmax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SubjectID</td>\n",
       "      <td>T_offset_1</td>\n",
       "      <td>Max1R13_1</td>\n",
       "      <td>Max1L13_1</td>\n",
       "      <td>aveAllR13_1</td>\n",
       "      <td>aveAllL13_1</td>\n",
       "      <td>T_RC_1</td>\n",
       "      <td>T_RC_Dry_1</td>\n",
       "      <td>T_RC_Wet_1</td>\n",
       "      <td>T_RC_Max_1</td>\n",
       "      <td>...</td>\n",
       "      <td>T_Max_1</td>\n",
       "      <td>T_OR_1</td>\n",
       "      <td>T_OR_Max_1</td>\n",
       "      <td>Gender</td>\n",
       "      <td>Age</td>\n",
       "      <td>Ethnicity</td>\n",
       "      <td>T_atm</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>Distance</td>\n",
       "      <td>aveOralM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161117-1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>34.98</td>\n",
       "      <td>35.36</td>\n",
       "      <td>34.44</td>\n",
       "      <td>34.85</td>\n",
       "      <td>34.91</td>\n",
       "      <td>34.91</td>\n",
       "      <td>34.6</td>\n",
       "      <td>34.98</td>\n",
       "      <td>...</td>\n",
       "      <td>35.36</td>\n",
       "      <td>35.19</td>\n",
       "      <td>35.2</td>\n",
       "      <td>Male</td>\n",
       "      <td>41-50</td>\n",
       "      <td>White</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>0.8</td>\n",
       "      <td>36.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161117-2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>34.71</td>\n",
       "      <td>34.51</td>\n",
       "      <td>34.46</td>\n",
       "      <td>34.24</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.68</td>\n",
       "      <td>34.44</td>\n",
       "      <td>34.71</td>\n",
       "      <td>...</td>\n",
       "      <td>34.79</td>\n",
       "      <td>34.5</td>\n",
       "      <td>34.52</td>\n",
       "      <td>Female</td>\n",
       "      <td>31-40</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161117-3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>35.7</td>\n",
       "      <td>35.44</td>\n",
       "      <td>35</td>\n",
       "      <td>34.78</td>\n",
       "      <td>35.67</td>\n",
       "      <td>35.67</td>\n",
       "      <td>35.46</td>\n",
       "      <td>35.7</td>\n",
       "      <td>...</td>\n",
       "      <td>35.7</td>\n",
       "      <td>35.57</td>\n",
       "      <td>35.59</td>\n",
       "      <td>Female</td>\n",
       "      <td>21-30</td>\n",
       "      <td>White</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>37.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>180208-03</td>\n",
       "      <td>0.89</td>\n",
       "      <td>35.68</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.06</td>\n",
       "      <td>35.1</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.62</td>\n",
       "      <td>35.24</td>\n",
       "      <td>35.68</td>\n",
       "      <td>...</td>\n",
       "      <td>35.74</td>\n",
       "      <td>35.29</td>\n",
       "      <td>35.36</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>24.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>36.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>180208-04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>35.58</td>\n",
       "      <td>35.58</td>\n",
       "      <td>35.27</td>\n",
       "      <td>35.28</td>\n",
       "      <td>35.56</td>\n",
       "      <td>35.56</td>\n",
       "      <td>35.47</td>\n",
       "      <td>35.58</td>\n",
       "      <td>...</td>\n",
       "      <td>36.36</td>\n",
       "      <td>36.33</td>\n",
       "      <td>36.36</td>\n",
       "      <td>Female</td>\n",
       "      <td>21-25</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "      <td>37.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>180208-07</td>\n",
       "      <td>0.91</td>\n",
       "      <td>36.82</td>\n",
       "      <td>36.47</td>\n",
       "      <td>36.21</td>\n",
       "      <td>36.15</td>\n",
       "      <td>36.81</td>\n",
       "      <td>36.8</td>\n",
       "      <td>36.75</td>\n",
       "      <td>36.82</td>\n",
       "      <td>...</td>\n",
       "      <td>36.82</td>\n",
       "      <td>36.46</td>\n",
       "      <td>36.52</td>\n",
       "      <td>Male</td>\n",
       "      <td>21-25</td>\n",
       "      <td>Multiracial</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>37.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>180208-08</td>\n",
       "      <td>1.1</td>\n",
       "      <td>36.98</td>\n",
       "      <td>36.96</td>\n",
       "      <td>36.5</td>\n",
       "      <td>36.29</td>\n",
       "      <td>36.94</td>\n",
       "      <td>36.94</td>\n",
       "      <td>36.85</td>\n",
       "      <td>36.98</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>36.15</td>\n",
       "      <td>36.16</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-20</td>\n",
       "      <td>White</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>38.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>180208-09</td>\n",
       "      <td>1.07</td>\n",
       "      <td>37.1</td>\n",
       "      <td>37.19</td>\n",
       "      <td>36.86</td>\n",
       "      <td>37.03</td>\n",
       "      <td>37.35</td>\n",
       "      <td>37.08</td>\n",
       "      <td>37.35</td>\n",
       "      <td>37.4</td>\n",
       "      <td>...</td>\n",
       "      <td>37.4</td>\n",
       "      <td>36</td>\n",
       "      <td>36.05</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-20</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>38.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    Round 1: Unnamed: 3 Unnamed: 4   Unnamed: 5   Unnamed: 6  \\\n",
       "0          NaN         NaN    T_CRmax    T_CLmax        T̅_CR        T̅_CL   \n",
       "1    SubjectID  T_offset_1  Max1R13_1  Max1L13_1  aveAllR13_1  aveAllL13_1   \n",
       "2     161117-1        0.58      34.98      35.36        34.44        34.85   \n",
       "3     161117-2        0.83      34.71      34.51        34.46        34.24   \n",
       "4     161117-3        0.85       35.7      35.44           35        34.78   \n",
       "..         ...         ...        ...        ...          ...          ...   \n",
       "707  180208-03        0.89      35.68      35.62        35.06         35.1   \n",
       "708  180208-04        0.96      35.58      35.58        35.27        35.28   \n",
       "709  180208-07        0.91      36.82      36.47        36.21        36.15   \n",
       "710  180208-08         1.1      36.98      36.96         36.5        36.29   \n",
       "711  180208-09        1.07       37.1      37.19        36.86        37.03   \n",
       "\n",
       "    Unnamed: 7  Unnamed: 8  Unnamed: 9 Unnamed: 10  ... Unnamed: 26  \\\n",
       "0          NaN         NaN         NaN         NaN  ...       T_max   \n",
       "1       T_RC_1  T_RC_Dry_1  T_RC_Wet_1  T_RC_Max_1  ...     T_Max_1   \n",
       "2        34.91       34.91        34.6       34.98  ...       35.36   \n",
       "3        34.68       34.68       34.44       34.71  ...       34.79   \n",
       "4        35.67       35.67       35.46        35.7  ...        35.7   \n",
       "..         ...         ...         ...         ...  ...         ...   \n",
       "707      35.62       35.62       35.24       35.68  ...       35.74   \n",
       "708      35.56       35.56       35.47       35.58  ...       36.36   \n",
       "709      36.81        36.8       36.75       36.82  ...       36.82   \n",
       "710      36.94       36.94       36.85       36.98  ...          37   \n",
       "711      37.35       37.08       37.35        37.4  ...        37.4   \n",
       "\n",
       "    Unnamed: 27 Unnamed: 28 Other parameters: Unnamed: 115  \\\n",
       "0           NaN      T_Mmax               NaN          NaN   \n",
       "1        T_OR_1  T_OR_Max_1            Gender          Age   \n",
       "2         35.19        35.2              Male        41-50   \n",
       "3          34.5       34.52            Female        31-40   \n",
       "4         35.57       35.59            Female        21-30   \n",
       "..          ...         ...               ...          ...   \n",
       "707       35.29       35.36            Female        18-20   \n",
       "708       36.33       36.36            Female        21-25   \n",
       "709       36.46       36.52              Male        21-25   \n",
       "710       36.15       36.16              Male        18-20   \n",
       "711          36       36.05              Male        18-20   \n",
       "\n",
       "                  Unnamed: 116 Unnamed: 117 Unnamed: 118 Unnamed: 119  \\\n",
       "0                          NaN          NaN          NaN          NaN   \n",
       "1                    Ethnicity        T_atm     Humidity     Distance   \n",
       "2                        White           24           28          0.8   \n",
       "3    Black or African-American           24           26          0.8   \n",
       "4                        White           24           26          0.8   \n",
       "..                         ...          ...          ...          ...   \n",
       "707                      White         24.4         13.5          0.6   \n",
       "708                      Asian         24.4         14.7         0.63   \n",
       "709                Multiracial           22           30          0.6   \n",
       "710                      White           22           30          0.6   \n",
       "711  Black or African-American           22           30          0.6   \n",
       "\n",
       "      Target:  \n",
       "0         NaN  \n",
       "1    aveOralM  \n",
       "2       36.59  \n",
       "3       37.19  \n",
       "4       37.34  \n",
       "..        ...  \n",
       "707     36.89  \n",
       "708     37.14  \n",
       "709     37.79  \n",
       "710     38.14  \n",
       "711     38.39  \n",
       "\n",
       "[712 rows x 35 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_round_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_feature_start_index = len(columns) - 7  \n",
    "first_constant_feature = data.columns[0] \n",
    "\n",
    "round_indices = {f\"Round {i+1}\": None for i in range(4)}  \n",
    "\n",
    "for col in columns[1:]:  \n",
    "    for round_name in round_indices.keys():\n",
    "        if round_name in col and round_indices[round_name] is None:\n",
    "            round_indices[round_name] = columns.index(col)\n",
    "            break\n",
    "\n",
    "round_dfs = {}\n",
    "for i in range(1, 5):\n",
    "    round_name = f\"Round {i}\"\n",
    "    if i < 4:\n",
    "        next_round_start = round_indices[f\"Round {i+1}\"]\n",
    "    else:\n",
    "        next_round_start = constant_feature_start_index\n",
    "    round_columns = columns[round_indices[round_name]:next_round_start]\n",
    "    \n",
    "    round_dfs[round_name] = data[[first_constant_feature] + round_columns + columns[-7:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    df_round = df_round.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    new_header = df_round.iloc[0] \n",
    "    df_round = df_round[1:] \n",
    "    df_round.columns = new_header  \n",
    "    df_round.reset_index(drop=True, inplace=True)  \n",
    "    \n",
    "    round_dfs[round_name] = df_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Shape: (710, 35)\n",
      "Columns: ['SubjectID', 'T_offset_1', 'Max1R13_1', 'Max1L13_1', 'aveAllR13_1', 'aveAllL13_1', 'T_RC_1', 'T_RC_Dry_1', 'T_RC_Wet_1', 'T_RC_Max_1', 'T_LC_1', 'T_LC_Dry_1', 'T_LC_Wet_1', 'T_LC_Max_1', 'RCC_1', 'LCC_1', 'canthiMax_1', 'canthi4Max_1', 'T_FHCC_1', 'T_FHRC_1', 'T_FHLC_1', 'T_FHBC_1', 'T_FHTC_1', 'T_FH_Max_1', 'T_FHC_Max_1', 'T_Max_1', 'T_OR_1', 'T_OR_Max_1', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n",
      "Round 2\n",
      "Shape: (710, 35)\n",
      "Columns: ['SubjectID', 'T_offset_2', 'Max1R13_2', 'Max1L13_2', 'aveAllR13_2', 'aveAllL13_2', 'T_RC_2', 'T_RC_Dry_2', 'T_RC_Wet_2', 'T_RC_Max_2', 'T_LC_2', 'T_LC_Dry_2', 'T_LC_Wet_2', 'T_LC_Max_2', 'RCC_2', 'LCC_2', 'canthiMax_2', 'canthi4Max_2', 'T_FHCC_2', 'T_FHRC_2', 'T_FHLC_2', 'T_FHBC_2', 'T_FHTC_2', 'T_FH_Max_2', 'T_FHC_Max_2', 'T_Max_2', 'T_OR_2', 'T_OR_Max_2', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n",
      "Round 3\n",
      "Shape: (710, 35)\n",
      "Columns: ['SubjectID', 'T_offset_3', 'Max1R13_3', 'Max1L13_3', 'aveAllR13_3', 'aveAllL13_3', 'T_RC_3', 'T_RC_Dry_3', 'T_RC_Wet_3', 'T_RC_Max_3', 'T_LC_3', 'T_LC_Dry_3', 'T_LC_Wet_3', 'T_LC_Max_3', 'RCC_3', 'LCC_3', 'canthiMax_3', 'canthi4Max_3', 'T_FHCC_3', 'T_FHRC_3', 'T_FHLC_3', 'T_FHBC_3', 'T_FHTC_3', 'T_FH_Max_3', 'T_FHC_Max_3', 'T_Max_3', 'T_OR_3', 'T_OR_Max_3', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n",
      "Round 4\n",
      "Shape: (710, 35)\n",
      "Columns: ['SubjectID', 'T_offset_4', 'Max1R13_4', 'Max1L13_4', 'aveAllR13_4', 'aveAllL13_4', 'T_RC_4', 'T_RC_Dry_4', 'T_RC_Wet_4', 'T_RC_Max_4', 'T_LC_4', 'T_LC_Dry_4', 'T_LC_Wet_4', 'T_LC_Max_4', 'RCC_4', 'LCC_4', 'canthiMax_4', 'canthi4Max_4', 'T_FHCC_4', 'T_FHRC_4', 'T_FHLC_4', 'T_FHBC_4', 'T_FHTC_4', 'T_FH_Max_4', 'T_FHC_Max_4', 'T_Max_4', 'T_OR_4', 'T_OR_Max_4', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name}\")\n",
    "    print(f\"Shape: {df_round.shape}\")  \n",
    "    print(\"Columns:\", df_round.columns.tolist())  \n",
    "    print(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = 35 \n",
    "column_averages = []\n",
    "\n",
    "for col_index in range(num_columns):\n",
    "    column_values = []\n",
    "    \n",
    "    for df_round in round_dfs.values():\n",
    "        numeric_values = pd.to_numeric(df_round.iloc[:, col_index], errors='coerce') \n",
    "        column_values.extend(numeric_values.dropna().values) \n",
    "\n",
    "    if column_values:\n",
    "        column_averages.append(np.mean(column_values))\n",
    "    else:\n",
    "        column_averages.append(np.nan)\n",
    "\n",
    "for round_name, df_round in round_dfs.items():\n",
    "    for col_index in range(num_columns):\n",
    "        if not np.isnan(column_averages[col_index]):\n",
    "            numeric_values = pd.to_numeric(df_round.iloc[:, col_index], errors='coerce')\n",
    "            df_round.iloc[:, col_index] = numeric_values.fillna(column_averages[col_index], inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "0\n",
      "SubjectID       0\n",
      "T_offset_1      0\n",
      "Max1R13_1       0\n",
      "Max1L13_1       0\n",
      "aveAllR13_1     0\n",
      "aveAllL13_1     0\n",
      "T_RC_1          0\n",
      "T_RC_Dry_1      0\n",
      "T_RC_Wet_1      0\n",
      "T_RC_Max_1      0\n",
      "T_LC_1          0\n",
      "T_LC_Dry_1      0\n",
      "T_LC_Wet_1      0\n",
      "T_LC_Max_1      0\n",
      "RCC_1           0\n",
      "LCC_1           0\n",
      "canthiMax_1     0\n",
      "canthi4Max_1    0\n",
      "T_FHCC_1        0\n",
      "T_FHRC_1        0\n",
      "T_FHLC_1        0\n",
      "T_FHBC_1        0\n",
      "T_FHTC_1        0\n",
      "T_FH_Max_1      0\n",
      "T_FHC_Max_1     0\n",
      "T_Max_1         0\n",
      "T_OR_1          0\n",
      "T_OR_Max_1      0\n",
      "Gender          0\n",
      "Age             0\n",
      "Ethnicity       0\n",
      "T_atm           0\n",
      "Humidity        0\n",
      "Distance        0\n",
      "aveOralM        0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 2 \n",
      "0\n",
      "SubjectID       0\n",
      "T_offset_2      0\n",
      "Max1R13_2       0\n",
      "Max1L13_2       0\n",
      "aveAllR13_2     0\n",
      "aveAllL13_2     0\n",
      "T_RC_2          0\n",
      "T_RC_Dry_2      0\n",
      "T_RC_Wet_2      0\n",
      "T_RC_Max_2      0\n",
      "T_LC_2          0\n",
      "T_LC_Dry_2      0\n",
      "T_LC_Wet_2      0\n",
      "T_LC_Max_2      0\n",
      "RCC_2           0\n",
      "LCC_2           0\n",
      "canthiMax_2     0\n",
      "canthi4Max_2    0\n",
      "T_FHCC_2        0\n",
      "T_FHRC_2        0\n",
      "T_FHLC_2        0\n",
      "T_FHBC_2        0\n",
      "T_FHTC_2        0\n",
      "T_FH_Max_2      0\n",
      "T_FHC_Max_2     0\n",
      "T_Max_2         0\n",
      "T_OR_2          0\n",
      "T_OR_Max_2      0\n",
      "Gender          0\n",
      "Age             0\n",
      "Ethnicity       0\n",
      "T_atm           0\n",
      "Humidity        0\n",
      "Distance        0\n",
      "aveOralM        0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 3 \n",
      "0\n",
      "SubjectID       0\n",
      "T_offset_3      0\n",
      "Max1R13_3       0\n",
      "Max1L13_3       0\n",
      "aveAllR13_3     0\n",
      "aveAllL13_3     0\n",
      "T_RC_3          0\n",
      "T_RC_Dry_3      0\n",
      "T_RC_Wet_3      0\n",
      "T_RC_Max_3      0\n",
      "T_LC_3          0\n",
      "T_LC_Dry_3      0\n",
      "T_LC_Wet_3      0\n",
      "T_LC_Max_3      0\n",
      "RCC_3           0\n",
      "LCC_3           0\n",
      "canthiMax_3     0\n",
      "canthi4Max_3    0\n",
      "T_FHCC_3        0\n",
      "T_FHRC_3        0\n",
      "T_FHLC_3        0\n",
      "T_FHBC_3        0\n",
      "T_FHTC_3        0\n",
      "T_FH_Max_3      0\n",
      "T_FHC_Max_3     0\n",
      "T_Max_3         0\n",
      "T_OR_3          0\n",
      "T_OR_Max_3      0\n",
      "Gender          0\n",
      "Age             0\n",
      "Ethnicity       0\n",
      "T_atm           0\n",
      "Humidity        0\n",
      "Distance        0\n",
      "aveOralM        0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 4 \n",
      "0\n",
      "SubjectID       0\n",
      "T_offset_4      0\n",
      "Max1R13_4       0\n",
      "Max1L13_4       0\n",
      "aveAllR13_4     0\n",
      "aveAllL13_4     0\n",
      "T_RC_4          0\n",
      "T_RC_Dry_4      0\n",
      "T_RC_Wet_4      0\n",
      "T_RC_Max_4      0\n",
      "T_LC_4          0\n",
      "T_LC_Dry_4      0\n",
      "T_LC_Wet_4      0\n",
      "T_LC_Max_4      0\n",
      "RCC_4           0\n",
      "LCC_4           0\n",
      "canthiMax_4     0\n",
      "canthi4Max_4    0\n",
      "T_FHCC_4        0\n",
      "T_FHRC_4        0\n",
      "T_FHLC_4        0\n",
      "T_FHBC_4        0\n",
      "T_FHTC_4        0\n",
      "T_FH_Max_4      0\n",
      "T_FHC_Max_4     0\n",
      "T_Max_4         0\n",
      "T_OR_4          0\n",
      "T_OR_Max_4      0\n",
      "Gender          0\n",
      "Age             0\n",
      "Ethnicity       0\n",
      "T_atm           0\n",
      "Humidity        0\n",
      "Distance        0\n",
      "aveOralM        0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.isnull().sum()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='if_binary')\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    gender_encoded = encoder.fit_transform(df[['Gender']])\n",
    "    \n",
    "    gender_encoded_df = pd.DataFrame(gender_encoded, index=df.index, columns=['Gender_NEW'])\n",
    "    \n",
    "    df.drop('Gender', axis=1, inplace=True)\n",
    "    round_dfs[round_name] = pd.concat([df, gender_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_Max_1 T_OR_1 T_OR_Max_1    Age  \\\n",
      "0      34.91       34.6      34.98  ...   35.36  35.19       35.2  41-50   \n",
      "1      34.68      34.44      34.71  ...   34.79   34.5      34.52  31-40   \n",
      "2      35.67      35.46       35.7  ...    35.7  35.57      35.59  21-30   \n",
      "3      35.14      35.08      35.17  ...   35.71  34.74      34.76  21-30   \n",
      "4       35.3       35.5      35.52  ...   35.55  34.94       35.0  18-20   \n",
      "\n",
      "                   Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0                      White  24.0     28.0      0.8    36.59        1.0  \n",
      "1  Black or African-American  24.0     26.0      0.8    37.19        0.0  \n",
      "2                      White  24.0     26.0      0.8    37.34        0.0  \n",
      "3  Black or African-American  24.0     27.0      0.8    37.09        0.0  \n",
      "4                      White  24.0     27.0      0.8    37.04        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_Max_2 T_OR_2 T_OR_Max_2    Age  \\\n",
      "0      35.02      34.92      35.07  ...   35.54  35.53      35.54  41-50   \n",
      "1      34.67      34.64      34.71  ...   34.93  34.92      34.93  31-40   \n",
      "2      35.63      35.57      35.69  ...    35.8  35.78       35.8  21-30   \n",
      "3      35.15      35.16      35.18  ...    35.7  35.03      35.05  21-30   \n",
      "4      35.34      35.59      35.66  ...   35.67  35.42      35.44  18-20   \n",
      "\n",
      "                   Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0                      White  24.0     28.0      0.8    36.59        1.0  \n",
      "1  Black or African-American  24.0     26.0      0.8    37.19        0.0  \n",
      "2                      White  24.0     26.0      0.8    37.34        0.0  \n",
      "3  Black or African-American  24.0     27.0      0.8    37.09        0.0  \n",
      "4                      White  24.0     27.0      0.8    37.04        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_Max_3 T_OR_3 T_OR_Max_3    Age  \\\n",
      "0      35.01      34.56      35.05  ...   35.48  35.45      35.48  41-50   \n",
      "1      34.51      34.73      34.76  ...   35.14  35.13      35.14  31-40   \n",
      "2      35.68      35.68       35.7  ...   35.75  35.72      35.75  21-30   \n",
      "3      35.12      35.14      35.14  ...   35.77  35.03      35.04  21-30   \n",
      "4      35.59      35.63      35.69  ...   35.72  35.36      35.39  18-20   \n",
      "\n",
      "                   Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0                      White  24.0     28.0      0.8    36.59        1.0  \n",
      "1  Black or African-American  24.0     26.0      0.8    37.19        0.0  \n",
      "2                      White  24.0     26.0      0.8    37.34        0.0  \n",
      "3  Black or African-American  24.0     27.0      0.8    37.09        0.0  \n",
      "4                      White  24.0     27.0      0.8    37.04        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_Max_4 T_OR_4 T_OR_Max_4    Age  \\\n",
      "0       35.0      34.97      35.03  ...   36.39  36.37      36.39  41-50   \n",
      "1      34.67      34.75      34.79  ...   35.84  35.82      35.84  31-40   \n",
      "2      35.69      35.75      35.78  ...    36.4  36.37       36.4  21-30   \n",
      "3      35.39      35.09      35.41  ...    35.7  35.06      35.08  21-30   \n",
      "4      35.67      35.56      35.69  ...   36.64  36.63      36.64  18-20   \n",
      "\n",
      "                   Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0                      White  24.0     28.0      0.8    36.59        1.0  \n",
      "1  Black or African-American  24.0     26.0      0.8    37.19        0.0  \n",
      "2                      White  24.0     26.0      0.8    37.34        0.0  \n",
      "3  Black or African-American  24.0     27.0      0.8    37.09        0.0  \n",
      "4                      White  24.0     27.0      0.8    37.04        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories=[['White', 'Black or African-American', 'Asian', 'Multiracial']], handle_unknown='ignore')\n",
    "\n",
    "ethnicity_mapping = {\n",
    "    'White': 1,\n",
    "    'Black or African-American': 2,\n",
    "    'Asian': 3,\n",
    "    'Multiracial': 4\n",
    "}\n",
    "\n",
    "inverse_ethnicity_mapping = {v: k for k, v in ethnicity_mapping.items()}\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    if 'Ethnicity' in df.columns:\n",
    "\n",
    "        ethnicity_encoded = encoder.fit_transform(df[['Ethnicity']])\n",
    "        \n",
    "        ethnicity_encoded_df = pd.DataFrame(ethnicity_encoded, columns=encoder.get_feature_names_out(['Ethnicity']))\n",
    "        \n",
    "        df['Ethnicity_encoded'] = np.argmax(ethnicity_encoded, axis=1) \n",
    "        \n",
    "        df.drop('Ethnicity', axis=1, inplace=True)\n",
    "    \n",
    "        round_dfs[round_name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_Max_1 T_OR_1 T_OR_Max_1    Age  \\\n",
      "0      34.91       34.6      34.98  ...   35.36  35.19       35.2  41-50   \n",
      "1      34.68      34.44      34.71  ...   34.79   34.5      34.52  31-40   \n",
      "2      35.67      35.46       35.7  ...    35.7  35.57      35.59  21-30   \n",
      "3      35.14      35.08      35.17  ...   35.71  34.74      34.76  21-30   \n",
      "4       35.3       35.5      35.52  ...   35.55  34.94       35.0  18-20   \n",
      "5      34.97      34.85      34.98  ...   35.92  35.89      35.92  21-30   \n",
      "6      35.12      35.05      35.15  ...   35.52  35.14      35.16  21-30   \n",
      "7      34.71       34.8      34.87  ...   34.87  34.09      34.11  18-20   \n",
      "8      35.27       35.3      35.32  ...   35.62  34.63      34.64  18-20   \n",
      "9      35.75      35.36      35.84  ...   35.84  35.09       35.1  18-20   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  24.0     28.0      0.8    36.59        1.0                 0  \n",
      "1  24.0     26.0      0.8    37.19        0.0                 1  \n",
      "2  24.0     26.0      0.8    37.34        0.0                 0  \n",
      "3  24.0     27.0      0.8    37.09        0.0                 1  \n",
      "4  24.0     27.0      0.8    37.04        1.0                 0  \n",
      "5  24.0     26.0      0.8    36.99        0.0                 0  \n",
      "6  24.0     31.0      0.8    36.59        1.0                 0  \n",
      "7  25.0     30.0      0.8    36.49        0.0                 0  \n",
      "8  25.0     30.0      0.8    36.59        0.0                 2  \n",
      "9  26.0     31.0      0.8    36.89        1.0                 3  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_Max_2 T_OR_2 T_OR_Max_2    Age  \\\n",
      "0      35.02      34.92      35.07  ...   35.54  35.53      35.54  41-50   \n",
      "1      34.67      34.64      34.71  ...   34.93  34.92      34.93  31-40   \n",
      "2      35.63      35.57      35.69  ...    35.8  35.78       35.8  21-30   \n",
      "3      35.15      35.16      35.18  ...    35.7  35.03      35.05  21-30   \n",
      "4      35.34      35.59      35.66  ...   35.67  35.42      35.44  18-20   \n",
      "5      35.15      35.17      35.21  ...   35.99  35.97      35.99  21-30   \n",
      "6      35.17      35.28      35.29  ...   35.67   35.6      35.65  21-30   \n",
      "7      35.13      34.98      35.22  ...   35.73  35.67      35.73  18-20   \n",
      "8      35.34      35.38      35.43  ...   35.65  35.26      35.33  18-20   \n",
      "9      35.84      35.76      35.85  ...   36.36  36.34      36.36  18-20   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  24.0     28.0      0.8    36.59        1.0                 0  \n",
      "1  24.0     26.0      0.8    37.19        0.0                 1  \n",
      "2  24.0     26.0      0.8    37.34        0.0                 0  \n",
      "3  24.0     27.0      0.8    37.09        0.0                 1  \n",
      "4  24.0     27.0      0.8    37.04        1.0                 0  \n",
      "5  24.0     26.0      0.8    36.99        0.0                 0  \n",
      "6  24.0     31.0      0.8    36.59        1.0                 0  \n",
      "7  25.0     30.0      0.8    36.49        0.0                 0  \n",
      "8  25.0     30.0      0.8    36.59        0.0                 2  \n",
      "9  26.0     31.0      0.8    36.89        1.0                 3  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_Max_3 T_OR_3 T_OR_Max_3    Age  \\\n",
      "0      35.01      34.56      35.05  ...   35.48  35.45      35.48  41-50   \n",
      "1      34.51      34.73      34.76  ...   35.14  35.13      35.14  31-40   \n",
      "2      35.68      35.68       35.7  ...   35.75  35.72      35.75  21-30   \n",
      "3      35.12      35.14      35.14  ...   35.77  35.03      35.04  21-30   \n",
      "4      35.59      35.63      35.69  ...   35.72  35.36      35.39  18-20   \n",
      "5      35.14      35.03      35.17  ...   35.76  35.73      35.76  21-30   \n",
      "6      35.34       35.2      35.38  ...   35.64  35.43       35.5  21-30   \n",
      "7      35.28      35.04      35.35  ...   35.71  35.67      35.71  18-20   \n",
      "8      35.21      35.32      35.33  ...   35.76  35.17      35.18  18-20   \n",
      "9      35.87       35.6      35.91  ...   36.21  36.14      36.21  18-20   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  24.0     28.0      0.8    36.59        1.0                 0  \n",
      "1  24.0     26.0      0.8    37.19        0.0                 1  \n",
      "2  24.0     26.0      0.8    37.34        0.0                 0  \n",
      "3  24.0     27.0      0.8    37.09        0.0                 1  \n",
      "4  24.0     27.0      0.8    37.04        1.0                 0  \n",
      "5  24.0     26.0      0.8    36.99        0.0                 0  \n",
      "6  24.0     31.0      0.8    36.59        1.0                 0  \n",
      "7  25.0     30.0      0.8    36.49        0.0                 0  \n",
      "8  25.0     30.0      0.8    36.59        0.0                 2  \n",
      "9  26.0     31.0      0.8    36.89        1.0                 3  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_Max_4 T_OR_4 T_OR_Max_4    Age  \\\n",
      "0       35.0      34.97      35.03  ...   36.39  36.37      36.39  41-50   \n",
      "1      34.67      34.75      34.79  ...   35.84  35.82      35.84  31-40   \n",
      "2      35.69      35.75      35.78  ...    36.4  36.37       36.4  21-30   \n",
      "3      35.39      35.09      35.41  ...    35.7  35.06      35.08  21-30   \n",
      "4      35.67      35.56      35.69  ...   36.64  36.63      36.64  18-20   \n",
      "5      35.23      35.15      35.23  ...   35.73  35.68      35.73  21-30   \n",
      "6      35.37      35.31       35.4  ...   36.52  36.49      36.52  21-30   \n",
      "7      35.18      34.94      35.21  ...   36.41  36.34      36.41  18-20   \n",
      "8      35.28      35.36      35.38  ...   36.38  36.37      36.38  18-20   \n",
      "9      35.88       35.7      35.96  ...   36.41  36.33      36.41  18-20   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  24.0     28.0      0.8    36.59        1.0                 0  \n",
      "1  24.0     26.0      0.8    37.19        0.0                 1  \n",
      "2  24.0     26.0      0.8    37.34        0.0                 0  \n",
      "3  24.0     27.0      0.8    37.09        0.0                 1  \n",
      "4  24.0     27.0      0.8    37.04        1.0                 0  \n",
      "5  24.0     26.0      0.8    36.99        0.0                 0  \n",
      "6  24.0     31.0      0.8    36.59        1.0                 0  \n",
      "7  25.0     30.0      0.8    36.49        0.0                 0  \n",
      "8  25.0     30.0      0.8    36.59        0.0                 2  \n",
      "9  26.0     31.0      0.8    36.89        1.0                 3  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "\n",
    "for round_name, df_round in round_dfs.items():\n",
    "    if 'Age' in df_round.columns:\n",
    "    \n",
    "        age_reshaped = df_round['Age'].values.reshape(-1, 1)\n",
    "        age_encoded = encoder.fit_transform(age_reshaped)\n",
    "        \n",
    "        df_round['Age_encoded'] = np.argmax(age_encoded, axis=1) + 1\n",
    "        \n",
    "        df_round.drop('Age', axis=1, inplace=True)\n",
    "        \n",
    "        df_round.rename(columns={'Age_encoded': 'Age'}, inplace=True)\n",
    "\n",
    "        round_dfs[round_name] = df_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_Max_1 T_OR_1 T_OR_Max_1 T_atm  \\\n",
      "0      34.91       34.6      34.98  ...   35.36  35.19       35.2  24.0   \n",
      "1      34.68      34.44      34.71  ...   34.79   34.5      34.52  24.0   \n",
      "2      35.67      35.46       35.7  ...    35.7  35.57      35.59  24.0   \n",
      "3      35.14      35.08      35.17  ...   35.71  34.74      34.76  24.0   \n",
      "4       35.3       35.5      35.52  ...   35.55  34.94       35.0  24.0   \n",
      "5      34.97      34.85      34.98  ...   35.92  35.89      35.92  24.0   \n",
      "6      35.12      35.05      35.15  ...   35.52  35.14      35.16  24.0   \n",
      "7      34.71       34.8      34.87  ...   34.87  34.09      34.11  25.0   \n",
      "8      35.27       35.3      35.32  ...   35.62  34.63      34.64  25.0   \n",
      "9      35.75      35.36      35.84  ...   35.84  35.09       35.1  26.0   \n",
      "\n",
      "  Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0     28.0      0.8    36.59        1.0                 0   6  \n",
      "1     26.0      0.8    37.19        0.0                 1   5  \n",
      "2     26.0      0.8    37.34        0.0                 0   3  \n",
      "3     27.0      0.8    37.09        0.0                 1   3  \n",
      "4     27.0      0.8    37.04        1.0                 0   1  \n",
      "5     26.0      0.8    36.99        0.0                 0   3  \n",
      "6     31.0      0.8    36.59        1.0                 0   3  \n",
      "7     30.0      0.8    36.49        0.0                 0   1  \n",
      "8     30.0      0.8    36.59        0.0                 2   1  \n",
      "9     31.0      0.8    36.89        1.0                 3   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_Max_2 T_OR_2 T_OR_Max_2 T_atm  \\\n",
      "0      35.02      34.92      35.07  ...   35.54  35.53      35.54  24.0   \n",
      "1      34.67      34.64      34.71  ...   34.93  34.92      34.93  24.0   \n",
      "2      35.63      35.57      35.69  ...    35.8  35.78       35.8  24.0   \n",
      "3      35.15      35.16      35.18  ...    35.7  35.03      35.05  24.0   \n",
      "4      35.34      35.59      35.66  ...   35.67  35.42      35.44  24.0   \n",
      "5      35.15      35.17      35.21  ...   35.99  35.97      35.99  24.0   \n",
      "6      35.17      35.28      35.29  ...   35.67   35.6      35.65  24.0   \n",
      "7      35.13      34.98      35.22  ...   35.73  35.67      35.73  25.0   \n",
      "8      35.34      35.38      35.43  ...   35.65  35.26      35.33  25.0   \n",
      "9      35.84      35.76      35.85  ...   36.36  36.34      36.36  26.0   \n",
      "\n",
      "  Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0     28.0      0.8    36.59        1.0                 0   6  \n",
      "1     26.0      0.8    37.19        0.0                 1   5  \n",
      "2     26.0      0.8    37.34        0.0                 0   3  \n",
      "3     27.0      0.8    37.09        0.0                 1   3  \n",
      "4     27.0      0.8    37.04        1.0                 0   1  \n",
      "5     26.0      0.8    36.99        0.0                 0   3  \n",
      "6     31.0      0.8    36.59        1.0                 0   3  \n",
      "7     30.0      0.8    36.49        0.0                 0   1  \n",
      "8     30.0      0.8    36.59        0.0                 2   1  \n",
      "9     31.0      0.8    36.89        1.0                 3   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_Max_3 T_OR_3 T_OR_Max_3 T_atm  \\\n",
      "0      35.01      34.56      35.05  ...   35.48  35.45      35.48  24.0   \n",
      "1      34.51      34.73      34.76  ...   35.14  35.13      35.14  24.0   \n",
      "2      35.68      35.68       35.7  ...   35.75  35.72      35.75  24.0   \n",
      "3      35.12      35.14      35.14  ...   35.77  35.03      35.04  24.0   \n",
      "4      35.59      35.63      35.69  ...   35.72  35.36      35.39  24.0   \n",
      "5      35.14      35.03      35.17  ...   35.76  35.73      35.76  24.0   \n",
      "6      35.34       35.2      35.38  ...   35.64  35.43       35.5  24.0   \n",
      "7      35.28      35.04      35.35  ...   35.71  35.67      35.71  25.0   \n",
      "8      35.21      35.32      35.33  ...   35.76  35.17      35.18  25.0   \n",
      "9      35.87       35.6      35.91  ...   36.21  36.14      36.21  26.0   \n",
      "\n",
      "  Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0     28.0      0.8    36.59        1.0                 0   6  \n",
      "1     26.0      0.8    37.19        0.0                 1   5  \n",
      "2     26.0      0.8    37.34        0.0                 0   3  \n",
      "3     27.0      0.8    37.09        0.0                 1   3  \n",
      "4     27.0      0.8    37.04        1.0                 0   1  \n",
      "5     26.0      0.8    36.99        0.0                 0   3  \n",
      "6     31.0      0.8    36.59        1.0                 0   3  \n",
      "7     30.0      0.8    36.49        0.0                 0   1  \n",
      "8     30.0      0.8    36.59        0.0                 2   1  \n",
      "9     31.0      0.8    36.89        1.0                 3   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_Max_4 T_OR_4 T_OR_Max_4 T_atm  \\\n",
      "0       35.0      34.97      35.03  ...   36.39  36.37      36.39  24.0   \n",
      "1      34.67      34.75      34.79  ...   35.84  35.82      35.84  24.0   \n",
      "2      35.69      35.75      35.78  ...    36.4  36.37       36.4  24.0   \n",
      "3      35.39      35.09      35.41  ...    35.7  35.06      35.08  24.0   \n",
      "4      35.67      35.56      35.69  ...   36.64  36.63      36.64  24.0   \n",
      "5      35.23      35.15      35.23  ...   35.73  35.68      35.73  24.0   \n",
      "6      35.37      35.31       35.4  ...   36.52  36.49      36.52  24.0   \n",
      "7      35.18      34.94      35.21  ...   36.41  36.34      36.41  25.0   \n",
      "8      35.28      35.36      35.38  ...   36.38  36.37      36.38  25.0   \n",
      "9      35.88       35.7      35.96  ...   36.41  36.33      36.41  26.0   \n",
      "\n",
      "  Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0     28.0      0.8    36.59        1.0                 0   6  \n",
      "1     26.0      0.8    37.19        0.0                 1   5  \n",
      "2     26.0      0.8    37.34        0.0                 0   3  \n",
      "3     27.0      0.8    37.09        0.0                 1   3  \n",
      "4     27.0      0.8    37.04        1.0                 0   1  \n",
      "5     26.0      0.8    36.99        0.0                 0   3  \n",
      "6     31.0      0.8    36.59        1.0                 0   3  \n",
      "7     30.0      0.8    36.49        0.0                 0   1  \n",
      "8     30.0      0.8    36.59        0.0                 2   1  \n",
      "9     31.0      0.8    36.89        1.0                 3   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "SubjectID            0\n",
      "T_offset_1           0\n",
      "Max1R13_1            0\n",
      "Max1L13_1            0\n",
      "aveAllR13_1          0\n",
      "aveAllL13_1          0\n",
      "T_RC_1               0\n",
      "T_RC_Dry_1           0\n",
      "T_RC_Wet_1           0\n",
      "T_RC_Max_1           0\n",
      "T_LC_1               0\n",
      "T_LC_Dry_1           0\n",
      "T_LC_Wet_1           0\n",
      "T_LC_Max_1           0\n",
      "RCC_1                0\n",
      "LCC_1                0\n",
      "canthiMax_1          0\n",
      "canthi4Max_1         0\n",
      "T_FHCC_1             0\n",
      "T_FHRC_1             0\n",
      "T_FHLC_1             0\n",
      "T_FHBC_1             0\n",
      "T_FHTC_1             0\n",
      "T_FH_Max_1           0\n",
      "T_FHC_Max_1          0\n",
      "T_Max_1              0\n",
      "T_OR_1               0\n",
      "T_OR_Max_1           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 2 \n",
      "SubjectID            0\n",
      "T_offset_2           0\n",
      "Max1R13_2            0\n",
      "Max1L13_2            0\n",
      "aveAllR13_2          0\n",
      "aveAllL13_2          0\n",
      "T_RC_2               0\n",
      "T_RC_Dry_2           0\n",
      "T_RC_Wet_2           0\n",
      "T_RC_Max_2           0\n",
      "T_LC_2               0\n",
      "T_LC_Dry_2           0\n",
      "T_LC_Wet_2           0\n",
      "T_LC_Max_2           0\n",
      "RCC_2                0\n",
      "LCC_2                0\n",
      "canthiMax_2          0\n",
      "canthi4Max_2         0\n",
      "T_FHCC_2             0\n",
      "T_FHRC_2             0\n",
      "T_FHLC_2             0\n",
      "T_FHBC_2             0\n",
      "T_FHTC_2             0\n",
      "T_FH_Max_2           0\n",
      "T_FHC_Max_2          0\n",
      "T_Max_2              0\n",
      "T_OR_2               0\n",
      "T_OR_Max_2           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 3 \n",
      "SubjectID            0\n",
      "T_offset_3           0\n",
      "Max1R13_3            0\n",
      "Max1L13_3            0\n",
      "aveAllR13_3          0\n",
      "aveAllL13_3          0\n",
      "T_RC_3               0\n",
      "T_RC_Dry_3           0\n",
      "T_RC_Wet_3           0\n",
      "T_RC_Max_3           0\n",
      "T_LC_3               0\n",
      "T_LC_Dry_3           0\n",
      "T_LC_Wet_3           0\n",
      "T_LC_Max_3           0\n",
      "RCC_3                0\n",
      "LCC_3                0\n",
      "canthiMax_3          0\n",
      "canthi4Max_3         0\n",
      "T_FHCC_3             0\n",
      "T_FHRC_3             0\n",
      "T_FHLC_3             0\n",
      "T_FHBC_3             0\n",
      "T_FHTC_3             0\n",
      "T_FH_Max_3           0\n",
      "T_FHC_Max_3          0\n",
      "T_Max_3              0\n",
      "T_OR_3               0\n",
      "T_OR_Max_3           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 4 \n",
      "SubjectID            0\n",
      "T_offset_4           0\n",
      "Max1R13_4            0\n",
      "Max1L13_4            0\n",
      "aveAllR13_4          0\n",
      "aveAllL13_4          0\n",
      "T_RC_4               0\n",
      "T_RC_Dry_4           0\n",
      "T_RC_Wet_4           0\n",
      "T_RC_Max_4           0\n",
      "T_LC_4               0\n",
      "T_LC_Dry_4           0\n",
      "T_LC_Wet_4           0\n",
      "T_LC_Max_4           0\n",
      "RCC_4                0\n",
      "LCC_4                0\n",
      "canthiMax_4          0\n",
      "canthi4Max_4         0\n",
      "T_FHCC_4             0\n",
      "T_FHRC_4             0\n",
      "T_FHLC_4             0\n",
      "T_FHBC_4             0\n",
      "T_FHTC_4             0\n",
      "T_FH_Max_4           0\n",
      "T_FHC_Max_4          0\n",
      "T_Max_4              0\n",
      "T_OR_4               0\n",
      "T_OR_Max_4           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.isnull().sum()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  35.456819  33.82   38.0      35.45\n",
      "1         1.0  35.621748   33.9  38.16  35.594601\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.518769  34.11  38.05    35.5\n",
      "1         1.0  35.696512  34.42  37.92  35.645\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.534282  34.05  38.07  35.505\n",
      "1         1.0  35.725379   34.6  38.03   35.66\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.576551  34.33  37.95   35.55\n",
      "1         1.0  35.757636  34.64  37.97  35.675\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'T_LC_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... Humidity Distance aveOralM Gender_NEW  \\\n",
      "0      34.91       34.6      34.98  ...     28.0      0.8    36.59        1.0   \n",
      "1      34.68      34.44      34.71  ...     26.0      0.8    37.19        0.0   \n",
      "2      35.67      35.46       35.7  ...     26.0      0.8    37.34        0.0   \n",
      "3      35.14      35.08      35.17  ...     27.0      0.8    37.09        0.0   \n",
      "4       35.3       35.5      35.52  ...     27.0      0.8    37.04        1.0   \n",
      "5      34.97      34.85      34.98  ...     26.0      0.8    36.99        0.0   \n",
      "6      35.12      35.05      35.15  ...     31.0      0.8    36.59        1.0   \n",
      "7      34.71       34.8      34.87  ...     30.0      0.8    36.49        0.0   \n",
      "8      35.27       35.3      35.32  ...     30.0      0.8    36.59        0.0   \n",
      "9      35.75      35.36      35.84  ...     31.0      0.8    36.89        1.0   \n",
      "\n",
      "  Ethnicity_encoded Age T_LC_1_Mean T_LC_1_Min T_LC_1_Max T_LC_1_Median  \n",
      "0                 0   6   35.621748      33.90      38.16     35.594601  \n",
      "1                 1   5   35.456819      33.82      38.00     35.450000  \n",
      "2                 0   3   35.456819      33.82      38.00     35.450000  \n",
      "3                 1   3   35.456819      33.82      38.00     35.450000  \n",
      "4                 0   1   35.621748      33.90      38.16     35.594601  \n",
      "5                 0   3   35.456819      33.82      38.00     35.450000  \n",
      "6                 0   3   35.621748      33.90      38.16     35.594601  \n",
      "7                 0   1   35.456819      33.82      38.00     35.450000  \n",
      "8                 2   1   35.456819      33.82      38.00     35.450000  \n",
      "9                 3   1   35.621748      33.90      38.16     35.594601  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... Humidity Distance aveOralM Gender_NEW  \\\n",
      "0      35.02      34.92      35.07  ...     28.0      0.8    36.59        1.0   \n",
      "1      34.67      34.64      34.71  ...     26.0      0.8    37.19        0.0   \n",
      "2      35.63      35.57      35.69  ...     26.0      0.8    37.34        0.0   \n",
      "3      35.15      35.16      35.18  ...     27.0      0.8    37.09        0.0   \n",
      "4      35.34      35.59      35.66  ...     27.0      0.8    37.04        1.0   \n",
      "5      35.15      35.17      35.21  ...     26.0      0.8    36.99        0.0   \n",
      "6      35.17      35.28      35.29  ...     31.0      0.8    36.59        1.0   \n",
      "7      35.13      34.98      35.22  ...     30.0      0.8    36.49        0.0   \n",
      "8      35.34      35.38      35.43  ...     30.0      0.8    36.59        0.0   \n",
      "9      35.84      35.76      35.85  ...     31.0      0.8    36.89        1.0   \n",
      "\n",
      "  Ethnicity_encoded Age T_LC_2_Mean T_LC_2_Min T_LC_2_Max T_LC_2_Median  \n",
      "0                 0   6   35.696512      34.42      37.92        35.645  \n",
      "1                 1   5   35.518769      34.11      38.05        35.500  \n",
      "2                 0   3   35.518769      34.11      38.05        35.500  \n",
      "3                 1   3   35.518769      34.11      38.05        35.500  \n",
      "4                 0   1   35.696512      34.42      37.92        35.645  \n",
      "5                 0   3   35.518769      34.11      38.05        35.500  \n",
      "6                 0   3   35.696512      34.42      37.92        35.645  \n",
      "7                 0   1   35.518769      34.11      38.05        35.500  \n",
      "8                 2   1   35.518769      34.11      38.05        35.500  \n",
      "9                 3   1   35.696512      34.42      37.92        35.645  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... Humidity Distance aveOralM Gender_NEW  \\\n",
      "0      35.01      34.56      35.05  ...     28.0      0.8    36.59        1.0   \n",
      "1      34.51      34.73      34.76  ...     26.0      0.8    37.19        0.0   \n",
      "2      35.68      35.68       35.7  ...     26.0      0.8    37.34        0.0   \n",
      "3      35.12      35.14      35.14  ...     27.0      0.8    37.09        0.0   \n",
      "4      35.59      35.63      35.69  ...     27.0      0.8    37.04        1.0   \n",
      "5      35.14      35.03      35.17  ...     26.0      0.8    36.99        0.0   \n",
      "6      35.34       35.2      35.38  ...     31.0      0.8    36.59        1.0   \n",
      "7      35.28      35.04      35.35  ...     30.0      0.8    36.49        0.0   \n",
      "8      35.21      35.32      35.33  ...     30.0      0.8    36.59        0.0   \n",
      "9      35.87       35.6      35.91  ...     31.0      0.8    36.89        1.0   \n",
      "\n",
      "  Ethnicity_encoded Age T_LC_3_Mean T_LC_3_Min T_LC_3_Max T_LC_3_Median  \n",
      "0                 0   6   35.725379      34.60      38.03        35.660  \n",
      "1                 1   5   35.534282      34.05      38.07        35.505  \n",
      "2                 0   3   35.534282      34.05      38.07        35.505  \n",
      "3                 1   3   35.534282      34.05      38.07        35.505  \n",
      "4                 0   1   35.725379      34.60      38.03        35.660  \n",
      "5                 0   3   35.534282      34.05      38.07        35.505  \n",
      "6                 0   3   35.725379      34.60      38.03        35.660  \n",
      "7                 0   1   35.534282      34.05      38.07        35.505  \n",
      "8                 2   1   35.534282      34.05      38.07        35.505  \n",
      "9                 3   1   35.725379      34.60      38.03        35.660  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... Humidity Distance aveOralM Gender_NEW  \\\n",
      "0       35.0      34.97      35.03  ...     28.0      0.8    36.59        1.0   \n",
      "1      34.67      34.75      34.79  ...     26.0      0.8    37.19        0.0   \n",
      "2      35.69      35.75      35.78  ...     26.0      0.8    37.34        0.0   \n",
      "3      35.39      35.09      35.41  ...     27.0      0.8    37.09        0.0   \n",
      "4      35.67      35.56      35.69  ...     27.0      0.8    37.04        1.0   \n",
      "5      35.23      35.15      35.23  ...     26.0      0.8    36.99        0.0   \n",
      "6      35.37      35.31       35.4  ...     31.0      0.8    36.59        1.0   \n",
      "7      35.18      34.94      35.21  ...     30.0      0.8    36.49        0.0   \n",
      "8      35.28      35.36      35.38  ...     30.0      0.8    36.59        0.0   \n",
      "9      35.88       35.7      35.96  ...     31.0      0.8    36.89        1.0   \n",
      "\n",
      "  Ethnicity_encoded Age T_LC_4_Mean T_LC_4_Min T_LC_4_Max T_LC_4_Median  \n",
      "0                 0   6   35.757636      34.64      37.97        35.675  \n",
      "1                 1   5   35.576551      34.33      37.95        35.550  \n",
      "2                 0   3   35.576551      34.33      37.95        35.550  \n",
      "3                 1   3   35.576551      34.33      37.95        35.550  \n",
      "4                 0   1   35.757636      34.64      37.97        35.675  \n",
      "5                 0   3   35.576551      34.33      37.95        35.550  \n",
      "6                 0   3   35.757636      34.64      37.97        35.675  \n",
      "7                 0   1   35.576551      34.33      37.95        35.550  \n",
      "8                 2   1   35.576551      34.33      37.95        35.550  \n",
      "9                 3   1   35.757636      34.64      37.97        35.675  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.597999  33.85  38.03  35.585\n",
      "1         1.0  35.777812   34.3  38.52   35.74\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min    max median\n",
      "0         0.0  35.662038  34.14  38.06  35.63\n",
      "1         1.0   35.84663  34.76  38.25  35.76\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max median\n",
      "0         0.0  35.682364  34.06  38.11  35.65\n",
      "1         1.0  35.876759  34.83  38.32   35.8\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.725187  34.53  37.97  35.705\n",
      "1         1.0  35.913429  34.87  38.37  35.795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'canthiMax_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  34.913829   31.7  37.06     34.945\n",
      "1         1.0  34.970455  31.63  37.38  35.032753\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.030533  32.16  37.38   35.05\n",
      "1         1.0  35.096935  32.85  37.32  35.125\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.042979  32.16  37.46   35.06\n",
      "1         1.0  35.143034  32.97  37.31  35.175\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.018249   32.8  37.47   35.02\n",
      "1         1.0  35.102236  33.15  37.15  35.095\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'T_FHC_Max_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max median\n",
      "0         0.0   34.32962  30.02  36.36  34.41\n",
      "1         1.0  34.454314  30.93  36.77  34.54\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  34.492811  30.24  36.54  34.520068\n",
      "1         1.0  34.622519  32.31  36.79      34.69\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  34.501906  28.94   36.6  34.520136\n",
      "1         1.0   34.65969   32.5  36.63      34.71\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  34.529527  30.53  36.62  34.525068\n",
      "1         1.0  34.672317  32.63  36.77     34.695\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'T_FHCC_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... canthiMax_1_Max canthiMax_1_Median  \\\n",
      "0      34.91       34.6      34.98  ...           38.52             35.740   \n",
      "1      34.68      34.44      34.71  ...           38.03             35.585   \n",
      "2      35.67      35.46       35.7  ...           38.03             35.585   \n",
      "3      35.14      35.08      35.17  ...           38.03             35.585   \n",
      "4       35.3       35.5      35.52  ...           38.52             35.740   \n",
      "5      34.97      34.85      34.98  ...           38.03             35.585   \n",
      "6      35.12      35.05      35.15  ...           38.52             35.740   \n",
      "7      34.71       34.8      34.87  ...           38.03             35.585   \n",
      "8      35.27       35.3      35.32  ...           38.03             35.585   \n",
      "9      35.75      35.36      35.84  ...           38.52             35.740   \n",
      "\n",
      "  T_FHC_Max_1_Mean T_FHC_Max_1_Min T_FHC_Max_1_Max T_FHC_Max_1_Median  \\\n",
      "0        34.970455           31.63           37.38          35.032753   \n",
      "1        34.913829           31.70           37.06          34.945000   \n",
      "2        34.913829           31.70           37.06          34.945000   \n",
      "3        34.913829           31.70           37.06          34.945000   \n",
      "4        34.970455           31.63           37.38          35.032753   \n",
      "5        34.913829           31.70           37.06          34.945000   \n",
      "6        34.970455           31.63           37.38          35.032753   \n",
      "7        34.913829           31.70           37.06          34.945000   \n",
      "8        34.913829           31.70           37.06          34.945000   \n",
      "9        34.970455           31.63           37.38          35.032753   \n",
      "\n",
      "  T_FHCC_1_Mean T_FHCC_1_Min T_FHCC_1_Max T_FHCC_1_Median  \n",
      "0     34.454314        30.93        36.77           34.54  \n",
      "1     34.329620        30.02        36.36           34.41  \n",
      "2     34.329620        30.02        36.36           34.41  \n",
      "3     34.329620        30.02        36.36           34.41  \n",
      "4     34.454314        30.93        36.77           34.54  \n",
      "5     34.329620        30.02        36.36           34.41  \n",
      "6     34.454314        30.93        36.77           34.54  \n",
      "7     34.329620        30.02        36.36           34.41  \n",
      "8     34.329620        30.02        36.36           34.41  \n",
      "9     34.454314        30.93        36.77           34.54  \n",
      "\n",
      "[10 rows x 51 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... canthiMax_2_Max canthiMax_2_Median  \\\n",
      "0      35.02      34.92      35.07  ...           38.25              35.76   \n",
      "1      34.67      34.64      34.71  ...           38.06              35.63   \n",
      "2      35.63      35.57      35.69  ...           38.06              35.63   \n",
      "3      35.15      35.16      35.18  ...           38.06              35.63   \n",
      "4      35.34      35.59      35.66  ...           38.25              35.76   \n",
      "5      35.15      35.17      35.21  ...           38.06              35.63   \n",
      "6      35.17      35.28      35.29  ...           38.25              35.76   \n",
      "7      35.13      34.98      35.22  ...           38.06              35.63   \n",
      "8      35.34      35.38      35.43  ...           38.06              35.63   \n",
      "9      35.84      35.76      35.85  ...           38.25              35.76   \n",
      "\n",
      "  T_FHC_Max_2_Mean T_FHC_Max_2_Min T_FHC_Max_2_Max T_FHC_Max_2_Median  \\\n",
      "0        35.096935           32.85           37.32             35.125   \n",
      "1        35.030533           32.16           37.38             35.050   \n",
      "2        35.030533           32.16           37.38             35.050   \n",
      "3        35.030533           32.16           37.38             35.050   \n",
      "4        35.096935           32.85           37.32             35.125   \n",
      "5        35.030533           32.16           37.38             35.050   \n",
      "6        35.096935           32.85           37.32             35.125   \n",
      "7        35.030533           32.16           37.38             35.050   \n",
      "8        35.030533           32.16           37.38             35.050   \n",
      "9        35.096935           32.85           37.32             35.125   \n",
      "\n",
      "  T_FHCC_2_Mean T_FHCC_2_Min T_FHCC_2_Max T_FHCC_2_Median  \n",
      "0     34.622519        32.31        36.79       34.690000  \n",
      "1     34.492811        30.24        36.54       34.520068  \n",
      "2     34.492811        30.24        36.54       34.520068  \n",
      "3     34.492811        30.24        36.54       34.520068  \n",
      "4     34.622519        32.31        36.79       34.690000  \n",
      "5     34.492811        30.24        36.54       34.520068  \n",
      "6     34.622519        32.31        36.79       34.690000  \n",
      "7     34.492811        30.24        36.54       34.520068  \n",
      "8     34.492811        30.24        36.54       34.520068  \n",
      "9     34.622519        32.31        36.79       34.690000  \n",
      "\n",
      "[10 rows x 51 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... canthiMax_3_Max canthiMax_3_Median  \\\n",
      "0      35.01      34.56      35.05  ...           38.32              35.80   \n",
      "1      34.51      34.73      34.76  ...           38.11              35.65   \n",
      "2      35.68      35.68       35.7  ...           38.11              35.65   \n",
      "3      35.12      35.14      35.14  ...           38.11              35.65   \n",
      "4      35.59      35.63      35.69  ...           38.32              35.80   \n",
      "5      35.14      35.03      35.17  ...           38.11              35.65   \n",
      "6      35.34       35.2      35.38  ...           38.32              35.80   \n",
      "7      35.28      35.04      35.35  ...           38.11              35.65   \n",
      "8      35.21      35.32      35.33  ...           38.11              35.65   \n",
      "9      35.87       35.6      35.91  ...           38.32              35.80   \n",
      "\n",
      "  T_FHC_Max_3_Mean T_FHC_Max_3_Min T_FHC_Max_3_Max T_FHC_Max_3_Median  \\\n",
      "0        35.143034           32.97           37.31             35.175   \n",
      "1        35.042979           32.16           37.46             35.060   \n",
      "2        35.042979           32.16           37.46             35.060   \n",
      "3        35.042979           32.16           37.46             35.060   \n",
      "4        35.143034           32.97           37.31             35.175   \n",
      "5        35.042979           32.16           37.46             35.060   \n",
      "6        35.143034           32.97           37.31             35.175   \n",
      "7        35.042979           32.16           37.46             35.060   \n",
      "8        35.042979           32.16           37.46             35.060   \n",
      "9        35.143034           32.97           37.31             35.175   \n",
      "\n",
      "  T_FHCC_3_Mean T_FHCC_3_Min T_FHCC_3_Max T_FHCC_3_Median  \n",
      "0     34.659690        32.50        36.63       34.710000  \n",
      "1     34.501906        28.94        36.60       34.520136  \n",
      "2     34.501906        28.94        36.60       34.520136  \n",
      "3     34.501906        28.94        36.60       34.520136  \n",
      "4     34.659690        32.50        36.63       34.710000  \n",
      "5     34.501906        28.94        36.60       34.520136  \n",
      "6     34.659690        32.50        36.63       34.710000  \n",
      "7     34.501906        28.94        36.60       34.520136  \n",
      "8     34.501906        28.94        36.60       34.520136  \n",
      "9     34.659690        32.50        36.63       34.710000  \n",
      "\n",
      "[10 rows x 51 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... canthiMax_4_Max canthiMax_4_Median  \\\n",
      "0       35.0      34.97      35.03  ...           38.37             35.795   \n",
      "1      34.67      34.75      34.79  ...           37.97             35.705   \n",
      "2      35.69      35.75      35.78  ...           37.97             35.705   \n",
      "3      35.39      35.09      35.41  ...           37.97             35.705   \n",
      "4      35.67      35.56      35.69  ...           38.37             35.795   \n",
      "5      35.23      35.15      35.23  ...           37.97             35.705   \n",
      "6      35.37      35.31       35.4  ...           38.37             35.795   \n",
      "7      35.18      34.94      35.21  ...           37.97             35.705   \n",
      "8      35.28      35.36      35.38  ...           37.97             35.705   \n",
      "9      35.88       35.7      35.96  ...           38.37             35.795   \n",
      "\n",
      "  T_FHC_Max_4_Mean T_FHC_Max_4_Min T_FHC_Max_4_Max T_FHC_Max_4_Median  \\\n",
      "0        35.102236           33.15           37.15             35.095   \n",
      "1        35.018249           32.80           37.47             35.020   \n",
      "2        35.018249           32.80           37.47             35.020   \n",
      "3        35.018249           32.80           37.47             35.020   \n",
      "4        35.102236           33.15           37.15             35.095   \n",
      "5        35.018249           32.80           37.47             35.020   \n",
      "6        35.102236           33.15           37.15             35.095   \n",
      "7        35.018249           32.80           37.47             35.020   \n",
      "8        35.018249           32.80           37.47             35.020   \n",
      "9        35.102236           33.15           37.15             35.095   \n",
      "\n",
      "  T_FHCC_4_Mean T_FHCC_4_Min T_FHCC_4_Max T_FHCC_4_Median  \n",
      "0     34.672317        32.63        36.77       34.695000  \n",
      "1     34.529527        30.53        36.62       34.525068  \n",
      "2     34.529527        30.53        36.62       34.525068  \n",
      "3     34.529527        30.53        36.62       34.525068  \n",
      "4     34.672317        32.63        36.77       34.695000  \n",
      "5     34.529527        30.53        36.62       34.525068  \n",
      "6     34.672317        32.63        36.77       34.695000  \n",
      "7     34.529527        30.53        36.62       34.525068  \n",
      "8     34.529527        30.53        36.62       34.525068  \n",
      "9     34.672317        32.63        36.77       34.695000  \n",
      "\n",
      "[10 rows x 51 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max     median\n",
      "0                  0  34.411406  30.93  36.47       34.5\n",
      "1                  1  34.177282  31.75  36.36       34.3\n",
      "2                  2  34.400964  30.02  36.36  34.520136\n",
      "3                  3  34.542258  32.89  36.77      34.48\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  34.573661  31.16  36.61   34.6\n",
      "1                  1  34.330485  32.81  36.38  34.36\n",
      "2                  2  34.583223  30.24  36.54  34.69\n",
      "3                  3   34.68871  32.95  36.79  34.72\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  34.606692  28.94  36.59  34.66\n",
      "1                  1   34.35437  33.12  36.44  34.35\n",
      "2                  2  34.584014  30.22   36.6  34.66\n",
      "3                  3  34.650645  32.97  36.63  34.71\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  34.631808  30.53  36.68  34.63\n",
      "1                  1  34.325731  33.08  36.19  34.32\n",
      "2                  2  34.630742  32.57  36.62  34.66\n",
      "3                  3  34.648065  32.96  36.77  34.73\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'T_FHCC_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_FHC_Max_1_Max T_FHC_Max_1_Median  \\\n",
      "0      34.91       34.6      34.98  ...           37.38          35.032753   \n",
      "1      34.68      34.44      34.71  ...           37.06          34.945000   \n",
      "2      35.67      35.46       35.7  ...           37.06          34.945000   \n",
      "3      35.14      35.08      35.17  ...           37.06          34.945000   \n",
      "4       35.3       35.5      35.52  ...           37.38          35.032753   \n",
      "5      34.97      34.85      34.98  ...           37.06          34.945000   \n",
      "6      35.12      35.05      35.15  ...           37.38          35.032753   \n",
      "7      34.71       34.8      34.87  ...           37.06          34.945000   \n",
      "8      35.27       35.3      35.32  ...           37.06          34.945000   \n",
      "9      35.75      35.36      35.84  ...           37.38          35.032753   \n",
      "\n",
      "  T_FHCC_1_Mean_x T_FHCC_1_Min_x T_FHCC_1_Max_x T_FHCC_1_Median_x  \\\n",
      "0       34.454314          30.93          36.77             34.54   \n",
      "1       34.329620          30.02          36.36             34.41   \n",
      "2       34.329620          30.02          36.36             34.41   \n",
      "3       34.329620          30.02          36.36             34.41   \n",
      "4       34.454314          30.93          36.77             34.54   \n",
      "5       34.329620          30.02          36.36             34.41   \n",
      "6       34.454314          30.93          36.77             34.54   \n",
      "7       34.329620          30.02          36.36             34.41   \n",
      "8       34.329620          30.02          36.36             34.41   \n",
      "9       34.454314          30.93          36.77             34.54   \n",
      "\n",
      "  T_FHCC_1_Mean_y T_FHCC_1_Min_y T_FHCC_1_Max_y T_FHCC_1_Median_y  \n",
      "0       34.411406          30.93          36.47              34.5  \n",
      "1       34.177282          31.75          36.36              34.3  \n",
      "2       34.411406          30.93          36.47              34.5  \n",
      "3       34.177282          31.75          36.36              34.3  \n",
      "4       34.411406          30.93          36.47              34.5  \n",
      "5       34.411406          30.93          36.47              34.5  \n",
      "6       34.411406          30.93          36.47              34.5  \n",
      "7       34.411406          30.93          36.47              34.5  \n",
      "8       34.400964          30.02          36.36         34.520136  \n",
      "9       34.542258          32.89          36.77             34.48  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_FHC_Max_2_Max T_FHC_Max_2_Median  \\\n",
      "0      35.02      34.92      35.07  ...           37.32             35.125   \n",
      "1      34.67      34.64      34.71  ...           37.38             35.050   \n",
      "2      35.63      35.57      35.69  ...           37.38             35.050   \n",
      "3      35.15      35.16      35.18  ...           37.38             35.050   \n",
      "4      35.34      35.59      35.66  ...           37.32             35.125   \n",
      "5      35.15      35.17      35.21  ...           37.38             35.050   \n",
      "6      35.17      35.28      35.29  ...           37.32             35.125   \n",
      "7      35.13      34.98      35.22  ...           37.38             35.050   \n",
      "8      35.34      35.38      35.43  ...           37.38             35.050   \n",
      "9      35.84      35.76      35.85  ...           37.32             35.125   \n",
      "\n",
      "  T_FHCC_2_Mean_x T_FHCC_2_Min_x T_FHCC_2_Max_x T_FHCC_2_Median_x  \\\n",
      "0       34.622519          32.31          36.79         34.690000   \n",
      "1       34.492811          30.24          36.54         34.520068   \n",
      "2       34.492811          30.24          36.54         34.520068   \n",
      "3       34.492811          30.24          36.54         34.520068   \n",
      "4       34.622519          32.31          36.79         34.690000   \n",
      "5       34.492811          30.24          36.54         34.520068   \n",
      "6       34.622519          32.31          36.79         34.690000   \n",
      "7       34.492811          30.24          36.54         34.520068   \n",
      "8       34.492811          30.24          36.54         34.520068   \n",
      "9       34.622519          32.31          36.79         34.690000   \n",
      "\n",
      "  T_FHCC_2_Mean_y T_FHCC_2_Min_y T_FHCC_2_Max_y T_FHCC_2_Median_y  \n",
      "0       34.573661          31.16          36.61              34.6  \n",
      "1       34.330485          32.81          36.38             34.36  \n",
      "2       34.573661          31.16          36.61              34.6  \n",
      "3       34.330485          32.81          36.38             34.36  \n",
      "4       34.573661          31.16          36.61              34.6  \n",
      "5       34.573661          31.16          36.61              34.6  \n",
      "6       34.573661          31.16          36.61              34.6  \n",
      "7       34.573661          31.16          36.61              34.6  \n",
      "8       34.583223          30.24          36.54             34.69  \n",
      "9        34.68871          32.95          36.79             34.72  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_FHC_Max_3_Max T_FHC_Max_3_Median  \\\n",
      "0      35.01      34.56      35.05  ...           37.31             35.175   \n",
      "1      34.51      34.73      34.76  ...           37.46             35.060   \n",
      "2      35.68      35.68       35.7  ...           37.46             35.060   \n",
      "3      35.12      35.14      35.14  ...           37.46             35.060   \n",
      "4      35.59      35.63      35.69  ...           37.31             35.175   \n",
      "5      35.14      35.03      35.17  ...           37.46             35.060   \n",
      "6      35.34       35.2      35.38  ...           37.31             35.175   \n",
      "7      35.28      35.04      35.35  ...           37.46             35.060   \n",
      "8      35.21      35.32      35.33  ...           37.46             35.060   \n",
      "9      35.87       35.6      35.91  ...           37.31             35.175   \n",
      "\n",
      "  T_FHCC_3_Mean_x T_FHCC_3_Min_x T_FHCC_3_Max_x T_FHCC_3_Median_x  \\\n",
      "0       34.659690          32.50          36.63         34.710000   \n",
      "1       34.501906          28.94          36.60         34.520136   \n",
      "2       34.501906          28.94          36.60         34.520136   \n",
      "3       34.501906          28.94          36.60         34.520136   \n",
      "4       34.659690          32.50          36.63         34.710000   \n",
      "5       34.501906          28.94          36.60         34.520136   \n",
      "6       34.659690          32.50          36.63         34.710000   \n",
      "7       34.501906          28.94          36.60         34.520136   \n",
      "8       34.501906          28.94          36.60         34.520136   \n",
      "9       34.659690          32.50          36.63         34.710000   \n",
      "\n",
      "  T_FHCC_3_Mean_y T_FHCC_3_Min_y T_FHCC_3_Max_y T_FHCC_3_Median_y  \n",
      "0       34.606692          28.94          36.59             34.66  \n",
      "1        34.35437          33.12          36.44             34.35  \n",
      "2       34.606692          28.94          36.59             34.66  \n",
      "3        34.35437          33.12          36.44             34.35  \n",
      "4       34.606692          28.94          36.59             34.66  \n",
      "5       34.606692          28.94          36.59             34.66  \n",
      "6       34.606692          28.94          36.59             34.66  \n",
      "7       34.606692          28.94          36.59             34.66  \n",
      "8       34.584014          30.22           36.6             34.66  \n",
      "9       34.650645          32.97          36.63             34.71  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_FHC_Max_4_Max T_FHC_Max_4_Median  \\\n",
      "0       35.0      34.97      35.03  ...           37.15             35.095   \n",
      "1      34.67      34.75      34.79  ...           37.47             35.020   \n",
      "2      35.69      35.75      35.78  ...           37.47             35.020   \n",
      "3      35.39      35.09      35.41  ...           37.47             35.020   \n",
      "4      35.67      35.56      35.69  ...           37.15             35.095   \n",
      "5      35.23      35.15      35.23  ...           37.47             35.020   \n",
      "6      35.37      35.31       35.4  ...           37.15             35.095   \n",
      "7      35.18      34.94      35.21  ...           37.47             35.020   \n",
      "8      35.28      35.36      35.38  ...           37.47             35.020   \n",
      "9      35.88       35.7      35.96  ...           37.15             35.095   \n",
      "\n",
      "  T_FHCC_4_Mean_x T_FHCC_4_Min_x T_FHCC_4_Max_x T_FHCC_4_Median_x  \\\n",
      "0       34.672317          32.63          36.77         34.695000   \n",
      "1       34.529527          30.53          36.62         34.525068   \n",
      "2       34.529527          30.53          36.62         34.525068   \n",
      "3       34.529527          30.53          36.62         34.525068   \n",
      "4       34.672317          32.63          36.77         34.695000   \n",
      "5       34.529527          30.53          36.62         34.525068   \n",
      "6       34.672317          32.63          36.77         34.695000   \n",
      "7       34.529527          30.53          36.62         34.525068   \n",
      "8       34.529527          30.53          36.62         34.525068   \n",
      "9       34.672317          32.63          36.77         34.695000   \n",
      "\n",
      "  T_FHCC_4_Mean_y T_FHCC_4_Min_y T_FHCC_4_Max_y T_FHCC_4_Median_y  \n",
      "0       34.631808          30.53          36.68             34.63  \n",
      "1       34.325731          33.08          36.19             34.32  \n",
      "2       34.631808          30.53          36.68             34.63  \n",
      "3       34.325731          33.08          36.19             34.32  \n",
      "4       34.631808          30.53          36.68             34.63  \n",
      "5       34.631808          30.53          36.68             34.63  \n",
      "6       34.631808          30.53          36.68             34.63  \n",
      "7       34.631808          30.53          36.68             34.63  \n",
      "8       34.630742          32.57          36.62             34.66  \n",
      "9       34.648065          32.96          36.77             34.73  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.772387  34.44  37.94  35.74\n",
      "1                  1  35.501262   34.3  38.03  35.47\n",
      "2                  2  35.517473  33.85  37.48  35.47\n",
      "3                  3  35.816774   34.4  38.52  35.74\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0   35.83283  34.74  38.06  35.76\n",
      "1                  1  35.557087  34.71  38.01  35.48\n",
      "2                  2  35.602596  34.14  37.79  35.54\n",
      "3                  3   35.87871  34.59  38.25  35.76\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.859148  34.67  38.11   35.8\n",
      "1                  1   35.59673  34.74  37.94  35.54\n",
      "2                  2  35.613952  34.06  37.83  35.57\n",
      "3                  3  35.900645  34.74  38.32   35.8\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.887707  34.72   38.0  35.82\n",
      "1                  1   35.63045  34.78  37.77  35.59\n",
      "2                  2  35.688089  34.53  37.81  35.69\n",
      "3                  3  35.920968  34.67  38.37  35.77\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'canthiMax_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max     median\n",
      "0                  0  34.950299  31.63  36.69      35.03\n",
      "1                  1  34.771553  32.99  36.63      34.74\n",
      "2                  2  34.983057   31.7  37.06  35.032753\n",
      "3                  3  35.051613  33.97  37.38      34.94\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.082021  32.16  36.71   35.1\n",
      "1                  1  34.871748  33.44  37.02  34.84\n",
      "2                  2  35.098804   32.3  37.38  35.15\n",
      "3                  3  35.126774  33.92  37.32  35.09\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.116992  32.32  37.41  35.13\n",
      "1                  1  34.896434  33.75   36.7  34.88\n",
      "2                  2  35.112476  32.16  37.46  35.16\n",
      "3                  3  35.116452  33.83  37.31  35.12\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.094111   32.8  37.03  35.07\n",
      "1                  1  34.807044  33.61  36.55   34.8\n",
      "2                  2  35.100721  33.49  37.47  35.09\n",
      "3                  3  35.058387  33.54  37.15  35.15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'T_FHC_Max_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0   35.84951  34.47  37.94  35.81\n",
      "1                  1  35.635146  34.67  38.03  35.59\n",
      "2                  2  35.673647  33.85  37.64  35.66\n",
      "3                  3      35.91  34.88  38.52  35.75\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  36.033003   34.8   38.4  35.98\n",
      "1                  1  35.778252  34.93  38.01  35.71\n",
      "2                  2  35.818921   34.7  37.79  35.77\n",
      "3                  3  36.092581  35.16  38.66  35.99\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  36.033684  34.86  38.11  35.99\n",
      "1                  1  35.818508  35.03  37.94  35.76\n",
      "2                  2  35.814515  34.81  37.83  35.78\n",
      "3                  3   36.10871  35.15  38.37  35.93\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  36.568847  35.02  38.84  36.57\n",
      "1                  1  36.532842  35.34  38.73   36.6\n",
      "2                  2  36.491659  35.29  38.72   36.5\n",
      "3                  3  36.740323  35.87  39.68  36.69\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'T_Max_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... canthiMax_1_Max_y  \\\n",
      "0      34.91       34.6      34.98  ...             37.94   \n",
      "1      34.68      34.44      34.71  ...             38.03   \n",
      "2      35.67      35.46       35.7  ...             37.94   \n",
      "3      35.14      35.08      35.17  ...             38.03   \n",
      "4       35.3       35.5      35.52  ...             37.94   \n",
      "5      34.97      34.85      34.98  ...             37.94   \n",
      "6      35.12      35.05      35.15  ...             37.94   \n",
      "7      34.71       34.8      34.87  ...             37.94   \n",
      "8      35.27       35.3      35.32  ...             37.48   \n",
      "9      35.75      35.36      35.84  ...             38.52   \n",
      "\n",
      "  canthiMax_1_Median_y T_FHC_Max_1_Mean_y T_FHC_Max_1_Min_y T_FHC_Max_1_Max_y  \\\n",
      "0                35.74          34.950299             31.63             36.69   \n",
      "1                35.47          34.771553             32.99             36.63   \n",
      "2                35.74          34.950299             31.63             36.69   \n",
      "3                35.47          34.771553             32.99             36.63   \n",
      "4                35.74          34.950299             31.63             36.69   \n",
      "5                35.74          34.950299             31.63             36.69   \n",
      "6                35.74          34.950299             31.63             36.69   \n",
      "7                35.74          34.950299             31.63             36.69   \n",
      "8                35.47          34.983057              31.7             37.06   \n",
      "9                35.74          35.051613             33.97             37.38   \n",
      "\n",
      "  T_FHC_Max_1_Median_y T_Max_1_Mean T_Max_1_Min T_Max_1_Max T_Max_1_Median  \n",
      "0                35.03     35.84951       34.47       37.94          35.81  \n",
      "1                34.74    35.635146       34.67       38.03          35.59  \n",
      "2                35.03     35.84951       34.47       37.94          35.81  \n",
      "3                34.74    35.635146       34.67       38.03          35.59  \n",
      "4                35.03     35.84951       34.47       37.94          35.81  \n",
      "5                35.03     35.84951       34.47       37.94          35.81  \n",
      "6                35.03     35.84951       34.47       37.94          35.81  \n",
      "7                35.03     35.84951       34.47       37.94          35.81  \n",
      "8            35.032753    35.673647       33.85       37.64          35.66  \n",
      "9                34.94        35.91       34.88       38.52          35.75  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... canthiMax_2_Max_y  \\\n",
      "0      35.02      34.92      35.07  ...             38.06   \n",
      "1      34.67      34.64      34.71  ...             38.01   \n",
      "2      35.63      35.57      35.69  ...             38.06   \n",
      "3      35.15      35.16      35.18  ...             38.01   \n",
      "4      35.34      35.59      35.66  ...             38.06   \n",
      "5      35.15      35.17      35.21  ...             38.06   \n",
      "6      35.17      35.28      35.29  ...             38.06   \n",
      "7      35.13      34.98      35.22  ...             38.06   \n",
      "8      35.34      35.38      35.43  ...             37.79   \n",
      "9      35.84      35.76      35.85  ...             38.25   \n",
      "\n",
      "  canthiMax_2_Median_y T_FHC_Max_2_Mean_y T_FHC_Max_2_Min_y T_FHC_Max_2_Max_y  \\\n",
      "0                35.76          35.082021             32.16             36.71   \n",
      "1                35.48          34.871748             33.44             37.02   \n",
      "2                35.76          35.082021             32.16             36.71   \n",
      "3                35.48          34.871748             33.44             37.02   \n",
      "4                35.76          35.082021             32.16             36.71   \n",
      "5                35.76          35.082021             32.16             36.71   \n",
      "6                35.76          35.082021             32.16             36.71   \n",
      "7                35.76          35.082021             32.16             36.71   \n",
      "8                35.54          35.098804              32.3             37.38   \n",
      "9                35.76          35.126774             33.92             37.32   \n",
      "\n",
      "  T_FHC_Max_2_Median_y T_Max_2_Mean T_Max_2_Min T_Max_2_Max T_Max_2_Median  \n",
      "0                 35.1    36.033003        34.8        38.4          35.98  \n",
      "1                34.84    35.778252       34.93       38.01          35.71  \n",
      "2                 35.1    36.033003        34.8        38.4          35.98  \n",
      "3                34.84    35.778252       34.93       38.01          35.71  \n",
      "4                 35.1    36.033003        34.8        38.4          35.98  \n",
      "5                 35.1    36.033003        34.8        38.4          35.98  \n",
      "6                 35.1    36.033003        34.8        38.4          35.98  \n",
      "7                 35.1    36.033003        34.8        38.4          35.98  \n",
      "8                35.15    35.818921        34.7       37.79          35.77  \n",
      "9                35.09    36.092581       35.16       38.66          35.99  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... canthiMax_3_Max_y  \\\n",
      "0      35.01      34.56      35.05  ...             38.11   \n",
      "1      34.51      34.73      34.76  ...             37.94   \n",
      "2      35.68      35.68       35.7  ...             38.11   \n",
      "3      35.12      35.14      35.14  ...             37.94   \n",
      "4      35.59      35.63      35.69  ...             38.11   \n",
      "5      35.14      35.03      35.17  ...             38.11   \n",
      "6      35.34       35.2      35.38  ...             38.11   \n",
      "7      35.28      35.04      35.35  ...             38.11   \n",
      "8      35.21      35.32      35.33  ...             37.83   \n",
      "9      35.87       35.6      35.91  ...             38.32   \n",
      "\n",
      "  canthiMax_3_Median_y T_FHC_Max_3_Mean_y T_FHC_Max_3_Min_y T_FHC_Max_3_Max_y  \\\n",
      "0                 35.8          35.116992             32.32             37.41   \n",
      "1                35.54          34.896434             33.75              36.7   \n",
      "2                 35.8          35.116992             32.32             37.41   \n",
      "3                35.54          34.896434             33.75              36.7   \n",
      "4                 35.8          35.116992             32.32             37.41   \n",
      "5                 35.8          35.116992             32.32             37.41   \n",
      "6                 35.8          35.116992             32.32             37.41   \n",
      "7                 35.8          35.116992             32.32             37.41   \n",
      "8                35.57          35.112476             32.16             37.46   \n",
      "9                 35.8          35.116452             33.83             37.31   \n",
      "\n",
      "  T_FHC_Max_3_Median_y T_Max_3_Mean T_Max_3_Min T_Max_3_Max T_Max_3_Median  \n",
      "0                35.13    36.033684       34.86       38.11          35.99  \n",
      "1                34.88    35.818508       35.03       37.94          35.76  \n",
      "2                35.13    36.033684       34.86       38.11          35.99  \n",
      "3                34.88    35.818508       35.03       37.94          35.76  \n",
      "4                35.13    36.033684       34.86       38.11          35.99  \n",
      "5                35.13    36.033684       34.86       38.11          35.99  \n",
      "6                35.13    36.033684       34.86       38.11          35.99  \n",
      "7                35.13    36.033684       34.86       38.11          35.99  \n",
      "8                35.16    35.814515       34.81       37.83          35.78  \n",
      "9                35.12     36.10871       35.15       38.37          35.93  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... canthiMax_4_Max_y  \\\n",
      "0       35.0      34.97      35.03  ...              38.0   \n",
      "1      34.67      34.75      34.79  ...             37.77   \n",
      "2      35.69      35.75      35.78  ...              38.0   \n",
      "3      35.39      35.09      35.41  ...             37.77   \n",
      "4      35.67      35.56      35.69  ...              38.0   \n",
      "5      35.23      35.15      35.23  ...              38.0   \n",
      "6      35.37      35.31       35.4  ...              38.0   \n",
      "7      35.18      34.94      35.21  ...              38.0   \n",
      "8      35.28      35.36      35.38  ...             37.81   \n",
      "9      35.88       35.7      35.96  ...             38.37   \n",
      "\n",
      "  canthiMax_4_Median_y T_FHC_Max_4_Mean_y T_FHC_Max_4_Min_y T_FHC_Max_4_Max_y  \\\n",
      "0                35.82          35.094111              32.8             37.03   \n",
      "1                35.59          34.807044             33.61             36.55   \n",
      "2                35.82          35.094111              32.8             37.03   \n",
      "3                35.59          34.807044             33.61             36.55   \n",
      "4                35.82          35.094111              32.8             37.03   \n",
      "5                35.82          35.094111              32.8             37.03   \n",
      "6                35.82          35.094111              32.8             37.03   \n",
      "7                35.82          35.094111              32.8             37.03   \n",
      "8                35.69          35.100721             33.49             37.47   \n",
      "9                35.77          35.058387             33.54             37.15   \n",
      "\n",
      "  T_FHC_Max_4_Median_y T_Max_4_Mean T_Max_4_Min T_Max_4_Max T_Max_4_Median  \n",
      "0                35.07    36.568847       35.02       38.84          36.57  \n",
      "1                 34.8    36.532842       35.34       38.73           36.6  \n",
      "2                35.07    36.568847       35.02       38.84          36.57  \n",
      "3                 34.8    36.532842       35.34       38.73           36.6  \n",
      "4                35.07    36.568847       35.02       38.84          36.57  \n",
      "5                35.07    36.568847       35.02       38.84          36.57  \n",
      "6                35.07    36.568847       35.02       38.84          36.57  \n",
      "7                35.07    36.568847       35.02       38.84          36.57  \n",
      "8                35.09    36.491659       35.29       38.72           36.5  \n",
      "9                35.15    36.740323       35.87       39.68          36.69  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlated features with aveOralM in Round 1:\n",
      "Age                     0.101098\n",
      "Ethnicity_encoded       0.058819\n",
      "canthiMax_1_Median_x    0.019472\n",
      "T_LC_1_Min              0.019472\n",
      "T_LC_1_Median           0.019472\n",
      "T_LC_1_Max              0.019472\n",
      "T_FHCC_1_Max_x          0.019472\n",
      "canthiMax_1_Max_x       0.019472\n",
      "T_FHCC_1_Mean_x         0.019472\n",
      "T_FHC_Max_1_Max_x       0.019472\n",
      "Name: aveOralM, dtype: float64\n",
      "\n",
      "\n",
      "Top correlated features with aveOralM in Round 2:\n",
      "Age                     0.101098\n",
      "Ethnicity_encoded       0.058819\n",
      "T_FHC_Max_2_Mean_x      0.019472\n",
      "T_LC_2_Median           0.019472\n",
      "canthiMax_2_Mean_x      0.019472\n",
      "T_FHC_Max_2_Median_x    0.019472\n",
      "T_FHCC_2_Max_x          0.019472\n",
      "T_FHCC_2_Min_x          0.019472\n",
      "T_FHC_Max_2_Max_x       0.019472\n",
      "Gender_NEW              0.019472\n",
      "Name: aveOralM, dtype: float64\n",
      "\n",
      "\n",
      "Top correlated features with aveOralM in Round 3:\n",
      "Age                     0.101098\n",
      "Ethnicity_encoded       0.058819\n",
      "T_FHCC_3_Max_x          0.019472\n",
      "T_LC_3_Median           0.019472\n",
      "T_FHC_Max_3_Max_x       0.019472\n",
      "canthiMax_3_Median_x    0.019472\n",
      "T_FHC_Max_3_Median_x    0.019472\n",
      "canthiMax_3_Max_x       0.019472\n",
      "T_FHCC_3_Median_x       0.019472\n",
      "T_FHCC_3_Min_x          0.019472\n",
      "Name: aveOralM, dtype: float64\n",
      "\n",
      "\n",
      "Top correlated features with aveOralM in Round 4:\n",
      "Age                     0.101098\n",
      "Ethnicity_encoded       0.058819\n",
      "canthiMax_4_Median_x    0.019472\n",
      "T_FHCC_4_Max_x          0.019472\n",
      "T_FHC_Max_4_Mean_x      0.019472\n",
      "T_FHC_Max_4_Max_x       0.019472\n",
      "T_FHC_Max_4_Min_x       0.019472\n",
      "T_FHC_Max_4_Median_x    0.019472\n",
      "Gender_NEW              0.019472\n",
      "T_FHCC_4_Min_x          0.019472\n",
      "Name: aveOralM, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    if output_column in df.columns:\n",
    "        df[output_column] = pd.to_numeric(df[output_column], errors='coerce')\n",
    "\n",
    "        correlation_matrix = df.corr(method='pearson', numeric_only=True)\n",
    "        \n",
    "        if output_column in correlation_matrix.columns:\n",
    "            abs_correlation_with_output = correlation_matrix[output_column].abs().drop(output_column)\n",
    "            \n",
    "            sorted_features = abs_correlation_with_output.sort_values(ascending=False)\n",
    "            \n",
    "            print(f\"Top correlated features with {output_column} in {round_name}:\")\n",
    "            print(sorted_features.head(10))\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(f\"Could not find numeric output column '{output_column}' after conversion in {round_name}.\")\n",
    "    else:\n",
    "        print(f\"Output column '{output_column}' not found in {round_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features based on model performance in Round 1:\n",
      "T_Max_1: -0.10483862396263954\n",
      "canthi4Max_1: -0.11710294943721242\n",
      "canthiMax_1: -0.11727236113889214\n",
      "T_RC_Max_1: -0.12143886729186867\n",
      "T_RC_1: -0.12155499152375715\n",
      "T_LC_1: -0.126305934231814\n",
      "T_LC_Max_1: -0.12651702788044097\n",
      "T_RC_Dry_1: -0.1267557361833872\n",
      "T_LC_Dry_1: -0.12742628335030173\n",
      "T_RC_Wet_1: -0.13008928723610497\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define and train the model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 22\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Use the mean score across folds as the performance metric\u001b[39;00m\n\u001b[1;32m     25\u001b[0m feature_performance[feature] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(scores)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:714\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    712\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 714\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 425\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:912\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    909\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    911\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 912\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_params_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:977\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[1;32m    975\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:137\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _BaseScorer):\n\u001b[0;32m--> 137\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcached_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m         score \u001b[38;5;241m=\u001b[39m scorer(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mscore)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:345\u001b[0m, in \u001b[0;36m_Scorer._score\u001b[0;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_regressor(estimator) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pos_label()\n\u001b[1;32m    344\u001b[0m response_method \u001b[38;5;241m=\u001b[39m _check_response_method(estimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_method)\n\u001b[0;32m--> 345\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_caller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:87\u001b[0m, in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[0;32m---> 87\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_response_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_response.py:238\u001b[0m, in \u001b[0;36m_get_response_values\u001b[0;34m(estimator, X, response_method, pos_label, return_response_method_used)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should either be a classifier to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused with response_method=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or the response_method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got a regressor with response_method=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     prediction_method \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict\n\u001b[0;32m--> 238\u001b[0m     y_pred, pos_label \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_response_method_used:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, pos_label, prediction_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_base.py:286\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_base.py:269\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    267\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreal floating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplex floating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_array_api.py:328\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.isdtype\u001b[0;34m(self, dtype, kind)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype, kind):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43misdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_array_api.py:130\u001b[0m, in \u001b[0;36misdtype\u001b[0;34m(dtype, kind, xp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a boolean indicating whether a provided dtype is of type \"kind\".\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mIncluded in the v2022.12 of the Array API spec.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mhttps://data-apis.org/array-api/latest/API_specification/generated/array_api.isdtype.html\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kind, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(_isdtype_single(dtype, k, xp\u001b[38;5;241m=\u001b[39mxp) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kind)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isdtype_single(dtype, kind, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "D_prime = 10  \n",
    "feature_performance = {}\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    df[output_column] = pd.to_numeric(df[output_column], errors='coerce')\n",
    "    df = df.dropna(subset=[output_column])\n",
    "    \n",
    "    features = [col for col in df.columns if col not in [output_column, 'SubjectID',]]  \n",
    "    \n",
    "    for feature in features:\n",
    "        # Reshape data for sklearn model\n",
    "        X = df[[feature]].values.reshape(-1, 1)\n",
    "        y = df[output_column].values\n",
    "        \n",
    "        # Define and train the model\n",
    "        model = LinearRegression()\n",
    "        scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        # Use the mean score across folds as the performance metric\n",
    "        feature_performance[feature] = np.mean(scores)\n",
    "    \n",
    "    # Selecting the top D_prime features based on their performance\n",
    "    top_features = sorted(feature_performance, key=feature_performance.get, reverse=True)[:D_prime]\n",
    "    \n",
    "    print(f\"Top {D_prime} features based on model performance in {round_name}:\")\n",
    "    for feature in top_features:\n",
    "        print(f\"{feature}: {feature_performance[feature]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Selected Features: ['Age', 'T_LC_1_Mean', 'T_LC_1_Max', 'T_LC_1_Median', 'canthiMax_1_Min_x', 'T_FHC_Max_1_Min_x', 'T_FHC_Max_1_Median_x', 'T_FHCC_1_Mean_x', 'T_FHCC_1_Min_x', 'T_FHCC_1_Max_x']\n",
      "Round Round 2: Selected Features: ['Age', 'T_LC_2_Min', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'canthiMax_2_Max_x', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Max_x', 'T_FHC_Max_2_Median_x', 'T_FHCC_2_Min_x']\n",
      "Round Round 3: Selected Features: ['Age', 'T_LC_3_Min', 'T_LC_3_Max', 'canthiMax_3_Mean_x', 'canthiMax_3_Min_x', 'canthiMax_3_Max_x', 'canthiMax_3_Median_x', 'T_FHC_Max_3_Median_x', 'T_FHCC_3_Mean_x', 'T_FHCC_3_Median_x']\n",
      "Round Round 4: Selected Features: ['Age', 'T_LC_4_Min', 'T_LC_4_Median', 'canthiMax_4_Mean_x', 'canthiMax_4_Min_x', 'T_FHC_Max_4_Min_x', 'T_FHC_Max_4_Max_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Min_x', 'T_FHCC_4_Max_x']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    df[output_column] = pd.to_numeric(df[output_column], errors='coerce').dropna()\n",
    "    \n",
    "    X = df.select_dtypes(include=[np.number]).drop(columns=[output_column])  \n",
    "    y = df[output_column]\n",
    "    \n",
    "    model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "    \n",
    "    sfs = SequentialFeatureSelector(model,\n",
    "                                    n_features_to_select=10,  \n",
    "                                    direction='forward',\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=5)\n",
    "    \n",
    "    sfs.fit(X, y)\n",
    "    selected_features = X.columns[sfs.get_support()]\n",
    "    \n",
    "    print(f\"Round {round_name}: Selected Features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Selected Features + 'SubjectID': ['SubjectID', 'Age', 'T_LC_1_Mean', 'T_LC_1_Max', 'T_LC_1_Median', 'canthiMax_1_Min_x', 'T_FHC_Max_1_Min_x', 'T_FHC_Max_1_Median_x', 'T_FHCC_1_Mean_x', 'T_FHCC_1_Min_x', 'T_FHCC_1_Max_x']\n",
      "Round Round 2: Selected Features + 'SubjectID': ['SubjectID', 'Age', 'T_LC_2_Min', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'canthiMax_2_Max_x', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Max_x', 'T_FHC_Max_2_Median_x', 'T_FHCC_2_Min_x']\n",
      "Round Round 3: Selected Features + 'SubjectID': ['SubjectID', 'Age', 'T_LC_3_Min', 'T_LC_3_Max', 'canthiMax_3_Mean_x', 'canthiMax_3_Min_x', 'canthiMax_3_Max_x', 'canthiMax_3_Median_x', 'T_FHC_Max_3_Median_x', 'T_FHCC_3_Mean_x', 'T_FHCC_3_Median_x']\n",
      "Round Round 4: Selected Features + 'SubjectID': ['SubjectID', 'Age', 'T_LC_4_Min', 'T_LC_4_Median', 'canthiMax_4_Mean_x', 'canthiMax_4_Min_x', 'T_FHC_Max_4_Min_x', 'T_FHC_Max_4_Max_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Min_x', 'T_FHCC_4_Max_x']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Updated round_dfs with only selected features\n",
    "updated_round_dfs = {}\n",
    "\n",
    "for round_name, df in round_dfs.items():\n",
    "    # Ensure output column is numeric and drop any rows with NaN in it\n",
    "    df_clean = df.dropna(subset=[output_column])\n",
    "    df_clean[output_column] = pd.to_numeric(df_clean[output_column], errors='coerce')\n",
    "    \n",
    "    # Select features and target variable, exclude 'SubjectID' and non-numeric columns for the feature selection\n",
    "    X = df_clean.select_dtypes(include=[np.number]).drop(columns=[output_column])\n",
    "    y = df_clean[output_column]\n",
    "    \n",
    "    # Standardize features and use Linear Regression as the model\n",
    "    model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "    \n",
    "    # Sequential Forward Selection\n",
    "    sfs = SequentialFeatureSelector(model, \n",
    "                                    n_features_to_select=10,  # Selecting top 10 features\n",
    "                                    direction='forward',\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=5)\n",
    "    \n",
    "    # Fit SFS and get the selected features\n",
    "    sfs.fit(X, y)\n",
    "    selected_features = X.columns[sfs.get_support()].tolist()\n",
    "    \n",
    "    # Update the DataFrame to only include selected features + 'SubjectID' + output column\n",
    "    updated_df = df_clean[['SubjectID'] + selected_features + [output_column]]\n",
    "    \n",
    "    # Update the dictionary with the new DataFrame\n",
    "    updated_round_dfs[round_name] = updated_df\n",
    "\n",
    "    print(f\"Round {round_name}: Selected Features + 'SubjectID':\", ['SubjectID'] + selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1:\n",
      "  SubjectID  Age  T_LC_1_Mean  T_LC_1_Max  T_LC_1_Median  canthiMax_1_Min_x  \\\n",
      "0  161117-1    6    35.621748       38.16      35.594601              34.30   \n",
      "1  161117-2    5    35.456819       38.00      35.450000              33.85   \n",
      "2  161117-3    3    35.456819       38.00      35.450000              33.85   \n",
      "3  161117-4    3    35.456819       38.00      35.450000              33.85   \n",
      "4  161117-5    1    35.621748       38.16      35.594601              34.30   \n",
      "\n",
      "   T_FHC_Max_1_Min_x  T_FHC_Max_1_Median_x  T_FHCC_1_Mean_x  T_FHCC_1_Min_x  \\\n",
      "0              31.63             35.032753        34.454314           30.93   \n",
      "1              31.70             34.945000        34.329620           30.02   \n",
      "2              31.70             34.945000        34.329620           30.02   \n",
      "3              31.70             34.945000        34.329620           30.02   \n",
      "4              31.63             35.032753        34.454314           30.93   \n",
      "\n",
      "   T_FHCC_1_Max_x  aveOralM  \n",
      "0           36.77     36.59  \n",
      "1           36.36     37.19  \n",
      "2           36.36     37.34  \n",
      "3           36.36     37.09  \n",
      "4           36.77     37.04  \n",
      "\n",
      "\n",
      "Round 2:\n",
      "  SubjectID  Age  T_LC_2_Min  T_LC_2_Max  canthiMax_2_Mean_x  \\\n",
      "0  161117-1    6       34.42       37.92           35.846630   \n",
      "1  161117-2    5       34.11       38.05           35.662038   \n",
      "2  161117-3    3       34.11       38.05           35.662038   \n",
      "3  161117-4    3       34.11       38.05           35.662038   \n",
      "4  161117-5    1       34.42       37.92           35.846630   \n",
      "\n",
      "   canthiMax_2_Min_x  canthiMax_2_Max_x  T_FHC_Max_2_Mean_x  \\\n",
      "0              34.76              38.25           35.096935   \n",
      "1              34.14              38.06           35.030533   \n",
      "2              34.14              38.06           35.030533   \n",
      "3              34.14              38.06           35.030533   \n",
      "4              34.76              38.25           35.096935   \n",
      "\n",
      "   T_FHC_Max_2_Max_x  T_FHC_Max_2_Median_x  T_FHCC_2_Min_x  aveOralM  \n",
      "0              37.32                35.125           32.31     36.59  \n",
      "1              37.38                35.050           30.24     37.19  \n",
      "2              37.38                35.050           30.24     37.34  \n",
      "3              37.38                35.050           30.24     37.09  \n",
      "4              37.32                35.125           32.31     37.04  \n",
      "\n",
      "\n",
      "Round 3:\n",
      "  SubjectID  Age  T_LC_3_Min  T_LC_3_Max  canthiMax_3_Mean_x  \\\n",
      "0  161117-1    6       34.60       38.03           35.876759   \n",
      "1  161117-2    5       34.05       38.07           35.682364   \n",
      "2  161117-3    3       34.05       38.07           35.682364   \n",
      "3  161117-4    3       34.05       38.07           35.682364   \n",
      "4  161117-5    1       34.60       38.03           35.876759   \n",
      "\n",
      "   canthiMax_3_Min_x  canthiMax_3_Max_x  canthiMax_3_Median_x  \\\n",
      "0              34.83              38.32                 35.80   \n",
      "1              34.06              38.11                 35.65   \n",
      "2              34.06              38.11                 35.65   \n",
      "3              34.06              38.11                 35.65   \n",
      "4              34.83              38.32                 35.80   \n",
      "\n",
      "   T_FHC_Max_3_Median_x  T_FHCC_3_Mean_x  T_FHCC_3_Median_x  aveOralM  \n",
      "0                35.175        34.659690          34.710000     36.59  \n",
      "1                35.060        34.501906          34.520136     37.19  \n",
      "2                35.060        34.501906          34.520136     37.34  \n",
      "3                35.060        34.501906          34.520136     37.09  \n",
      "4                35.175        34.659690          34.710000     37.04  \n",
      "\n",
      "\n",
      "Round 4:\n",
      "  SubjectID  Age  T_LC_4_Min  T_LC_4_Median  canthiMax_4_Mean_x  \\\n",
      "0  161117-1    6       34.64         35.675           35.913429   \n",
      "1  161117-2    5       34.33         35.550           35.725187   \n",
      "2  161117-3    3       34.33         35.550           35.725187   \n",
      "3  161117-4    3       34.33         35.550           35.725187   \n",
      "4  161117-5    1       34.64         35.675           35.913429   \n",
      "\n",
      "   canthiMax_4_Min_x  T_FHC_Max_4_Min_x  T_FHC_Max_4_Max_x  T_FHCC_4_Mean_x  \\\n",
      "0              34.87              33.15              37.15        34.672317   \n",
      "1              34.53              32.80              37.47        34.529527   \n",
      "2              34.53              32.80              37.47        34.529527   \n",
      "3              34.53              32.80              37.47        34.529527   \n",
      "4              34.87              33.15              37.15        34.672317   \n",
      "\n",
      "   T_FHCC_4_Min_x  T_FHCC_4_Max_x  aveOralM  \n",
      "0           32.63           36.77     36.59  \n",
      "1           30.53           36.62     37.19  \n",
      "2           30.53           36.62     37.34  \n",
      "3           30.53           36.62     37.09  \n",
      "4           32.63           36.77     37.04  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df in updated_round_dfs.items():\n",
    "    print(f\"{round_name}:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = train_data.isnull().sum()\n",
    "\n",
    "columns_to_drop = null_counts[null_counts == len(train_data)].index\n",
    "data1 = test_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_feature_start_index = len(columns) - 7  \n",
    "first_constant_feature = data.columns[0]\n",
    "\n",
    "round_indices = {f\"Round {i+1}\": None for i in range(4)}  \n",
    "\n",
    "for col in columns[1:]:  \n",
    "    for round_name in round_indices.keys():\n",
    "        if round_name in col and round_indices[round_name] is None:\n",
    "            round_indices[round_name] = columns.index(col)\n",
    "            break\n",
    "\n",
    "round_dfs1 = {}\n",
    "for i in range(1, 5):\n",
    "    round_name = f\"Round {i}\"\n",
    "    if i < 4:\n",
    "        next_round_start = round_indices[f\"Round {i+1}\"]\n",
    "    else:\n",
    "        next_round_start = constant_feature_start_index\n",
    "    round_columns = columns[round_indices[round_name]:next_round_start]\n",
    "    \n",
    "    round_dfs1[round_name] = data1[[first_constant_feature] + round_columns + columns[-7:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    df_round = df_round.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    new_header = df_round.iloc[0] \n",
    "    df_round = df_round[1:] \n",
    "    df_round.columns = new_header  \n",
    "    df_round.reset_index(drop=True, inplace=True)  \n",
    "    \n",
    "    round_dfs1[round_name] = df_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Shape: (310, 35)\n",
      "Columns: ['SubjectID', 'T_offset_1', 'Max1R13_1', 'Max1L13_1', 'aveAllR13_1', 'aveAllL13_1', 'T_RC_1', 'T_RC_Dry_1', 'T_RC_Wet_1', 'T_RC_Max_1', 'T_LC_1', 'T_LC_Dry_1', 'T_LC_Wet_1', 'T_LC_Max_1', 'RCC_1', 'LCC_1', 'canthiMax_1', 'canthi4Max_1', 'T_FHCC_1', 'T_FHRC_1', 'T_FHLC_1', 'T_FHBC_1', 'T_FHTC_1', 'T_FH_Max_1', 'T_FHC_Max_1', 'T_Max_1', 'T_OR_1', 'T_OR_Max_1', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n",
      "Round 2\n",
      "Shape: (310, 35)\n",
      "Columns: ['SubjectID', 'T_offset_2', 'Max1R13_2', 'Max1L13_2', 'aveAllR13_2', 'aveAllL13_2', 'T_RC_2', 'T_RC_Dry_2', 'T_RC_Wet_2', 'T_RC_Max_2', 'T_LC_2', 'T_LC_Dry_2', 'T_LC_Wet_2', 'T_LC_Max_2', 'RCC_2', 'LCC_2', 'canthiMax_2', 'canthi4Max_2', 'T_FHCC_2', 'T_FHRC_2', 'T_FHLC_2', 'T_FHBC_2', 'T_FHTC_2', 'T_FH_Max_2', 'T_FHC_Max_2', 'T_Max_2', 'T_OR_2', 'T_OR_Max_2', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n",
      "Round 3\n",
      "Shape: (310, 35)\n",
      "Columns: ['SubjectID', 'T_offset_3', 'Max1R13_3', 'Max1L13_3', 'aveAllR13_3', 'aveAllL13_3', 'T_RC_3', 'T_RC_Dry_3', 'T_RC_Wet_3', 'T_RC_Max_3', 'T_LC_3', 'T_LC_Dry_3', 'T_LC_Wet_3', 'T_LC_Max_3', 'RCC_3', 'LCC_3', 'canthiMax_3', 'canthi4Max_3', 'T_FHCC_3', 'T_FHRC_3', 'T_FHLC_3', 'T_FHBC_3', 'T_FHTC_3', 'T_FH_Max_3', 'T_FHC_Max_3', 'T_Max_3', 'T_OR_3', 'T_OR_Max_3', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n",
      "Round 4\n",
      "Shape: (310, 35)\n",
      "Columns: ['SubjectID', 'T_offset_4', 'Max1R13_4', 'Max1L13_4', 'aveAllR13_4', 'aveAllL13_4', 'T_RC_4', 'T_RC_Dry_4', 'T_RC_Wet_4', 'T_RC_Max_4', 'T_LC_4', 'T_LC_Dry_4', 'T_LC_Wet_4', 'T_LC_Max_4', 'RCC_4', 'LCC_4', 'canthiMax_4', 'canthi4Max_4', 'T_FHCC_4', 'T_FHRC_4', 'T_FHLC_4', 'T_FHBC_4', 'T_FHTC_4', 'T_FH_Max_4', 'T_FHC_Max_4', 'T_Max_4', 'T_OR_4', 'T_OR_Max_4', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name}\")\n",
    "    print(f\"Shape: {df_round.shape}\")  \n",
    "    print(\"Columns:\", df_round.columns.tolist())  \n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = 35 \n",
    "column_averages = []\n",
    "\n",
    "for col_index in range(num_columns):\n",
    "    column_values = []\n",
    "    \n",
    "    for df_round in round_dfs1.values():\n",
    "        numeric_values = pd.to_numeric(df_round.iloc[:, col_index], errors='coerce') \n",
    "        column_values.extend(numeric_values.dropna().values) \n",
    "\n",
    "    if column_values:\n",
    "        column_averages.append(np.mean(column_values))\n",
    "    else:\n",
    "        column_averages.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    for col_index in range(num_columns):\n",
    "        if not np.isnan(column_averages[col_index]):\n",
    "            numeric_values = pd.to_numeric(df_round.iloc[:, col_index], errors='coerce')\n",
    "            df_round.iloc[:, col_index] = numeric_values.fillna(column_averages[col_index], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "SubjectID               0\n",
      "T_offset_1              0\n",
      "Max1R13_1               0\n",
      "Max1L13_1               0\n",
      "aveAllR13_1             0\n",
      "                       ..\n",
      "T_FHC_Max_1_Median_y    0\n",
      "T_Max_1_Mean            0\n",
      "T_Max_1_Min             0\n",
      "T_Max_1_Max             0\n",
      "T_Max_1_Median          0\n",
      "Length: 67, dtype: int64\n",
      "\n",
      "\n",
      "Round 2 \n",
      "SubjectID               0\n",
      "T_offset_2              0\n",
      "Max1R13_2               0\n",
      "Max1L13_2               0\n",
      "aveAllR13_2             0\n",
      "                       ..\n",
      "T_FHC_Max_2_Median_y    0\n",
      "T_Max_2_Mean            0\n",
      "T_Max_2_Min             0\n",
      "T_Max_2_Max             0\n",
      "T_Max_2_Median          0\n",
      "Length: 67, dtype: int64\n",
      "\n",
      "\n",
      "Round 3 \n",
      "SubjectID               0\n",
      "T_offset_3              0\n",
      "Max1R13_3               0\n",
      "Max1L13_3               0\n",
      "aveAllR13_3             0\n",
      "                       ..\n",
      "T_FHC_Max_3_Median_y    0\n",
      "T_Max_3_Mean            0\n",
      "T_Max_3_Min             0\n",
      "T_Max_3_Max             0\n",
      "T_Max_3_Median          0\n",
      "Length: 67, dtype: int64\n",
      "\n",
      "\n",
      "Round 4 \n",
      "SubjectID               0\n",
      "T_offset_4              0\n",
      "Max1R13_4               0\n",
      "Max1L13_4               0\n",
      "aveAllR13_4             0\n",
      "                       ..\n",
      "T_FHC_Max_4_Median_y    0\n",
      "T_Max_4_Mean            0\n",
      "T_Max_4_Min             0\n",
      "T_Max_4_Max             0\n",
      "T_Max_4_Median          0\n",
      "Length: 67, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.isnull().sum()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='if_binary')\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    gender_encoded = encoder.fit_transform(df[['Gender']])\n",
    "    \n",
    "    gender_encoded_df = pd.DataFrame(gender_encoded, index=df.index, columns=['Gender_NEW'])\n",
    "    \n",
    "    df.drop('Gender', axis=1, inplace=True)\n",
    "    round_dfs1[round_name] = pd.concat([df, gender_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "   SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  180208-10        1.0     35.62     35.39       35.33       35.07  35.59   \n",
      "1  180209-01       0.44     36.21     36.27       35.47       35.86  36.19   \n",
      "2  180209-02       0.65     37.68     37.16       37.06       36.52  37.71   \n",
      "3  180209-03       0.91     35.28     35.69       34.89       34.81  35.26   \n",
      "4  180209-05       0.81     34.19     34.46       33.64       33.87  34.33   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_Max_1 T_OR_1 T_OR_Max_1    Age  \\\n",
      "0      35.59      35.44      35.62  ...   35.62   35.3       35.4  18-20   \n",
      "1      36.19      35.87      36.21  ...    36.4  36.36       36.4  18-20   \n",
      "2       37.6      37.69      37.73  ...   37.73  37.25      37.31  18-20   \n",
      "3      35.26      34.94      35.28  ...   35.69  35.42      35.49  21-25   \n",
      "4      34.33      33.98      34.37  ...   34.98  34.89      34.91  41-50   \n",
      "\n",
      "  Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0     Asian  22.0     30.0      0.6    36.74        0.0  \n",
      "1     White  24.1     15.6     0.62    37.69        0.0  \n",
      "2     Asian  24.1     15.6     0.62    39.34        1.0  \n",
      "3     Asian  24.1     15.6     0.66    36.99        1.0  \n",
      "4     White  24.1     18.0      0.6    36.84        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "   SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  180208-10       1.05      35.6      35.3       35.25       34.95   35.6   \n",
      "1  180209-01       0.56     36.24     36.18       35.81       35.85  36.22   \n",
      "2  180209-02       0.72     37.57     37.21       37.22       36.73  37.71   \n",
      "3  180209-03       0.85     35.48     35.47       35.09       35.04  35.52   \n",
      "4  180209-05       0.91     34.68     34.69       34.31       34.24  34.71   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_Max_2 T_OR_2 T_OR_Max_2    Age  \\\n",
      "0       35.6      35.45       35.6  ...    36.0  35.96       36.0  18-20   \n",
      "1      36.22      35.63      36.24  ...   36.59  36.53      36.59  18-20   \n",
      "2      37.56      37.71      37.75  ...   37.75  37.42      37.46  18-20   \n",
      "3      35.52      35.29      35.54  ...    36.0  35.52      35.65  21-25   \n",
      "4      34.71      34.51      34.72  ...   35.36  35.18      35.19  41-50   \n",
      "\n",
      "  Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0     Asian  22.0     30.0      0.6    36.74        0.0  \n",
      "1     White  24.1     15.6     0.62    37.69        0.0  \n",
      "2     Asian  24.1     15.6     0.62    39.34        1.0  \n",
      "3     Asian  24.1     15.6     0.66    36.99        1.0  \n",
      "4     White  24.1     18.0      0.6    36.84        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "   SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  180208-10       1.14     35.55      35.3       35.27       35.01  35.54   \n",
      "1  180209-01       0.64     35.88     36.23       35.35       35.64  36.22   \n",
      "2  180209-02       0.79      37.6     36.97       37.22       36.25  37.64   \n",
      "3  180209-03        0.8     35.63     35.75       35.09        35.2  35.61   \n",
      "4  180209-05        0.9     35.08     34.78       34.22       34.32  35.01   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_Max_3 T_OR_3 T_OR_Max_3    Age  \\\n",
      "0      35.54      35.33      35.55  ...   36.05  36.03      36.05  18-20   \n",
      "1      36.22      35.29      36.23  ...   36.68  36.63      36.68  18-20   \n",
      "2      37.57      37.64       37.7  ...    37.7  37.04      37.09  18-20   \n",
      "3      35.61      35.42      35.63  ...   36.03  35.95      36.03  21-25   \n",
      "4      35.01      34.57      35.08  ...   35.51  35.22      35.25  41-50   \n",
      "\n",
      "  Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0     Asian  22.0     30.0      0.6    36.74        0.0  \n",
      "1     White  24.1     15.6     0.62    37.69        0.0  \n",
      "2     Asian  24.1     15.6     0.62    39.34        1.0  \n",
      "3     Asian  24.1     15.6     0.66    36.99        1.0  \n",
      "4     White  24.1     18.0      0.6    36.84        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "   SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  180208-10       1.08     35.63     35.34       35.37       34.98  35.61   \n",
      "1  180209-01       0.58     36.12     36.27       35.51       35.89  36.12   \n",
      "2  180209-02       0.91     37.63     37.18       37.15       36.62  37.72   \n",
      "3  180209-03       0.84     35.57      35.6       34.39        34.8  35.57   \n",
      "4  180209-05       0.89     34.99     34.71       34.24       34.31  34.94   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_Max_4 T_OR_4 T_OR_Max_4    Age  \\\n",
      "0      35.61      35.36      35.63  ...   36.62  36.59      36.62  18-20   \n",
      "1      36.12      35.76      36.12  ...   37.21  37.19      37.21  18-20   \n",
      "2      37.54      37.72      37.78  ...   38.72  38.67      38.72  18-20   \n",
      "3      35.54      35.55      35.62  ...   36.71   36.7      36.71  21-25   \n",
      "4      34.94      34.45      34.99  ...   36.36  36.29      36.36  41-50   \n",
      "\n",
      "  Ethnicity T_atm Humidity Distance aveOralM Gender_NEW  \n",
      "0     Asian  22.0     30.0      0.6    36.74        0.0  \n",
      "1     White  24.1     15.6     0.62    37.69        0.0  \n",
      "2     Asian  24.1     15.6     0.62    39.34        1.0  \n",
      "3     Asian  24.1     15.6     0.66    36.99        1.0  \n",
      "4     White  24.1     18.0      0.6    36.84        1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories=[['White', 'Black or African-American', 'Asian', 'Multiracial']], handle_unknown='ignore')\n",
    "\n",
    "ethnicity_mapping = {\n",
    "    'White': 1,\n",
    "    'Black or African-American': 2,\n",
    "    'Asian': 3,\n",
    "    'Multiracial': 4\n",
    "    'Hispanic/Latino'  :5\n",
    "}\n",
    "\n",
    "inverse_ethnicity_mapping = {v: k for k, v in ethnicity_mapping.items()}\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    if 'Ethnicity' in df.columns:\n",
    "\n",
    "        ethnicity_encoded = encoder.fit_transform(df[['Ethnicity']])\n",
    "        \n",
    "        ethnicity_encoded_df = pd.DataFrame(ethnicity_encoded, columns=encoder.get_feature_names_out(['Ethnicity']))\n",
    "        \n",
    "        df['Ethnicity_encoded'] = np.argmax(ethnicity_encoded, axis=1) \n",
    "        \n",
    "        df.drop('Ethnicity', axis=1, inplace=True)\n",
    "    \n",
    "        round_dfs1[round_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "   SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  180208-10        1.0     35.62     35.39       35.33       35.07  35.59   \n",
      "1  180209-01       0.44     36.21     36.27       35.47       35.86  36.19   \n",
      "2  180209-02       0.65     37.68     37.16       37.06       36.52  37.71   \n",
      "3  180209-03       0.91     35.28     35.69       34.89       34.81  35.26   \n",
      "4  180209-05       0.81     34.19     34.46       33.64       33.87  34.33   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_Max_1 T_OR_1 T_OR_Max_1    Age  \\\n",
      "0      35.59      35.44      35.62  ...   35.62   35.3       35.4  18-20   \n",
      "1      36.19      35.87      36.21  ...    36.4  36.36       36.4  18-20   \n",
      "2       37.6      37.69      37.73  ...   37.73  37.25      37.31  18-20   \n",
      "3      35.26      34.94      35.28  ...   35.69  35.42      35.49  21-25   \n",
      "4      34.33      33.98      34.37  ...   34.98  34.89      34.91  41-50   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  22.0     30.0      0.6    36.74        0.0                 2  \n",
      "1  24.1     15.6     0.62    37.69        0.0                 0  \n",
      "2  24.1     15.6     0.62    39.34        1.0                 2  \n",
      "3  24.1     15.6     0.66    36.99        1.0                 2  \n",
      "4  24.1     18.0      0.6    36.84        1.0                 0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "   SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  180208-10       1.05      35.6      35.3       35.25       34.95   35.6   \n",
      "1  180209-01       0.56     36.24     36.18       35.81       35.85  36.22   \n",
      "2  180209-02       0.72     37.57     37.21       37.22       36.73  37.71   \n",
      "3  180209-03       0.85     35.48     35.47       35.09       35.04  35.52   \n",
      "4  180209-05       0.91     34.68     34.69       34.31       34.24  34.71   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_Max_2 T_OR_2 T_OR_Max_2    Age  \\\n",
      "0       35.6      35.45       35.6  ...    36.0  35.96       36.0  18-20   \n",
      "1      36.22      35.63      36.24  ...   36.59  36.53      36.59  18-20   \n",
      "2      37.56      37.71      37.75  ...   37.75  37.42      37.46  18-20   \n",
      "3      35.52      35.29      35.54  ...    36.0  35.52      35.65  21-25   \n",
      "4      34.71      34.51      34.72  ...   35.36  35.18      35.19  41-50   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  22.0     30.0      0.6    36.74        0.0                 2  \n",
      "1  24.1     15.6     0.62    37.69        0.0                 0  \n",
      "2  24.1     15.6     0.62    39.34        1.0                 2  \n",
      "3  24.1     15.6     0.66    36.99        1.0                 2  \n",
      "4  24.1     18.0      0.6    36.84        1.0                 0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "   SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  180208-10       1.14     35.55      35.3       35.27       35.01  35.54   \n",
      "1  180209-01       0.64     35.88     36.23       35.35       35.64  36.22   \n",
      "2  180209-02       0.79      37.6     36.97       37.22       36.25  37.64   \n",
      "3  180209-03        0.8     35.63     35.75       35.09        35.2  35.61   \n",
      "4  180209-05        0.9     35.08     34.78       34.22       34.32  35.01   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_Max_3 T_OR_3 T_OR_Max_3    Age  \\\n",
      "0      35.54      35.33      35.55  ...   36.05  36.03      36.05  18-20   \n",
      "1      36.22      35.29      36.23  ...   36.68  36.63      36.68  18-20   \n",
      "2      37.57      37.64       37.7  ...    37.7  37.04      37.09  18-20   \n",
      "3      35.61      35.42      35.63  ...   36.03  35.95      36.03  21-25   \n",
      "4      35.01      34.57      35.08  ...   35.51  35.22      35.25  41-50   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  22.0     30.0      0.6    36.74        0.0                 2  \n",
      "1  24.1     15.6     0.62    37.69        0.0                 0  \n",
      "2  24.1     15.6     0.62    39.34        1.0                 2  \n",
      "3  24.1     15.6     0.66    36.99        1.0                 2  \n",
      "4  24.1     18.0      0.6    36.84        1.0                 0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "   SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  180208-10       1.08     35.63     35.34       35.37       34.98  35.61   \n",
      "1  180209-01       0.58     36.12     36.27       35.51       35.89  36.12   \n",
      "2  180209-02       0.91     37.63     37.18       37.15       36.62  37.72   \n",
      "3  180209-03       0.84     35.57      35.6       34.39        34.8  35.57   \n",
      "4  180209-05       0.89     34.99     34.71       34.24       34.31  34.94   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_Max_4 T_OR_4 T_OR_Max_4    Age  \\\n",
      "0      35.61      35.36      35.63  ...   36.62  36.59      36.62  18-20   \n",
      "1      36.12      35.76      36.12  ...   37.21  37.19      37.21  18-20   \n",
      "2      37.54      37.72      37.78  ...   38.72  38.67      38.72  18-20   \n",
      "3      35.54      35.55      35.62  ...   36.71   36.7      36.71  21-25   \n",
      "4      34.94      34.45      34.99  ...   36.36  36.29      36.36  41-50   \n",
      "\n",
      "  T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded  \n",
      "0  22.0     30.0      0.6    36.74        0.0                 2  \n",
      "1  24.1     15.6     0.62    37.69        0.0                 0  \n",
      "2  24.1     15.6     0.62    39.34        1.0                 2  \n",
      "3  24.1     15.6     0.66    36.99        1.0                 2  \n",
      "4  24.1     18.0      0.6    36.84        1.0                 0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head()) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "\n",
    "for round_name, df_round in round_dfs1.items():\n",
    "    if 'Age' in df_round.columns:\n",
    "    \n",
    "        age_reshaped = df_round['Age'].values.reshape(-1, 1)\n",
    "        age_encoded = encoder.fit_transform(age_reshaped)\n",
    "        \n",
    "        df_round['Age_encoded'] = np.argmax(age_encoded, axis=1) + 1\n",
    "        \n",
    "        df_round.drop('Age', axis=1, inplace=True)\n",
    "        \n",
    "        df_round.rename(columns={'Age_encoded': 'Age'}, inplace=True)\n",
    "\n",
    "        round_dfs1[round_name] = df_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "   SubjectID T_offset_1  Max1R13_1  Max1L13_1 aveAllR13_1 aveAllL13_1  \\\n",
      "0  180208-10        1.0      35.62      35.39       35.33       35.07   \n",
      "1  180209-01       0.44      36.21      36.27       35.47       35.86   \n",
      "2  180209-02       0.65      37.68      37.16       37.06       36.52   \n",
      "3  180209-03       0.91      35.28      35.69       34.89       34.81   \n",
      "4  180209-05       0.81      34.19      34.46       33.64       33.87   \n",
      "5  180209-06        0.8      35.98      35.86       35.18       35.19   \n",
      "6  180209-07       0.86      35.16      35.88       34.54       35.41   \n",
      "7  180209-08       0.78      35.48      35.59       33.88       34.47   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10        0.9      35.44      35.31       34.84       34.66   \n",
      "\n",
      "      T_RC_1 T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ...    T_Max_1     T_OR_1  \\\n",
      "0      35.59      35.59      35.44      35.62  ...      35.62       35.3   \n",
      "1      36.19      36.19      35.87      36.21  ...       36.4      36.36   \n",
      "2      37.71       37.6      37.69      37.73  ...      37.73      37.25   \n",
      "3      35.26      35.26      34.94      35.28  ...      35.69      35.42   \n",
      "4      34.33      34.33      33.98      34.37  ...      34.98      34.89   \n",
      "5      36.05      35.93      36.05      36.07  ...      36.23       35.9   \n",
      "6      35.47      35.17      35.47      35.47  ...      35.94      35.78   \n",
      "7      35.51      35.41      35.51      35.56  ...      35.64      34.55   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...  36.154574  35.864123   \n",
      "9      35.59      35.59      34.66      35.59  ...      35.59      34.84   \n",
      "\n",
      "  T_OR_Max_1 T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0       35.4  22.0     30.0      0.6    36.74        0.0                 2   1  \n",
      "1       36.4  24.1     15.6     0.62    37.69        0.0                 0   1  \n",
      "2      37.31  24.1     15.6     0.62    39.34        1.0                 2   1  \n",
      "3      35.49  24.1     15.6     0.66    36.99        1.0                 2   2  \n",
      "4      34.91  24.1     18.0      0.6    36.84        1.0                 0   6  \n",
      "5      35.98  24.1     18.5      0.6    37.39        0.0                 0   2  \n",
      "6      35.81  24.3     18.8      0.6    37.14        1.0                 3   7  \n",
      "7      34.59  24.2     18.1      0.6    36.89        0.0                 0   2  \n",
      "8  35.895934  24.0     19.4      0.7    36.69        1.0                 0   2  \n",
      "9      34.86  24.0     19.8      0.7    36.89        0.0                 0   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "   SubjectID T_offset_2  Max1R13_2  Max1L13_2 aveAllR13_2 aveAllL13_2  \\\n",
      "0  180208-10       1.05       35.6       35.3       35.25       34.95   \n",
      "1  180209-01       0.56      36.24      36.18       35.81       35.85   \n",
      "2  180209-02       0.72      37.57      37.21       37.22       36.73   \n",
      "3  180209-03       0.85      35.48      35.47       35.09       35.04   \n",
      "4  180209-05       0.91      34.68      34.69       34.31       34.24   \n",
      "5  180209-06       0.88      35.74      35.61       34.82       34.92   \n",
      "6  180209-07       0.95      35.72      36.04       35.35       35.27   \n",
      "7  180209-08       0.83      35.65      35.61       34.13       34.46   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10       0.91      35.72      35.51       35.06       34.96   \n",
      "\n",
      "      T_RC_2 T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ...    T_Max_2     T_OR_2  \\\n",
      "0       35.6       35.6      35.45       35.6  ...       36.0      35.96   \n",
      "1      36.22      36.22      35.63      36.24  ...      36.59      36.53   \n",
      "2      37.71      37.56      37.71      37.75  ...      37.75      37.42   \n",
      "3      35.52      35.52      35.29      35.54  ...       36.0      35.52   \n",
      "4      34.71      34.71      34.51      34.72  ...      35.36      35.18   \n",
      "5      35.86      35.78      35.86      35.89  ...      36.02      35.84   \n",
      "6      35.73      35.71      35.73      35.76  ...      36.14      36.12   \n",
      "7      35.63       35.6      35.62      35.65  ...      35.65      35.23   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...  36.154574  35.864123   \n",
      "9       35.7       35.7      35.44      35.72  ...      35.79      35.76   \n",
      "\n",
      "  T_OR_Max_2 T_atm Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0       36.0  22.0     30.0      0.6    36.74        0.0                 2   1  \n",
      "1      36.59  24.1     15.6     0.62    37.69        0.0                 0   1  \n",
      "2      37.46  24.1     15.6     0.62    39.34        1.0                 2   1  \n",
      "3      35.65  24.1     15.6     0.66    36.99        1.0                 2   2  \n",
      "4      35.19  24.1     18.0      0.6    36.84        1.0                 0   6  \n",
      "5      35.86  24.1     18.5      0.6    37.39        0.0                 0   2  \n",
      "6      36.14  24.3     18.8      0.6    37.14        1.0                 3   7  \n",
      "7      35.25  24.2     18.1      0.6    36.89        0.0                 0   2  \n",
      "8  35.895934  24.0     19.4      0.7    36.69        1.0                 0   2  \n",
      "9      35.79  24.0     19.8      0.7    36.89        0.0                 0   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "   SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  180208-10       1.14     35.55      35.3       35.27       35.01  35.54   \n",
      "1  180209-01       0.64     35.88     36.23       35.35       35.64  36.22   \n",
      "2  180209-02       0.79      37.6     36.97       37.22       36.25  37.64   \n",
      "3  180209-03        0.8     35.63     35.75       35.09        35.2  35.61   \n",
      "4  180209-05        0.9     35.08     34.78       34.22       34.32  35.01   \n",
      "5  180209-06       0.83     35.67     35.63       34.92       35.16  35.92   \n",
      "6  180209-07       0.94     35.54     35.98       35.05       35.65  35.68   \n",
      "7  180209-08       0.92     35.61     35.68       34.23       34.64  35.66   \n",
      "8  180209-09       0.91     35.46      35.7       34.44       34.74   35.5   \n",
      "9  180209-10       0.98     35.78     35.61       35.35       35.04  35.76   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_Max_3 T_OR_3 T_OR_Max_3 T_atm  \\\n",
      "0      35.54      35.33      35.55  ...   36.05  36.03      36.05  22.0   \n",
      "1      36.22      35.29      36.23  ...   36.68  36.63      36.68  24.1   \n",
      "2      37.57      37.64       37.7  ...    37.7  37.04      37.09  24.1   \n",
      "3      35.61      35.42      35.63  ...   36.03  35.95      36.03  24.1   \n",
      "4      35.01      34.57      35.08  ...   35.51  35.22      35.25  24.1   \n",
      "5      35.69      35.92      35.97  ...   36.07  36.06      36.07  24.1   \n",
      "6      35.53      35.68      35.71  ...   36.06  36.03      36.06  24.3   \n",
      "7       35.6      35.66      35.69  ...   35.69  35.43      35.44  24.2   \n",
      "8      35.41       35.5      35.53  ...   35.74  35.15      35.16  24.0   \n",
      "9      35.76      35.37      35.78  ...   35.78  35.58      35.63  24.0   \n",
      "\n",
      "  Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0     30.0      0.6    36.74        0.0                 2   1  \n",
      "1     15.6     0.62    37.69        0.0                 0   1  \n",
      "2     15.6     0.62    39.34        1.0                 2   1  \n",
      "3     15.6     0.66    36.99        1.0                 2   2  \n",
      "4     18.0      0.6    36.84        1.0                 0   6  \n",
      "5     18.5      0.6    37.39        0.0                 0   2  \n",
      "6     18.8      0.6    37.14        1.0                 3   7  \n",
      "7     18.1      0.6    36.89        0.0                 0   2  \n",
      "8     19.4      0.7    36.69        1.0                 0   2  \n",
      "9     19.8      0.7    36.89        0.0                 0   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "   SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  180208-10       1.08     35.63     35.34       35.37       34.98  35.61   \n",
      "1  180209-01       0.58     36.12     36.27       35.51       35.89  36.12   \n",
      "2  180209-02       0.91     37.63     37.18       37.15       36.62  37.72   \n",
      "3  180209-03       0.84     35.57      35.6       34.39        34.8  35.57   \n",
      "4  180209-05       0.89     34.99     34.71       34.24       34.31  34.94   \n",
      "5  180209-06       0.77     35.77     35.68       34.91       35.09  35.99   \n",
      "6  180209-07       0.91     35.49     35.99       34.88       35.42  35.68   \n",
      "7  180209-08       0.97     35.89     35.57       35.33       33.96  35.86   \n",
      "8  180209-09       0.89     35.44     35.72       34.11       34.76  35.49   \n",
      "9  180209-10       0.95     35.72     35.62       35.24       35.26  35.71   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_Max_4 T_OR_4 T_OR_Max_4 T_atm  \\\n",
      "0      35.61      35.36      35.63  ...   36.62  36.59      36.62  22.0   \n",
      "1      36.12      35.76      36.12  ...   37.21  37.19      37.21  24.1   \n",
      "2      37.54      37.72      37.78  ...   38.72  38.67      38.72  24.1   \n",
      "3      35.54      35.55      35.62  ...   36.71   36.7      36.71  24.1   \n",
      "4      34.94      34.45      34.99  ...   36.36  36.29      36.36  24.1   \n",
      "5      35.76      35.99      36.02  ...   36.66  36.57      36.66  24.1   \n",
      "6      35.44      35.68      35.71  ...   36.98  36.97      36.98  24.3   \n",
      "7      35.86      35.43      35.89  ...   36.51  36.49      36.51  24.2   \n",
      "8      35.44      35.46      35.53  ...   35.72  35.48      35.53  24.0   \n",
      "9      35.71      35.46      35.72  ...   36.54  36.52      36.54  24.0   \n",
      "\n",
      "  Humidity Distance aveOralM Gender_NEW Ethnicity_encoded Age  \n",
      "0     30.0      0.6    36.74        0.0                 2   1  \n",
      "1     15.6     0.62    37.69        0.0                 0   1  \n",
      "2     15.6     0.62    39.34        1.0                 2   1  \n",
      "3     15.6     0.66    36.99        1.0                 2   2  \n",
      "4     18.0      0.6    36.84        1.0                 0   6  \n",
      "5     18.5      0.6    37.39        0.0                 0   2  \n",
      "6     18.8      0.6    37.14        1.0                 3   7  \n",
      "7     18.1      0.6    36.89        0.0                 0   2  \n",
      "8     19.4      0.7    36.69        1.0                 0   2  \n",
      "9     19.8      0.7    36.89        0.0                 0   1  \n",
      "\n",
      "[10 rows x 35 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "SubjectID            0\n",
      "T_offset_1           0\n",
      "Max1R13_1            0\n",
      "Max1L13_1            0\n",
      "aveAllR13_1          0\n",
      "aveAllL13_1          0\n",
      "T_RC_1               0\n",
      "T_RC_Dry_1           0\n",
      "T_RC_Wet_1           0\n",
      "T_RC_Max_1           0\n",
      "T_LC_1               0\n",
      "T_LC_Dry_1           0\n",
      "T_LC_Wet_1           0\n",
      "T_LC_Max_1           0\n",
      "RCC_1                0\n",
      "LCC_1                0\n",
      "canthiMax_1          0\n",
      "canthi4Max_1         0\n",
      "T_FHCC_1             0\n",
      "T_FHRC_1             0\n",
      "T_FHLC_1             0\n",
      "T_FHBC_1             0\n",
      "T_FHTC_1             0\n",
      "T_FH_Max_1           0\n",
      "T_FHC_Max_1          0\n",
      "T_Max_1              0\n",
      "T_OR_1               0\n",
      "T_OR_Max_1           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 2 \n",
      "SubjectID            0\n",
      "T_offset_2           0\n",
      "Max1R13_2            0\n",
      "Max1L13_2            0\n",
      "aveAllR13_2          0\n",
      "aveAllL13_2          0\n",
      "T_RC_2               0\n",
      "T_RC_Dry_2           0\n",
      "T_RC_Wet_2           0\n",
      "T_RC_Max_2           0\n",
      "T_LC_2               0\n",
      "T_LC_Dry_2           0\n",
      "T_LC_Wet_2           0\n",
      "T_LC_Max_2           0\n",
      "RCC_2                0\n",
      "LCC_2                0\n",
      "canthiMax_2          0\n",
      "canthi4Max_2         0\n",
      "T_FHCC_2             0\n",
      "T_FHRC_2             0\n",
      "T_FHLC_2             0\n",
      "T_FHBC_2             0\n",
      "T_FHTC_2             0\n",
      "T_FH_Max_2           0\n",
      "T_FHC_Max_2          0\n",
      "T_Max_2              0\n",
      "T_OR_2               0\n",
      "T_OR_Max_2           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 3 \n",
      "SubjectID            0\n",
      "T_offset_3           0\n",
      "Max1R13_3            0\n",
      "Max1L13_3            0\n",
      "aveAllR13_3          0\n",
      "aveAllL13_3          0\n",
      "T_RC_3               0\n",
      "T_RC_Dry_3           0\n",
      "T_RC_Wet_3           0\n",
      "T_RC_Max_3           0\n",
      "T_LC_3               0\n",
      "T_LC_Dry_3           0\n",
      "T_LC_Wet_3           0\n",
      "T_LC_Max_3           0\n",
      "RCC_3                0\n",
      "LCC_3                0\n",
      "canthiMax_3          0\n",
      "canthi4Max_3         0\n",
      "T_FHCC_3             0\n",
      "T_FHRC_3             0\n",
      "T_FHLC_3             0\n",
      "T_FHBC_3             0\n",
      "T_FHTC_3             0\n",
      "T_FH_Max_3           0\n",
      "T_FHC_Max_3          0\n",
      "T_Max_3              0\n",
      "T_OR_3               0\n",
      "T_OR_Max_3           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Round 4 \n",
      "SubjectID            0\n",
      "T_offset_4           0\n",
      "Max1R13_4            0\n",
      "Max1L13_4            0\n",
      "aveAllR13_4          0\n",
      "aveAllL13_4          0\n",
      "T_RC_4               0\n",
      "T_RC_Dry_4           0\n",
      "T_RC_Wet_4           0\n",
      "T_RC_Max_4           0\n",
      "T_LC_4               0\n",
      "T_LC_Dry_4           0\n",
      "T_LC_Wet_4           0\n",
      "T_LC_Max_4           0\n",
      "RCC_4                0\n",
      "LCC_4                0\n",
      "canthiMax_4          0\n",
      "canthi4Max_4         0\n",
      "T_FHCC_4             0\n",
      "T_FHRC_4             0\n",
      "T_FHLC_4             0\n",
      "T_FHBC_4             0\n",
      "T_FHTC_4             0\n",
      "T_FH_Max_4           0\n",
      "T_FHC_Max_4          0\n",
      "T_Max_4              0\n",
      "T_OR_4               0\n",
      "T_OR_Max_4           0\n",
      "T_atm                0\n",
      "Humidity             0\n",
      "Distance             0\n",
      "aveOralM             0\n",
      "Gender_NEW           0\n",
      "Ethnicity_encoded    0\n",
      "Age                  0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.isnull().sum()) \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max median\n",
      "0         0.0  35.603666  33.78  37.01  35.61\n",
      "1         1.0  35.790879  34.34  38.03   35.7\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min   max  median\n",
      "0         0.0  35.662805  34.18  37.0   35.64\n",
      "1         1.0  35.876385  34.55  38.1  35.675\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  35.700415  34.12  36.99     35.705\n",
      "1         1.0  35.908819  34.69  38.05  35.751795\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.710083  34.22  37.03   35.69\n",
      "1         1.0  35.926414  34.83   38.1  35.765\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'T_LC_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "   SubjectID T_offset_1  Max1R13_1  Max1L13_1 aveAllR13_1 aveAllL13_1  \\\n",
      "0  180208-10        1.0      35.62      35.39       35.33       35.07   \n",
      "1  180209-01       0.44      36.21      36.27       35.47       35.86   \n",
      "2  180209-02       0.65      37.68      37.16       37.06       36.52   \n",
      "3  180209-03       0.91      35.28      35.69       34.89       34.81   \n",
      "4  180209-05       0.81      34.19      34.46       33.64       33.87   \n",
      "5  180209-06        0.8      35.98      35.86       35.18       35.19   \n",
      "6  180209-07       0.86      35.16      35.88       34.54       35.41   \n",
      "7  180209-08       0.78      35.48      35.59       33.88       34.47   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10        0.9      35.44      35.31       34.84       34.66   \n",
      "\n",
      "      T_RC_1 T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... Humidity Distance aveOralM  \\\n",
      "0      35.59      35.59      35.44      35.62  ...     30.0      0.6    36.74   \n",
      "1      36.19      36.19      35.87      36.21  ...     15.6     0.62    37.69   \n",
      "2      37.71       37.6      37.69      37.73  ...     15.6     0.62    39.34   \n",
      "3      35.26      35.26      34.94      35.28  ...     15.6     0.66    36.99   \n",
      "4      34.33      34.33      33.98      34.37  ...     18.0      0.6    36.84   \n",
      "5      36.05      35.93      36.05      36.07  ...     18.5      0.6    37.39   \n",
      "6      35.47      35.17      35.47      35.47  ...     18.8      0.6    37.14   \n",
      "7      35.51      35.41      35.51      35.56  ...     18.1      0.6    36.89   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...     19.4      0.7    36.69   \n",
      "9      35.59      35.59      34.66      35.59  ...     19.8      0.7    36.89   \n",
      "\n",
      "  Gender_NEW Ethnicity_encoded Age T_LC_1_Mean T_LC_1_Min T_LC_1_Max  \\\n",
      "0        0.0                 2   1   35.603666      33.78      37.01   \n",
      "1        0.0                 0   1   35.603666      33.78      37.01   \n",
      "2        1.0                 2   1   35.790879      34.34      38.03   \n",
      "3        1.0                 2   2   35.790879      34.34      38.03   \n",
      "4        1.0                 0   6   35.790879      34.34      38.03   \n",
      "5        0.0                 0   2   35.603666      33.78      37.01   \n",
      "6        1.0                 3   7   35.790879      34.34      38.03   \n",
      "7        0.0                 0   2   35.603666      33.78      37.01   \n",
      "8        1.0                 0   2   35.790879      34.34      38.03   \n",
      "9        0.0                 0   1   35.603666      33.78      37.01   \n",
      "\n",
      "  T_LC_1_Median  \n",
      "0         35.61  \n",
      "1         35.61  \n",
      "2         35.70  \n",
      "3         35.70  \n",
      "4         35.70  \n",
      "5         35.61  \n",
      "6         35.70  \n",
      "7         35.61  \n",
      "8         35.70  \n",
      "9         35.61  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "   SubjectID T_offset_2  Max1R13_2  Max1L13_2 aveAllR13_2 aveAllL13_2  \\\n",
      "0  180208-10       1.05       35.6       35.3       35.25       34.95   \n",
      "1  180209-01       0.56      36.24      36.18       35.81       35.85   \n",
      "2  180209-02       0.72      37.57      37.21       37.22       36.73   \n",
      "3  180209-03       0.85      35.48      35.47       35.09       35.04   \n",
      "4  180209-05       0.91      34.68      34.69       34.31       34.24   \n",
      "5  180209-06       0.88      35.74      35.61       34.82       34.92   \n",
      "6  180209-07       0.95      35.72      36.04       35.35       35.27   \n",
      "7  180209-08       0.83      35.65      35.61       34.13       34.46   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10       0.91      35.72      35.51       35.06       34.96   \n",
      "\n",
      "      T_RC_2 T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... Humidity Distance aveOralM  \\\n",
      "0       35.6       35.6      35.45       35.6  ...     30.0      0.6    36.74   \n",
      "1      36.22      36.22      35.63      36.24  ...     15.6     0.62    37.69   \n",
      "2      37.71      37.56      37.71      37.75  ...     15.6     0.62    39.34   \n",
      "3      35.52      35.52      35.29      35.54  ...     15.6     0.66    36.99   \n",
      "4      34.71      34.71      34.51      34.72  ...     18.0      0.6    36.84   \n",
      "5      35.86      35.78      35.86      35.89  ...     18.5      0.6    37.39   \n",
      "6      35.73      35.71      35.73      35.76  ...     18.8      0.6    37.14   \n",
      "7      35.63       35.6      35.62      35.65  ...     18.1      0.6    36.89   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...     19.4      0.7    36.69   \n",
      "9       35.7       35.7      35.44      35.72  ...     19.8      0.7    36.89   \n",
      "\n",
      "  Gender_NEW Ethnicity_encoded Age T_LC_2_Mean T_LC_2_Min T_LC_2_Max  \\\n",
      "0        0.0                 2   1   35.662805      34.18       37.0   \n",
      "1        0.0                 0   1   35.662805      34.18       37.0   \n",
      "2        1.0                 2   1   35.876385      34.55       38.1   \n",
      "3        1.0                 2   2   35.876385      34.55       38.1   \n",
      "4        1.0                 0   6   35.876385      34.55       38.1   \n",
      "5        0.0                 0   2   35.662805      34.18       37.0   \n",
      "6        1.0                 3   7   35.876385      34.55       38.1   \n",
      "7        0.0                 0   2   35.662805      34.18       37.0   \n",
      "8        1.0                 0   2   35.876385      34.55       38.1   \n",
      "9        0.0                 0   1   35.662805      34.18       37.0   \n",
      "\n",
      "  T_LC_2_Median  \n",
      "0        35.640  \n",
      "1        35.640  \n",
      "2        35.675  \n",
      "3        35.675  \n",
      "4        35.675  \n",
      "5        35.640  \n",
      "6        35.675  \n",
      "7        35.640  \n",
      "8        35.675  \n",
      "9        35.640  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "   SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  180208-10       1.14     35.55      35.3       35.27       35.01  35.54   \n",
      "1  180209-01       0.64     35.88     36.23       35.35       35.64  36.22   \n",
      "2  180209-02       0.79      37.6     36.97       37.22       36.25  37.64   \n",
      "3  180209-03        0.8     35.63     35.75       35.09        35.2  35.61   \n",
      "4  180209-05        0.9     35.08     34.78       34.22       34.32  35.01   \n",
      "5  180209-06       0.83     35.67     35.63       34.92       35.16  35.92   \n",
      "6  180209-07       0.94     35.54     35.98       35.05       35.65  35.68   \n",
      "7  180209-08       0.92     35.61     35.68       34.23       34.64  35.66   \n",
      "8  180209-09       0.91     35.46      35.7       34.44       34.74   35.5   \n",
      "9  180209-10       0.98     35.78     35.61       35.35       35.04  35.76   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... Humidity Distance aveOralM Gender_NEW  \\\n",
      "0      35.54      35.33      35.55  ...     30.0      0.6    36.74        0.0   \n",
      "1      36.22      35.29      36.23  ...     15.6     0.62    37.69        0.0   \n",
      "2      37.57      37.64       37.7  ...     15.6     0.62    39.34        1.0   \n",
      "3      35.61      35.42      35.63  ...     15.6     0.66    36.99        1.0   \n",
      "4      35.01      34.57      35.08  ...     18.0      0.6    36.84        1.0   \n",
      "5      35.69      35.92      35.97  ...     18.5      0.6    37.39        0.0   \n",
      "6      35.53      35.68      35.71  ...     18.8      0.6    37.14        1.0   \n",
      "7       35.6      35.66      35.69  ...     18.1      0.6    36.89        0.0   \n",
      "8      35.41       35.5      35.53  ...     19.4      0.7    36.69        1.0   \n",
      "9      35.76      35.37      35.78  ...     19.8      0.7    36.89        0.0   \n",
      "\n",
      "  Ethnicity_encoded Age T_LC_3_Mean T_LC_3_Min T_LC_3_Max T_LC_3_Median  \n",
      "0                 2   1   35.700415      34.12      36.99     35.705000  \n",
      "1                 0   1   35.700415      34.12      36.99     35.705000  \n",
      "2                 2   1   35.908819      34.69      38.05     35.751795  \n",
      "3                 2   2   35.908819      34.69      38.05     35.751795  \n",
      "4                 0   6   35.908819      34.69      38.05     35.751795  \n",
      "5                 0   2   35.700415      34.12      36.99     35.705000  \n",
      "6                 3   7   35.908819      34.69      38.05     35.751795  \n",
      "7                 0   2   35.700415      34.12      36.99     35.705000  \n",
      "8                 0   2   35.908819      34.69      38.05     35.751795  \n",
      "9                 0   1   35.700415      34.12      36.99     35.705000  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "   SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  180208-10       1.08     35.63     35.34       35.37       34.98  35.61   \n",
      "1  180209-01       0.58     36.12     36.27       35.51       35.89  36.12   \n",
      "2  180209-02       0.91     37.63     37.18       37.15       36.62  37.72   \n",
      "3  180209-03       0.84     35.57      35.6       34.39        34.8  35.57   \n",
      "4  180209-05       0.89     34.99     34.71       34.24       34.31  34.94   \n",
      "5  180209-06       0.77     35.77     35.68       34.91       35.09  35.99   \n",
      "6  180209-07       0.91     35.49     35.99       34.88       35.42  35.68   \n",
      "7  180209-08       0.97     35.89     35.57       35.33       33.96  35.86   \n",
      "8  180209-09       0.89     35.44     35.72       34.11       34.76  35.49   \n",
      "9  180209-10       0.95     35.72     35.62       35.24       35.26  35.71   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... Humidity Distance aveOralM Gender_NEW  \\\n",
      "0      35.61      35.36      35.63  ...     30.0      0.6    36.74        0.0   \n",
      "1      36.12      35.76      36.12  ...     15.6     0.62    37.69        0.0   \n",
      "2      37.54      37.72      37.78  ...     15.6     0.62    39.34        1.0   \n",
      "3      35.54      35.55      35.62  ...     15.6     0.66    36.99        1.0   \n",
      "4      34.94      34.45      34.99  ...     18.0      0.6    36.84        1.0   \n",
      "5      35.76      35.99      36.02  ...     18.5      0.6    37.39        0.0   \n",
      "6      35.44      35.68      35.71  ...     18.8      0.6    37.14        1.0   \n",
      "7      35.86      35.43      35.89  ...     18.1      0.6    36.89        0.0   \n",
      "8      35.44      35.46      35.53  ...     19.4      0.7    36.69        1.0   \n",
      "9      35.71      35.46      35.72  ...     19.8      0.7    36.89        0.0   \n",
      "\n",
      "  Ethnicity_encoded Age T_LC_4_Mean T_LC_4_Min T_LC_4_Max T_LC_4_Median  \n",
      "0                 2   1   35.710083      34.22      37.03        35.690  \n",
      "1                 0   1   35.710083      34.22      37.03        35.690  \n",
      "2                 2   1   35.926414      34.83      38.10        35.765  \n",
      "3                 2   2   35.926414      34.83      38.10        35.765  \n",
      "4                 0   6   35.926414      34.83      38.10        35.765  \n",
      "5                 0   2   35.710083      34.22      37.03        35.690  \n",
      "6                 3   7   35.926414      34.83      38.10        35.765  \n",
      "7                 0   2   35.710083      34.22      37.03        35.690  \n",
      "8                 0   2   35.926414      34.83      38.10        35.765  \n",
      "9                 0   1   35.710083      34.22      37.03        35.690  \n",
      "\n",
      "[10 rows x 39 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.764855  34.45  37.26  35.765\n",
      "1         1.0   35.91569  34.42  38.28   35.79\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min   max median\n",
      "0         0.0  35.810769  34.59  37.4  35.77\n",
      "1         1.0  35.995993  34.56  38.4  35.84\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.850766  34.52  37.16  35.835\n",
      "1         1.0  36.028518  34.72  38.41   35.85\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  35.862523  34.58  37.27      35.82\n",
      "1         1.0  36.057575  34.96  38.54  35.901557\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'canthiMax_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.120125  33.09  36.67   35.16\n",
      "1         1.0  35.187386  32.98  37.58  35.185\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  35.233028  33.86  36.71  35.255\n",
      "1         1.0  35.298655   34.0  37.32  35.225\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max    median\n",
      "0         0.0  35.248617  33.86  36.72  35.23659\n",
      "1         1.0  35.338277  33.84  37.78  35.23318\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max    median\n",
      "0         0.0  35.199675  33.88  36.74    35.215\n",
      "1         1.0  35.305319  34.03  37.85  35.23659\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'T_FHC_Max_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  34.533914  32.52  36.47     34.595\n",
      "1         1.0  34.648629  31.22  37.18  34.697992\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Gender_NEW       mean    min    max  median\n",
      "0         0.0  34.673215  32.74  36.55   34.69\n",
      "1         1.0  34.750468  32.56  36.92  34.775\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Gender_NEW       mean    min    max median\n",
      "0         0.0   34.72114  33.12  36.49  34.71\n",
      "1         1.0  34.829161  32.69  37.18  34.82\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Gender_NEW       mean    min    max     median\n",
      "0         0.0  34.690935  33.04  36.45  34.697992\n",
      "1         1.0  34.822855  32.93  37.22       34.8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    \n",
    "    temperature_column = f'T_FHCC_{round_number}'\n",
    "    \n",
    "    if 'Gender_NEW' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Gender_NEW')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            stat_dict = dict(zip(grouped_stats['Gender_NEW'], grouped_stats[stat]))\n",
    "            \n",
    "            df[f'{temperature_column}_{stat.capitalize()}'] = df['Gender_NEW'].map(stat_dict)\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Gender_NEW' or '{temperature_column}' not found in {round_name}. Skipping this round.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "  SubjectID T_offset_1 Max1R13_1 Max1L13_1 aveAllR13_1 aveAllL13_1 T_RC_1  \\\n",
      "0  161117-1       0.58     34.98     35.36       34.44       34.85  34.91   \n",
      "1  161117-2       0.83     34.71     34.51       34.46       34.24  34.68   \n",
      "2  161117-3       0.85      35.7     35.44        35.0       34.78  35.67   \n",
      "3  161117-4        0.9     35.17      35.5       34.25        35.0  35.14   \n",
      "4  161117-5       1.08     35.33     35.55       34.31       35.14   35.5   \n",
      "5  161117-6       0.86     34.95      35.1        34.3       34.59  34.97   \n",
      "6  161118-1       0.44     35.15     35.52       34.59       34.29  35.13   \n",
      "7  161118-2       0.56     34.64     34.83       33.52       33.65  34.81   \n",
      "8  161118-4       0.93      35.3     35.36        34.1       34.55   35.3   \n",
      "9  161118-5       1.05     35.84     35.72       35.42       35.14  35.75   \n",
      "\n",
      "  T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... canthiMax_1_Max_y  \\\n",
      "0      34.91       34.6      34.98  ...             37.94   \n",
      "1      34.68      34.44      34.71  ...             38.03   \n",
      "2      35.67      35.46       35.7  ...             37.94   \n",
      "3      35.14      35.08      35.17  ...             38.03   \n",
      "4       35.3       35.5      35.52  ...             37.94   \n",
      "5      34.97      34.85      34.98  ...             37.94   \n",
      "6      35.12      35.05      35.15  ...             37.94   \n",
      "7      34.71       34.8      34.87  ...             37.94   \n",
      "8      35.27       35.3      35.32  ...             37.48   \n",
      "9      35.75      35.36      35.84  ...             38.52   \n",
      "\n",
      "  canthiMax_1_Median_y T_FHC_Max_1_Mean_y T_FHC_Max_1_Min_y T_FHC_Max_1_Max_y  \\\n",
      "0                35.74          34.950299             31.63             36.69   \n",
      "1                35.47          34.771553             32.99             36.63   \n",
      "2                35.74          34.950299             31.63             36.69   \n",
      "3                35.47          34.771553             32.99             36.63   \n",
      "4                35.74          34.950299             31.63             36.69   \n",
      "5                35.74          34.950299             31.63             36.69   \n",
      "6                35.74          34.950299             31.63             36.69   \n",
      "7                35.74          34.950299             31.63             36.69   \n",
      "8                35.47          34.983057              31.7             37.06   \n",
      "9                35.74          35.051613             33.97             37.38   \n",
      "\n",
      "  T_FHC_Max_1_Median_y T_Max_1_Mean T_Max_1_Min T_Max_1_Max T_Max_1_Median  \n",
      "0                35.03     35.84951       34.47       37.94          35.81  \n",
      "1                34.74    35.635146       34.67       38.03          35.59  \n",
      "2                35.03     35.84951       34.47       37.94          35.81  \n",
      "3                34.74    35.635146       34.67       38.03          35.59  \n",
      "4                35.03     35.84951       34.47       37.94          35.81  \n",
      "5                35.03     35.84951       34.47       37.94          35.81  \n",
      "6                35.03     35.84951       34.47       37.94          35.81  \n",
      "7                35.03     35.84951       34.47       37.94          35.81  \n",
      "8            35.032753    35.673647       33.85       37.64          35.66  \n",
      "9                34.94        35.91       34.88       38.52          35.75  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "  SubjectID T_offset_2 Max1R13_2 Max1L13_2 aveAllR13_2 aveAllL13_2 T_RC_2  \\\n",
      "0  161117-1       0.74     35.07      35.3       34.36       34.85  35.02   \n",
      "1  161117-2       0.78     34.58     34.58       34.02       34.18  34.68   \n",
      "2  161117-3       0.84     35.52     35.45       33.87       34.82  35.63   \n",
      "3  161117-4       0.95     35.17     35.65       34.38       35.33  35.16   \n",
      "4  161117-5        0.8     35.49     35.67       35.08       35.42  35.59   \n",
      "5  161117-6       0.78     35.18     35.18       34.37       34.22  35.18   \n",
      "6  161118-1       0.28      35.2     35.67       34.72       35.16  35.28   \n",
      "7  161118-2       0.62     35.22     34.78       34.26       33.79  35.13   \n",
      "8  161118-4       0.99     35.35     35.44       34.91       35.16  35.38   \n",
      "9  161118-5       1.07     35.85     35.77       35.39       35.47  35.84   \n",
      "\n",
      "  T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... canthiMax_2_Max_y  \\\n",
      "0      35.02      34.92      35.07  ...             38.06   \n",
      "1      34.67      34.64      34.71  ...             38.01   \n",
      "2      35.63      35.57      35.69  ...             38.06   \n",
      "3      35.15      35.16      35.18  ...             38.01   \n",
      "4      35.34      35.59      35.66  ...             38.06   \n",
      "5      35.15      35.17      35.21  ...             38.06   \n",
      "6      35.17      35.28      35.29  ...             38.06   \n",
      "7      35.13      34.98      35.22  ...             38.06   \n",
      "8      35.34      35.38      35.43  ...             37.79   \n",
      "9      35.84      35.76      35.85  ...             38.25   \n",
      "\n",
      "  canthiMax_2_Median_y T_FHC_Max_2_Mean_y T_FHC_Max_2_Min_y T_FHC_Max_2_Max_y  \\\n",
      "0                35.76          35.082021             32.16             36.71   \n",
      "1                35.48          34.871748             33.44             37.02   \n",
      "2                35.76          35.082021             32.16             36.71   \n",
      "3                35.48          34.871748             33.44             37.02   \n",
      "4                35.76          35.082021             32.16             36.71   \n",
      "5                35.76          35.082021             32.16             36.71   \n",
      "6                35.76          35.082021             32.16             36.71   \n",
      "7                35.76          35.082021             32.16             36.71   \n",
      "8                35.54          35.098804              32.3             37.38   \n",
      "9                35.76          35.126774             33.92             37.32   \n",
      "\n",
      "  T_FHC_Max_2_Median_y T_Max_2_Mean T_Max_2_Min T_Max_2_Max T_Max_2_Median  \n",
      "0                 35.1    36.033003        34.8        38.4          35.98  \n",
      "1                34.84    35.778252       34.93       38.01          35.71  \n",
      "2                 35.1    36.033003        34.8        38.4          35.98  \n",
      "3                34.84    35.778252       34.93       38.01          35.71  \n",
      "4                 35.1    36.033003        34.8        38.4          35.98  \n",
      "5                 35.1    36.033003        34.8        38.4          35.98  \n",
      "6                 35.1    36.033003        34.8        38.4          35.98  \n",
      "7                 35.1    36.033003        34.8        38.4          35.98  \n",
      "8                35.15    35.818921        34.7       37.79          35.77  \n",
      "9                35.09    36.092581       35.16       38.66          35.99  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "  SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  161117-1       0.76     35.04     35.42        34.6       35.02  35.01   \n",
      "1  161117-2       0.72     34.34     34.47       33.78        34.2  34.73   \n",
      "2  161117-3       0.91      35.7     35.57       34.48       34.74  35.69   \n",
      "3  161117-4       0.95     35.14      35.6       33.79       35.23  35.14   \n",
      "4  161117-5       0.82     35.67     35.71       35.15       35.43  35.65   \n",
      "5  161117-6       0.85     35.17     35.23        34.7       34.83  35.14   \n",
      "6  161118-1        0.5     35.38     35.64       34.94       35.17  35.34   \n",
      "7  161118-2       0.54     35.35      34.9       34.75       34.18  35.28   \n",
      "8  161118-4       0.92     35.27     35.44        34.8       35.15  35.32   \n",
      "9  161118-5       1.11     35.91     35.82       35.55        35.3  35.87   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... canthiMax_3_Max_y  \\\n",
      "0      35.01      34.56      35.05  ...             38.11   \n",
      "1      34.51      34.73      34.76  ...             37.94   \n",
      "2      35.68      35.68       35.7  ...             38.11   \n",
      "3      35.12      35.14      35.14  ...             37.94   \n",
      "4      35.59      35.63      35.69  ...             38.11   \n",
      "5      35.14      35.03      35.17  ...             38.11   \n",
      "6      35.34       35.2      35.38  ...             38.11   \n",
      "7      35.28      35.04      35.35  ...             38.11   \n",
      "8      35.21      35.32      35.33  ...             37.83   \n",
      "9      35.87       35.6      35.91  ...             38.32   \n",
      "\n",
      "  canthiMax_3_Median_y T_FHC_Max_3_Mean_y T_FHC_Max_3_Min_y T_FHC_Max_3_Max_y  \\\n",
      "0                 35.8          35.116992             32.32             37.41   \n",
      "1                35.54          34.896434             33.75              36.7   \n",
      "2                 35.8          35.116992             32.32             37.41   \n",
      "3                35.54          34.896434             33.75              36.7   \n",
      "4                 35.8          35.116992             32.32             37.41   \n",
      "5                 35.8          35.116992             32.32             37.41   \n",
      "6                 35.8          35.116992             32.32             37.41   \n",
      "7                 35.8          35.116992             32.32             37.41   \n",
      "8                35.57          35.112476             32.16             37.46   \n",
      "9                 35.8          35.116452             33.83             37.31   \n",
      "\n",
      "  T_FHC_Max_3_Median_y T_Max_3_Mean T_Max_3_Min T_Max_3_Max T_Max_3_Median  \n",
      "0                35.13    36.033684       34.86       38.11          35.99  \n",
      "1                34.88    35.818508       35.03       37.94          35.76  \n",
      "2                35.13    36.033684       34.86       38.11          35.99  \n",
      "3                34.88    35.818508       35.03       37.94          35.76  \n",
      "4                35.13    36.033684       34.86       38.11          35.99  \n",
      "5                35.13    36.033684       34.86       38.11          35.99  \n",
      "6                35.13    36.033684       34.86       38.11          35.99  \n",
      "7                35.13    36.033684       34.86       38.11          35.99  \n",
      "8                35.16    35.814515       34.81       37.83          35.78  \n",
      "9                35.12     36.10871       35.15       38.37          35.93  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "  SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  161117-1       0.73     35.03     35.43        34.2       34.95   35.0   \n",
      "1  161117-2       0.79     34.57     34.52       33.46       34.28  34.75   \n",
      "2  161117-3       0.85     35.69     35.61       33.76       34.86  35.75   \n",
      "3  161117-4       0.92     35.41      35.7       35.12       35.43  35.39   \n",
      "4  161117-5       0.88     35.69     35.73        35.1       35.48  35.67   \n",
      "5  161117-6       0.82     35.23      35.3       34.74       34.69  35.23   \n",
      "6  161118-1       0.55      35.4     35.63       34.75       34.25  35.38   \n",
      "7  161118-2       0.63     35.21     34.97       34.56       34.49  35.18   \n",
      "8  161118-4       1.07     35.31     35.45       34.42       34.99  35.36   \n",
      "9  161118-5       1.04     35.96     35.75       35.33       35.21  35.88   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... canthiMax_4_Max_y  \\\n",
      "0       35.0      34.97      35.03  ...              38.0   \n",
      "1      34.67      34.75      34.79  ...             37.77   \n",
      "2      35.69      35.75      35.78  ...              38.0   \n",
      "3      35.39      35.09      35.41  ...             37.77   \n",
      "4      35.67      35.56      35.69  ...              38.0   \n",
      "5      35.23      35.15      35.23  ...              38.0   \n",
      "6      35.37      35.31       35.4  ...              38.0   \n",
      "7      35.18      34.94      35.21  ...              38.0   \n",
      "8      35.28      35.36      35.38  ...             37.81   \n",
      "9      35.88       35.7      35.96  ...             38.37   \n",
      "\n",
      "  canthiMax_4_Median_y T_FHC_Max_4_Mean_y T_FHC_Max_4_Min_y T_FHC_Max_4_Max_y  \\\n",
      "0                35.82          35.094111              32.8             37.03   \n",
      "1                35.59          34.807044             33.61             36.55   \n",
      "2                35.82          35.094111              32.8             37.03   \n",
      "3                35.59          34.807044             33.61             36.55   \n",
      "4                35.82          35.094111              32.8             37.03   \n",
      "5                35.82          35.094111              32.8             37.03   \n",
      "6                35.82          35.094111              32.8             37.03   \n",
      "7                35.82          35.094111              32.8             37.03   \n",
      "8                35.69          35.100721             33.49             37.47   \n",
      "9                35.77          35.058387             33.54             37.15   \n",
      "\n",
      "  T_FHC_Max_4_Median_y T_Max_4_Mean T_Max_4_Min T_Max_4_Max T_Max_4_Median  \n",
      "0                35.07    36.568847       35.02       38.84          36.57  \n",
      "1                 34.8    36.532842       35.34       38.73           36.6  \n",
      "2                35.07    36.568847       35.02       38.84          36.57  \n",
      "3                 34.8    36.532842       35.34       38.73           36.6  \n",
      "4                35.07    36.568847       35.02       38.84          36.57  \n",
      "5                35.07    36.568847       35.02       38.84          36.57  \n",
      "6                35.07    36.568847       35.02       38.84          36.57  \n",
      "7                35.07    36.568847       35.02       38.84          36.57  \n",
      "8                35.09    36.491659       35.29       38.72           36.5  \n",
      "9                35.15    36.740323       35.87       39.68          36.69  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0  34.631678  31.94  37.18   34.69\n",
      "1                  1   34.36475  31.22  35.96  34.465\n",
      "2                  2  34.566241  32.59  36.89   34.55\n",
      "3                  3  34.633052  33.85  35.49   34.75\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0  34.763845  33.05  36.92   34.77\n",
      "1                  1     34.545  32.96  35.94  34.605\n",
      "2                  2  34.666265  32.56  36.62   34.66\n",
      "3                  3  34.676316  33.95  35.42   34.76\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max     median\n",
      "0                  0  34.818774  33.21  37.18      34.81\n",
      "1                  1    34.6344  33.19  36.07  34.653996\n",
      "2                  2  34.718651  32.69  36.83       34.7\n",
      "3                  3  34.756316  33.88  35.59      34.85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0  34.796571  33.29  37.22  34.775\n",
      "1                  1   34.60495  33.39  35.82   34.63\n",
      "2                  2  34.705373  32.93   36.9   34.69\n",
      "3                  3  34.735789  34.01  35.55   34.66\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'T_FHCC_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "   SubjectID T_offset_1  Max1R13_1  Max1L13_1 aveAllR13_1 aveAllL13_1  \\\n",
      "0  180208-10        1.0      35.62      35.39       35.33       35.07   \n",
      "1  180209-01       0.44      36.21      36.27       35.47       35.86   \n",
      "2  180209-02       0.65      37.68      37.16       37.06       36.52   \n",
      "3  180209-03       0.91      35.28      35.69       34.89       34.81   \n",
      "4  180209-05       0.81      34.19      34.46       33.64       33.87   \n",
      "5  180209-06        0.8      35.98      35.86       35.18       35.19   \n",
      "6  180209-07       0.86      35.16      35.88       34.54       35.41   \n",
      "7  180209-08       0.78      35.48      35.59       33.88       34.47   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10        0.9      35.44      35.31       34.84       34.66   \n",
      "\n",
      "      T_RC_1 T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... T_FHC_Max_1_Max  \\\n",
      "0      35.59      35.59      35.44      35.62  ...           36.67   \n",
      "1      36.19      36.19      35.87      36.21  ...           36.67   \n",
      "2      37.71       37.6      37.69      37.73  ...           37.58   \n",
      "3      35.26      35.26      34.94      35.28  ...           37.58   \n",
      "4      34.33      34.33      33.98      34.37  ...           37.58   \n",
      "5      36.05      35.93      36.05      36.07  ...           36.67   \n",
      "6      35.47      35.17      35.47      35.47  ...           37.58   \n",
      "7      35.51      35.41      35.51      35.56  ...           36.67   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...           37.58   \n",
      "9      35.59      35.59      34.66      35.59  ...           36.67   \n",
      "\n",
      "  T_FHC_Max_1_Median T_FHCC_1_Mean_x T_FHCC_1_Min_x T_FHCC_1_Max_x  \\\n",
      "0             35.160       34.533914          32.52          36.47   \n",
      "1             35.160       34.533914          32.52          36.47   \n",
      "2             35.185       34.648629          31.22          37.18   \n",
      "3             35.185       34.648629          31.22          37.18   \n",
      "4             35.185       34.648629          31.22          37.18   \n",
      "5             35.160       34.533914          32.52          36.47   \n",
      "6             35.185       34.648629          31.22          37.18   \n",
      "7             35.160       34.533914          32.52          36.47   \n",
      "8             35.185       34.648629          31.22          37.18   \n",
      "9             35.160       34.533914          32.52          36.47   \n",
      "\n",
      "  T_FHCC_1_Median_x T_FHCC_1_Mean_y T_FHCC_1_Min_y T_FHCC_1_Max_y  \\\n",
      "0         34.595000       34.566241          32.59          36.89   \n",
      "1         34.595000       34.631678          31.94          37.18   \n",
      "2         34.697992       34.566241          32.59          36.89   \n",
      "3         34.697992       34.566241          32.59          36.89   \n",
      "4         34.697992       34.631678          31.94          37.18   \n",
      "5         34.595000       34.631678          31.94          37.18   \n",
      "6         34.697992       34.633052          33.85          35.49   \n",
      "7         34.595000       34.631678          31.94          37.18   \n",
      "8         34.697992       34.631678          31.94          37.18   \n",
      "9         34.595000       34.631678          31.94          37.18   \n",
      "\n",
      "  T_FHCC_1_Median_y  \n",
      "0             34.55  \n",
      "1             34.69  \n",
      "2             34.55  \n",
      "3             34.55  \n",
      "4             34.69  \n",
      "5             34.69  \n",
      "6             34.75  \n",
      "7             34.69  \n",
      "8             34.69  \n",
      "9             34.69  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "   SubjectID T_offset_2  Max1R13_2  Max1L13_2 aveAllR13_2 aveAllL13_2  \\\n",
      "0  180208-10       1.05       35.6       35.3       35.25       34.95   \n",
      "1  180209-01       0.56      36.24      36.18       35.81       35.85   \n",
      "2  180209-02       0.72      37.57      37.21       37.22       36.73   \n",
      "3  180209-03       0.85      35.48      35.47       35.09       35.04   \n",
      "4  180209-05       0.91      34.68      34.69       34.31       34.24   \n",
      "5  180209-06       0.88      35.74      35.61       34.82       34.92   \n",
      "6  180209-07       0.95      35.72      36.04       35.35       35.27   \n",
      "7  180209-08       0.83      35.65      35.61       34.13       34.46   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10       0.91      35.72      35.51       35.06       34.96   \n",
      "\n",
      "      T_RC_2 T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... T_FHC_Max_2_Max  \\\n",
      "0       35.6       35.6      35.45       35.6  ...           36.71   \n",
      "1      36.22      36.22      35.63      36.24  ...           36.71   \n",
      "2      37.71      37.56      37.71      37.75  ...           37.32   \n",
      "3      35.52      35.52      35.29      35.54  ...           37.32   \n",
      "4      34.71      34.71      34.51      34.72  ...           37.32   \n",
      "5      35.86      35.78      35.86      35.89  ...           36.71   \n",
      "6      35.73      35.71      35.73      35.76  ...           37.32   \n",
      "7      35.63       35.6      35.62      35.65  ...           36.71   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...           37.32   \n",
      "9       35.7       35.7      35.44      35.72  ...           36.71   \n",
      "\n",
      "  T_FHC_Max_2_Median T_FHCC_2_Mean_x T_FHCC_2_Min_x T_FHCC_2_Max_x  \\\n",
      "0             35.255       34.673215          32.74          36.55   \n",
      "1             35.255       34.673215          32.74          36.55   \n",
      "2             35.225       34.750468          32.56          36.92   \n",
      "3             35.225       34.750468          32.56          36.92   \n",
      "4             35.225       34.750468          32.56          36.92   \n",
      "5             35.255       34.673215          32.74          36.55   \n",
      "6             35.225       34.750468          32.56          36.92   \n",
      "7             35.255       34.673215          32.74          36.55   \n",
      "8             35.225       34.750468          32.56          36.92   \n",
      "9             35.255       34.673215          32.74          36.55   \n",
      "\n",
      "  T_FHCC_2_Median_x T_FHCC_2_Mean_y T_FHCC_2_Min_y T_FHCC_2_Max_y  \\\n",
      "0            34.690       34.666265          32.56          36.62   \n",
      "1            34.690       34.763845          33.05          36.92   \n",
      "2            34.775       34.666265          32.56          36.62   \n",
      "3            34.775       34.666265          32.56          36.62   \n",
      "4            34.775       34.763845          33.05          36.92   \n",
      "5            34.690       34.763845          33.05          36.92   \n",
      "6            34.775       34.676316          33.95          35.42   \n",
      "7            34.690       34.763845          33.05          36.92   \n",
      "8            34.775       34.763845          33.05          36.92   \n",
      "9            34.690       34.763845          33.05          36.92   \n",
      "\n",
      "  T_FHCC_2_Median_y  \n",
      "0             34.66  \n",
      "1             34.77  \n",
      "2             34.66  \n",
      "3             34.66  \n",
      "4             34.77  \n",
      "5             34.77  \n",
      "6             34.76  \n",
      "7             34.77  \n",
      "8             34.77  \n",
      "9             34.77  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "   SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  180208-10       1.14     35.55      35.3       35.27       35.01  35.54   \n",
      "1  180209-01       0.64     35.88     36.23       35.35       35.64  36.22   \n",
      "2  180209-02       0.79      37.6     36.97       37.22       36.25  37.64   \n",
      "3  180209-03        0.8     35.63     35.75       35.09        35.2  35.61   \n",
      "4  180209-05        0.9     35.08     34.78       34.22       34.32  35.01   \n",
      "5  180209-06       0.83     35.67     35.63       34.92       35.16  35.92   \n",
      "6  180209-07       0.94     35.54     35.98       35.05       35.65  35.68   \n",
      "7  180209-08       0.92     35.61     35.68       34.23       34.64  35.66   \n",
      "8  180209-09       0.91     35.46      35.7       34.44       34.74   35.5   \n",
      "9  180209-10       0.98     35.78     35.61       35.35       35.04  35.76   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... T_FHC_Max_3_Max T_FHC_Max_3_Median  \\\n",
      "0      35.54      35.33      35.55  ...           36.72           35.23659   \n",
      "1      36.22      35.29      36.23  ...           36.72           35.23659   \n",
      "2      37.57      37.64       37.7  ...           37.78           35.23318   \n",
      "3      35.61      35.42      35.63  ...           37.78           35.23318   \n",
      "4      35.01      34.57      35.08  ...           37.78           35.23318   \n",
      "5      35.69      35.92      35.97  ...           36.72           35.23659   \n",
      "6      35.53      35.68      35.71  ...           37.78           35.23318   \n",
      "7       35.6      35.66      35.69  ...           36.72           35.23659   \n",
      "8      35.41       35.5      35.53  ...           37.78           35.23318   \n",
      "9      35.76      35.37      35.78  ...           36.72           35.23659   \n",
      "\n",
      "  T_FHCC_3_Mean_x T_FHCC_3_Min_x T_FHCC_3_Max_x T_FHCC_3_Median_x  \\\n",
      "0       34.721140          33.12          36.49             34.71   \n",
      "1       34.721140          33.12          36.49             34.71   \n",
      "2       34.829161          32.69          37.18             34.82   \n",
      "3       34.829161          32.69          37.18             34.82   \n",
      "4       34.829161          32.69          37.18             34.82   \n",
      "5       34.721140          33.12          36.49             34.71   \n",
      "6       34.829161          32.69          37.18             34.82   \n",
      "7       34.721140          33.12          36.49             34.71   \n",
      "8       34.829161          32.69          37.18             34.82   \n",
      "9       34.721140          33.12          36.49             34.71   \n",
      "\n",
      "  T_FHCC_3_Mean_y T_FHCC_3_Min_y T_FHCC_3_Max_y T_FHCC_3_Median_y  \n",
      "0       34.718651          32.69          36.83              34.7  \n",
      "1       34.818774          33.21          37.18             34.81  \n",
      "2       34.718651          32.69          36.83              34.7  \n",
      "3       34.718651          32.69          36.83              34.7  \n",
      "4       34.818774          33.21          37.18             34.81  \n",
      "5       34.818774          33.21          37.18             34.81  \n",
      "6       34.756316          33.88          35.59             34.85  \n",
      "7       34.818774          33.21          37.18             34.81  \n",
      "8       34.818774          33.21          37.18             34.81  \n",
      "9       34.818774          33.21          37.18             34.81  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "   SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  180208-10       1.08     35.63     35.34       35.37       34.98  35.61   \n",
      "1  180209-01       0.58     36.12     36.27       35.51       35.89  36.12   \n",
      "2  180209-02       0.91     37.63     37.18       37.15       36.62  37.72   \n",
      "3  180209-03       0.84     35.57      35.6       34.39        34.8  35.57   \n",
      "4  180209-05       0.89     34.99     34.71       34.24       34.31  34.94   \n",
      "5  180209-06       0.77     35.77     35.68       34.91       35.09  35.99   \n",
      "6  180209-07       0.91     35.49     35.99       34.88       35.42  35.68   \n",
      "7  180209-08       0.97     35.89     35.57       35.33       33.96  35.86   \n",
      "8  180209-09       0.89     35.44     35.72       34.11       34.76  35.49   \n",
      "9  180209-10       0.95     35.72     35.62       35.24       35.26  35.71   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... T_FHC_Max_4_Max T_FHC_Max_4_Median  \\\n",
      "0      35.61      35.36      35.63  ...           36.74           35.21500   \n",
      "1      36.12      35.76      36.12  ...           36.74           35.21500   \n",
      "2      37.54      37.72      37.78  ...           37.85           35.23659   \n",
      "3      35.54      35.55      35.62  ...           37.85           35.23659   \n",
      "4      34.94      34.45      34.99  ...           37.85           35.23659   \n",
      "5      35.76      35.99      36.02  ...           36.74           35.21500   \n",
      "6      35.44      35.68      35.71  ...           37.85           35.23659   \n",
      "7      35.86      35.43      35.89  ...           36.74           35.21500   \n",
      "8      35.44      35.46      35.53  ...           37.85           35.23659   \n",
      "9      35.71      35.46      35.72  ...           36.74           35.21500   \n",
      "\n",
      "  T_FHCC_4_Mean_x T_FHCC_4_Min_x T_FHCC_4_Max_x T_FHCC_4_Median_x  \\\n",
      "0       34.690935          33.04          36.45         34.697992   \n",
      "1       34.690935          33.04          36.45         34.697992   \n",
      "2       34.822855          32.93          37.22         34.800000   \n",
      "3       34.822855          32.93          37.22         34.800000   \n",
      "4       34.822855          32.93          37.22         34.800000   \n",
      "5       34.690935          33.04          36.45         34.697992   \n",
      "6       34.822855          32.93          37.22         34.800000   \n",
      "7       34.690935          33.04          36.45         34.697992   \n",
      "8       34.822855          32.93          37.22         34.800000   \n",
      "9       34.690935          33.04          36.45         34.697992   \n",
      "\n",
      "  T_FHCC_4_Mean_y T_FHCC_4_Min_y T_FHCC_4_Max_y T_FHCC_4_Median_y  \n",
      "0       34.705373          32.93           36.9             34.69  \n",
      "1       34.796571          33.29          37.22            34.775  \n",
      "2       34.705373          32.93           36.9             34.69  \n",
      "3       34.705373          32.93           36.9             34.69  \n",
      "4       34.796571          33.29          37.22            34.775  \n",
      "5       34.796571          33.29          37.22            34.775  \n",
      "6       34.735789          34.01          35.55             34.66  \n",
      "7       34.796571          33.29          37.22            34.775  \n",
      "8       34.796571          33.29          37.22            34.775  \n",
      "9       34.796571          33.29          37.22            34.775  \n",
      "\n",
      "[10 rows x 55 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max     median\n",
      "0                  0  35.926562  34.47  38.28  35.886557\n",
      "1                  1   35.76125  34.45  37.83     35.685\n",
      "2                  2    35.6857  34.42  37.73      35.59\n",
      "3                  3  35.672796  35.12  36.16      35.72\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max     median\n",
      "0                  0  35.978549  34.76   38.4  35.893115\n",
      "1                  1   35.85775  34.59  37.92     35.735\n",
      "2                  2  35.737229  34.56  37.75      35.61\n",
      "3                  3  35.758421  35.16  36.27      35.78\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0  36.019758  35.09  38.41  35.915\n",
      "1                  1  35.904406  34.52  37.89   35.83\n",
      "2                  2  35.763893  34.72  37.71   35.64\n",
      "3                  3  35.783158  35.21   36.3   35.79\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  36.038925  34.99  38.54  35.94\n",
      "1                  1  35.906828  34.58  37.83  35.79\n",
      "2                  2  35.791039  34.96  37.78  35.68\n",
      "3                  3  35.794737  35.08   36.3  35.82\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'canthiMax_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max    median\n",
      "0                  0  35.186623  32.98  37.58  35.23318\n",
      "1                  1   35.02425  33.29  36.41    35.065\n",
      "2                  2   35.13305  34.07   37.0     35.08\n",
      "3                  3  35.116483  34.61  35.83     35.08\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.304383   34.0  37.32  35.33\n",
      "1                  1    35.1875  33.86  36.54  35.19\n",
      "2                  2  35.219759  34.35  36.99  35.16\n",
      "3                  3  35.184211  34.66  35.94  35.18\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max    median\n",
      "0                  0  35.323688  33.84  37.78    35.305\n",
      "1                  1  35.237159  33.86  36.66  35.20159\n",
      "2                  2  35.239918  34.37  37.01     35.15\n",
      "3                  3  35.232105  34.62  35.98      35.2\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  35.278033  33.88  37.85  35.25\n",
      "1                  1   35.14383  33.91  36.35  35.04\n",
      "2                  2  35.228631  34.03  37.01  35.18\n",
      "3                  3  35.187368  34.72  35.84  35.17\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'T_FHC_Max_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Round 1:\n",
      "   Ethnicity_encoded       mean    min    max median\n",
      "0                  0  36.006061  34.97  38.28  35.94\n",
      "1                  1    35.9145  34.72  37.83  35.81\n",
      "2                  2  35.837886   35.0  37.73   35.7\n",
      "3                  3  35.741293  35.25  36.27  35.73\n",
      "\n",
      "\n",
      "Statistics for Round 2:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0  36.129995  35.14   38.4   36.07\n",
      "1                  1   36.05725  35.11  37.92  35.905\n",
      "2                  2  35.906024  34.89   37.9   35.81\n",
      "3                  3  35.863684  35.29  36.27   35.85\n",
      "\n",
      "\n",
      "Statistics for Round 3:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0   36.15026  35.25  38.41  36.085\n",
      "1                  1  36.084729  35.29   37.9   35.97\n",
      "2                  2  35.918489  35.08   38.0   35.81\n",
      "3                  3  35.881579  35.29  36.57   35.87\n",
      "\n",
      "\n",
      "Statistics for Round 4:\n",
      "   Ethnicity_encoded       mean    min    max  median\n",
      "0                  0  36.611986  35.43  38.58  36.585\n",
      "1                  1  36.692864   35.9  38.32   36.65\n",
      "2                  2  36.486134  35.17  38.72   36.42\n",
      "3                  3  36.432105  35.61  36.98   36.46\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for round_name, df in round_dfs1.items():\n",
    "    round_number = round_name.split()[-1]\n",
    "    temperature_column = f'T_Max_{round_number}'\n",
    "    \n",
    "    if 'Ethnicity_encoded' in df.columns and temperature_column in df.columns:\n",
    "        grouped_stats = df.groupby('Ethnicity_encoded')[temperature_column].agg(['mean', 'min', 'max', 'median']).reset_index()\n",
    "        print(f\"Statistics for {round_name}:\")\n",
    "        print(grouped_stats)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        rename_stats = {stat: f'{temperature_column}_{stat.capitalize()}' for stat in ['mean', 'min', 'max', 'median']}\n",
    "        grouped_stats = grouped_stats.rename(columns=rename_stats)\n",
    "        \n",
    "        for stat in ['mean', 'min', 'max', 'median']:\n",
    "            new_col_name = f'{temperature_column}_{stat.capitalize()}'\n",
    "            temp_df = grouped_stats[['Ethnicity_encoded', new_col_name]]\n",
    "            df = pd.merge(df, temp_df, on='Ethnicity_encoded', how='left')\n",
    "        \n",
    "        round_dfs1[round_name] = df\n",
    "    else:\n",
    "        print(f\"Columns 'Ethnicity_encoded' or '{temperature_column}' not found in {round_name}. Skipping this round.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 \n",
      "   SubjectID T_offset_1  Max1R13_1  Max1L13_1 aveAllR13_1 aveAllL13_1  \\\n",
      "0  180208-10        1.0      35.62      35.39       35.33       35.07   \n",
      "1  180209-01       0.44      36.21      36.27       35.47       35.86   \n",
      "2  180209-02       0.65      37.68      37.16       37.06       36.52   \n",
      "3  180209-03       0.91      35.28      35.69       34.89       34.81   \n",
      "4  180209-05       0.81      34.19      34.46       33.64       33.87   \n",
      "5  180209-06        0.8      35.98      35.86       35.18       35.19   \n",
      "6  180209-07       0.86      35.16      35.88       34.54       35.41   \n",
      "7  180209-08       0.78      35.48      35.59       33.88       34.47   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10        0.9      35.44      35.31       34.84       34.66   \n",
      "\n",
      "      T_RC_1 T_RC_Dry_1 T_RC_Wet_1 T_RC_Max_1  ... canthiMax_1_Max_y  \\\n",
      "0      35.59      35.59      35.44      35.62  ...             37.73   \n",
      "1      36.19      36.19      35.87      36.21  ...             38.28   \n",
      "2      37.71       37.6      37.69      37.73  ...             37.73   \n",
      "3      35.26      35.26      34.94      35.28  ...             37.73   \n",
      "4      34.33      34.33      33.98      34.37  ...             38.28   \n",
      "5      36.05      35.93      36.05      36.07  ...             38.28   \n",
      "6      35.47      35.17      35.47      35.47  ...             36.16   \n",
      "7      35.51      35.41      35.51      35.56  ...             38.28   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...             38.28   \n",
      "9      35.59      35.59      34.66      35.59  ...             38.28   \n",
      "\n",
      "  canthiMax_1_Median_y T_FHC_Max_1_Mean_y T_FHC_Max_1_Min_y T_FHC_Max_1_Max_y  \\\n",
      "0                35.59           35.13305             34.07              37.0   \n",
      "1            35.886557          35.186623             32.98             37.58   \n",
      "2                35.59           35.13305             34.07              37.0   \n",
      "3                35.59           35.13305             34.07              37.0   \n",
      "4            35.886557          35.186623             32.98             37.58   \n",
      "5            35.886557          35.186623             32.98             37.58   \n",
      "6                35.72          35.116483             34.61             35.83   \n",
      "7            35.886557          35.186623             32.98             37.58   \n",
      "8            35.886557          35.186623             32.98             37.58   \n",
      "9            35.886557          35.186623             32.98             37.58   \n",
      "\n",
      "  T_FHC_Max_1_Median_y T_Max_1_Mean T_Max_1_Min T_Max_1_Max T_Max_1_Median  \n",
      "0                35.08    35.837886        35.0       37.73           35.7  \n",
      "1             35.23318    36.006061       34.97       38.28          35.94  \n",
      "2                35.08    35.837886        35.0       37.73           35.7  \n",
      "3                35.08    35.837886        35.0       37.73           35.7  \n",
      "4             35.23318    36.006061       34.97       38.28          35.94  \n",
      "5             35.23318    36.006061       34.97       38.28          35.94  \n",
      "6                35.08    35.741293       35.25       36.27          35.73  \n",
      "7             35.23318    36.006061       34.97       38.28          35.94  \n",
      "8             35.23318    36.006061       34.97       38.28          35.94  \n",
      "9             35.23318    36.006061       34.97       38.28          35.94  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 2 \n",
      "   SubjectID T_offset_2  Max1R13_2  Max1L13_2 aveAllR13_2 aveAllL13_2  \\\n",
      "0  180208-10       1.05       35.6       35.3       35.25       34.95   \n",
      "1  180209-01       0.56      36.24      36.18       35.81       35.85   \n",
      "2  180209-02       0.72      37.57      37.21       37.22       36.73   \n",
      "3  180209-03       0.85      35.48      35.47       35.09       35.04   \n",
      "4  180209-05       0.91      34.68      34.69       34.31       34.24   \n",
      "5  180209-06       0.88      35.74      35.61       34.82       34.92   \n",
      "6  180209-07       0.95      35.72      36.04       35.35       35.27   \n",
      "7  180209-08       0.83      35.65      35.61       34.13       34.46   \n",
      "8  180209-09   1.022238  35.681506  35.715786   34.948953   35.095949   \n",
      "9  180209-10       0.91      35.72      35.51       35.06       34.96   \n",
      "\n",
      "      T_RC_2 T_RC_Dry_2 T_RC_Wet_2 T_RC_Max_2  ... canthiMax_2_Max_y  \\\n",
      "0       35.6       35.6      35.45       35.6  ...             37.75   \n",
      "1      36.22      36.22      35.63      36.24  ...              38.4   \n",
      "2      37.71      37.56      37.71      37.75  ...             37.75   \n",
      "3      35.52      35.52      35.29      35.54  ...             37.75   \n",
      "4      34.71      34.71      34.51      34.72  ...              38.4   \n",
      "5      35.86      35.78      35.86      35.89  ...              38.4   \n",
      "6      35.73      35.71      35.73      35.76  ...             36.27   \n",
      "7      35.63       35.6      35.62      35.65  ...              38.4   \n",
      "8  35.761582  35.672803   35.66968  35.791443  ...              38.4   \n",
      "9       35.7       35.7      35.44      35.72  ...              38.4   \n",
      "\n",
      "  canthiMax_2_Median_y T_FHC_Max_2_Mean_y T_FHC_Max_2_Min_y T_FHC_Max_2_Max_y  \\\n",
      "0                35.61          35.219759             34.35             36.99   \n",
      "1            35.893115          35.304383              34.0             37.32   \n",
      "2                35.61          35.219759             34.35             36.99   \n",
      "3                35.61          35.219759             34.35             36.99   \n",
      "4            35.893115          35.304383              34.0             37.32   \n",
      "5            35.893115          35.304383              34.0             37.32   \n",
      "6                35.78          35.184211             34.66             35.94   \n",
      "7            35.893115          35.304383              34.0             37.32   \n",
      "8            35.893115          35.304383              34.0             37.32   \n",
      "9            35.893115          35.304383              34.0             37.32   \n",
      "\n",
      "  T_FHC_Max_2_Median_y T_Max_2_Mean T_Max_2_Min T_Max_2_Max T_Max_2_Median  \n",
      "0                35.16    35.906024       34.89        37.9          35.81  \n",
      "1                35.33    36.129995       35.14        38.4          36.07  \n",
      "2                35.16    35.906024       34.89        37.9          35.81  \n",
      "3                35.16    35.906024       34.89        37.9          35.81  \n",
      "4                35.33    36.129995       35.14        38.4          36.07  \n",
      "5                35.33    36.129995       35.14        38.4          36.07  \n",
      "6                35.18    35.863684       35.29       36.27          35.85  \n",
      "7                35.33    36.129995       35.14        38.4          36.07  \n",
      "8                35.33    36.129995       35.14        38.4          36.07  \n",
      "9                35.33    36.129995       35.14        38.4          36.07  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 3 \n",
      "   SubjectID T_offset_3 Max1R13_3 Max1L13_3 aveAllR13_3 aveAllL13_3 T_RC_3  \\\n",
      "0  180208-10       1.14     35.55      35.3       35.27       35.01  35.54   \n",
      "1  180209-01       0.64     35.88     36.23       35.35       35.64  36.22   \n",
      "2  180209-02       0.79      37.6     36.97       37.22       36.25  37.64   \n",
      "3  180209-03        0.8     35.63     35.75       35.09        35.2  35.61   \n",
      "4  180209-05        0.9     35.08     34.78       34.22       34.32  35.01   \n",
      "5  180209-06       0.83     35.67     35.63       34.92       35.16  35.92   \n",
      "6  180209-07       0.94     35.54     35.98       35.05       35.65  35.68   \n",
      "7  180209-08       0.92     35.61     35.68       34.23       34.64  35.66   \n",
      "8  180209-09       0.91     35.46      35.7       34.44       34.74   35.5   \n",
      "9  180209-10       0.98     35.78     35.61       35.35       35.04  35.76   \n",
      "\n",
      "  T_RC_Dry_3 T_RC_Wet_3 T_RC_Max_3  ... canthiMax_3_Max_y  \\\n",
      "0      35.54      35.33      35.55  ...             37.71   \n",
      "1      36.22      35.29      36.23  ...             38.41   \n",
      "2      37.57      37.64       37.7  ...             37.71   \n",
      "3      35.61      35.42      35.63  ...             37.71   \n",
      "4      35.01      34.57      35.08  ...             38.41   \n",
      "5      35.69      35.92      35.97  ...             38.41   \n",
      "6      35.53      35.68      35.71  ...              36.3   \n",
      "7       35.6      35.66      35.69  ...             38.41   \n",
      "8      35.41       35.5      35.53  ...             38.41   \n",
      "9      35.76      35.37      35.78  ...             38.41   \n",
      "\n",
      "  canthiMax_3_Median_y T_FHC_Max_3_Mean_y T_FHC_Max_3_Min_y T_FHC_Max_3_Max_y  \\\n",
      "0                35.64          35.239918             34.37             37.01   \n",
      "1               35.915          35.323688             33.84             37.78   \n",
      "2                35.64          35.239918             34.37             37.01   \n",
      "3                35.64          35.239918             34.37             37.01   \n",
      "4               35.915          35.323688             33.84             37.78   \n",
      "5               35.915          35.323688             33.84             37.78   \n",
      "6                35.79          35.232105             34.62             35.98   \n",
      "7               35.915          35.323688             33.84             37.78   \n",
      "8               35.915          35.323688             33.84             37.78   \n",
      "9               35.915          35.323688             33.84             37.78   \n",
      "\n",
      "  T_FHC_Max_3_Median_y T_Max_3_Mean T_Max_3_Min T_Max_3_Max T_Max_3_Median  \n",
      "0                35.15    35.918489       35.08        38.0          35.81  \n",
      "1               35.305     36.15026       35.25       38.41         36.085  \n",
      "2                35.15    35.918489       35.08        38.0          35.81  \n",
      "3                35.15    35.918489       35.08        38.0          35.81  \n",
      "4               35.305     36.15026       35.25       38.41         36.085  \n",
      "5               35.305     36.15026       35.25       38.41         36.085  \n",
      "6                 35.2    35.881579       35.29       36.57          35.87  \n",
      "7               35.305     36.15026       35.25       38.41         36.085  \n",
      "8               35.305     36.15026       35.25       38.41         36.085  \n",
      "9               35.305     36.15026       35.25       38.41         36.085  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n",
      "Round 4 \n",
      "   SubjectID T_offset_4 Max1R13_4 Max1L13_4 aveAllR13_4 aveAllL13_4 T_RC_4  \\\n",
      "0  180208-10       1.08     35.63     35.34       35.37       34.98  35.61   \n",
      "1  180209-01       0.58     36.12     36.27       35.51       35.89  36.12   \n",
      "2  180209-02       0.91     37.63     37.18       37.15       36.62  37.72   \n",
      "3  180209-03       0.84     35.57      35.6       34.39        34.8  35.57   \n",
      "4  180209-05       0.89     34.99     34.71       34.24       34.31  34.94   \n",
      "5  180209-06       0.77     35.77     35.68       34.91       35.09  35.99   \n",
      "6  180209-07       0.91     35.49     35.99       34.88       35.42  35.68   \n",
      "7  180209-08       0.97     35.89     35.57       35.33       33.96  35.86   \n",
      "8  180209-09       0.89     35.44     35.72       34.11       34.76  35.49   \n",
      "9  180209-10       0.95     35.72     35.62       35.24       35.26  35.71   \n",
      "\n",
      "  T_RC_Dry_4 T_RC_Wet_4 T_RC_Max_4  ... canthiMax_4_Max_y  \\\n",
      "0      35.61      35.36      35.63  ...             37.78   \n",
      "1      36.12      35.76      36.12  ...             38.54   \n",
      "2      37.54      37.72      37.78  ...             37.78   \n",
      "3      35.54      35.55      35.62  ...             37.78   \n",
      "4      34.94      34.45      34.99  ...             38.54   \n",
      "5      35.76      35.99      36.02  ...             38.54   \n",
      "6      35.44      35.68      35.71  ...              36.3   \n",
      "7      35.86      35.43      35.89  ...             38.54   \n",
      "8      35.44      35.46      35.53  ...             38.54   \n",
      "9      35.71      35.46      35.72  ...             38.54   \n",
      "\n",
      "  canthiMax_4_Median_y T_FHC_Max_4_Mean_y T_FHC_Max_4_Min_y T_FHC_Max_4_Max_y  \\\n",
      "0                35.68          35.228631             34.03             37.01   \n",
      "1                35.94          35.278033             33.88             37.85   \n",
      "2                35.68          35.228631             34.03             37.01   \n",
      "3                35.68          35.228631             34.03             37.01   \n",
      "4                35.94          35.278033             33.88             37.85   \n",
      "5                35.94          35.278033             33.88             37.85   \n",
      "6                35.82          35.187368             34.72             35.84   \n",
      "7                35.94          35.278033             33.88             37.85   \n",
      "8                35.94          35.278033             33.88             37.85   \n",
      "9                35.94          35.278033             33.88             37.85   \n",
      "\n",
      "  T_FHC_Max_4_Median_y T_Max_4_Mean T_Max_4_Min T_Max_4_Max T_Max_4_Median  \n",
      "0                35.18    36.486134       35.17       38.72          36.42  \n",
      "1                35.25    36.611986       35.43       38.58         36.585  \n",
      "2                35.18    36.486134       35.17       38.72          36.42  \n",
      "3                35.18    36.486134       35.17       38.72          36.42  \n",
      "4                35.25    36.611986       35.43       38.58         36.585  \n",
      "5                35.25    36.611986       35.43       38.58         36.585  \n",
      "6                35.17    36.432105       35.61       36.98          36.46  \n",
      "7                35.25    36.611986       35.43       38.58         36.585  \n",
      "8                35.25    36.611986       35.43       38.58         36.585  \n",
      "9                35.25    36.611986       35.43       38.58         36.585  \n",
      "\n",
      "[10 rows x 67 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round_name, df_round in round_dfs1.items():\n",
    "    print(f\"{round_name} \")\n",
    "    print(df_round.head(10)) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# output_column = 'aveOralM'\n",
    "\n",
    "# for round_name, df in round_dfs1.items():\n",
    "#     if output_column in df.columns:\n",
    "#         df[output_column] = pd.to_numeric(df[output_column], errors='coerce')\n",
    "\n",
    "#         correlation_matrix = df.corr(method='pearson', numeric_only=True)\n",
    "        \n",
    "#         if output_column in correlation_matrix.columns:\n",
    "#             abs_correlation_with_output = correlation_matrix[output_column].abs().drop(output_column)\n",
    "            \n",
    "#             sorted_features = abs_correlation_with_output.sort_values(ascending=False)\n",
    "            \n",
    "#             print(f\"Top correlated features with {output_column} in {round_name}:\")\n",
    "#             print(sorted_features.head(10))\n",
    "#             print(\"\\n\")\n",
    "#         else:\n",
    "#             print(f\"Could not find numeric output column '{output_column}' after conversion in {round_name}.\")\n",
    "#     else:\n",
    "#         print(f\"Output column '{output_column}' not found in {round_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import numpy as np\n",
    "\n",
    "# output_column = 'aveOralM'\n",
    "# D_prime = 10  \n",
    "# feature_performance = {}\n",
    "\n",
    "# for round_name, df in round_dfs1.items():\n",
    "#     df[output_column] = pd.to_numeric(df[output_column], errors='coerce')\n",
    "#     df = df.dropna(subset=[output_column])\n",
    "    \n",
    "#     features = [col for col in df.columns if col not in [output_column, 'SubjectID',]]  \n",
    "    \n",
    "#     for feature in features:\n",
    "#         # Reshape data for sklearn model\n",
    "#         X = df[[feature]].values.reshape(-1, 1)\n",
    "#         y = df[output_column].values\n",
    "        \n",
    "#         # Define and train the model\n",
    "#         model = LinearRegression()\n",
    "#         scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "#         # Use the mean score across folds as the performance metric\n",
    "#         feature_performance[feature] = np.mean(scores)\n",
    "    \n",
    "#     # Selecting the top D_prime features based on their performance\n",
    "#     top_features = sorted(feature_performance, key=feature_performance.get, reverse=True)[:D_prime]\n",
    "    \n",
    "#     print(f\"Top {D_prime} features based on model performance in {round_name}:\")\n",
    "#     for feature in top_features:\n",
    "#         print(f\"{feature}: {feature_performance[feature]}\")\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# output_column = 'aveOralM'\n",
    "\n",
    "# for round_name, df in round_dfs1.items():\n",
    "#     df[output_column] = pd.to_numeric(df[output_column], errors='coerce').dropna()\n",
    "    \n",
    "#     X = df.select_dtypes(include=[np.number]).drop(columns=[output_column])  \n",
    "#     y = df[output_column]\n",
    "    \n",
    "#     model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "    \n",
    "#     sfs = SequentialFeatureSelector(model,\n",
    "#                                     n_features_to_select=10,  \n",
    "#                                     direction='forward',\n",
    "#                                     scoring='neg_mean_squared_error',\n",
    "#                                     cv=5)\n",
    "    \n",
    "#     sfs.fit(X, y)\n",
    "#     selected_features = X.columns[sfs.get_support()]\n",
    "    \n",
    "#     print(f\"Round {round_name}: Selected Features:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# output_column = 'aveOralM'\n",
    "\n",
    "# # Updated round_dfs with only selected features\n",
    "# updated_round_dfs1 = {}\n",
    "\n",
    "# for round_name, df in round_dfs1.items():\n",
    "#     # Ensure output column is numeric and drop any rows with NaN in it\n",
    "#     df_clean = df.dropna(subset=[output_column])\n",
    "#     df_clean[output_column] = pd.to_numeric(df_clean[output_column], errors='coerce')\n",
    "    \n",
    "#     # Select features and target variable, exclude 'SubjectID' and non-numeric columns for the feature selection\n",
    "#     X = df_clean.select_dtypes(include=[np.number]).drop(columns=[output_column])\n",
    "#     y = df_clean[output_column]\n",
    "    \n",
    "#     # Standardize features and use Linear Regression as the model\n",
    "#     model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "    \n",
    "#     # Sequential Forward Selection\n",
    "#     sfs = SequentialFeatureSelector(model, \n",
    "#                                     n_features_to_select=10,  # Selecting top 10 features\n",
    "#                                     direction='forward',\n",
    "#                                     scoring='neg_mean_squared_error',\n",
    "#                                     cv=5)\n",
    "    \n",
    "#     # Fit SFS and get the selected features\n",
    "#     sfs.fit(X, y)\n",
    "#     selected_features = X.columns[sfs.get_support()].tolist()\n",
    "    \n",
    "#     # Update the DataFrame to only include selected features + 'SubjectID' + output column\n",
    "#     updated_df = df_clean[['SubjectID'] + selected_features + [output_column]]\n",
    "    \n",
    "#     # Update the dictionary with the new DataFrame\n",
    "#     updated_round_dfs1[round_name] = updated_df\n",
    "\n",
    "#     print(f\"Round {round_name}: Selected Features + 'SubjectID':\", ['SubjectID'] + selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round_name, df in updated_round_dfs1.items():\n",
    "#     print(f\"{round_name}:\")\n",
    "#     print(df.head())\n",
    "#     print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1:\n",
      "  Mean Output Value: 36.9994\n",
      "  Root Mean Squared Error (RMSE): 0.5582\n",
      "  Mean Squared Error (MSE): 0.3116\n",
      "  Mean Absolute Error (MAE): 0.3532\n",
      "\n",
      "\n",
      "Round 2:\n",
      "  Mean Output Value: 36.9994\n",
      "  Root Mean Squared Error (RMSE): 0.5582\n",
      "  Mean Squared Error (MSE): 0.3116\n",
      "  Mean Absolute Error (MAE): 0.3532\n",
      "\n",
      "\n",
      "Round 3:\n",
      "  Mean Output Value: 36.9994\n",
      "  Root Mean Squared Error (RMSE): 0.5582\n",
      "  Mean Squared Error (MSE): 0.3116\n",
      "  Mean Absolute Error (MAE): 0.3532\n",
      "\n",
      "\n",
      "Round 4:\n",
      "  Mean Output Value: 36.9994\n",
      "  Root Mean Squared Error (RMSE): 0.5582\n",
      "  Mean Squared Error (MSE): 0.3116\n",
      "  Mean Absolute Error (MAE): 0.3532\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "  Overall RMSE: 0.5582\n",
      "  Overall MSE: 0.3116\n",
      "  Overall MAE: 0.3532\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Store RMSE, MSE, MAE values for all rounds\n",
    "all_rmse = []\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "# Loop through each round\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    if output_column in train_df:\n",
    "        mean_output = train_df[output_column].mean()\n",
    "        \n",
    "        test_df = round_dfs1.get(round_name, None)\n",
    "        if test_df is not None and output_column in test_df:\n",
    "            # Make predictions for the test set\n",
    "            test_predictions = [mean_output] * len(test_df)\n",
    "            test_actuals = test_df[output_column]\n",
    "\n",
    "            # Calculate RMSE, MSE, and MAE\n",
    "            mse = mean_squared_error(test_actuals, test_predictions)\n",
    "            rmse = sqrt(mse)\n",
    "            mae = mean_absolute_error(test_actuals, test_predictions)\n",
    "\n",
    "            # Store in lists for later overall calculation\n",
    "            all_rmse.append(rmse)\n",
    "            all_mse.append(mse)\n",
    "            all_mae.append(mae)\n",
    "\n",
    "            # Display metrics for this round\n",
    "            print(f\"{round_name}:\")\n",
    "            print(f\"  Mean Output Value: {mean_output:.4f}\")\n",
    "            print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "            print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
    "            print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Calculate and display overall metrics\n",
    "overall_rmse = sqrt(sum(all_mse) / len(all_mse))\n",
    "overall_mse = sum(all_mse) / len(all_mse)\n",
    "overall_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Common Columns: ['Age', 'T_FHCC_1_Max_x', 'T_FHC_Max_1_Min_x', 'T_FHCC_1_Mean_x', 'canthiMax_1_Min_x', 'T_LC_1_Median', 'T_LC_1_Mean', 'T_FHCC_1_Min_x', 'T_LC_1_Max', 'T_FHC_Max_1_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.6333\n",
      "    MSE: 0.4010\n",
      "    MAE: 0.4668\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.7132\n",
      "    MSE: 0.5086\n",
      "    MAE: 0.5308\n",
      "Round Round 2: Common Columns: ['Age', 'T_FHCC_2_Min_x', 'canthiMax_2_Max_x', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'T_FHC_Max_2_Max_x', 'T_LC_2_Min', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.6131\n",
      "    MSE: 0.3758\n",
      "    MAE: 0.4224\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.7074\n",
      "    MSE: 0.5005\n",
      "    MAE: 0.5250\n",
      "Round Round 3: Common Columns: ['Age', 'T_FHC_Max_3_Median_x', 'canthiMax_3_Min_x', 'T_LC_3_Max', 'canthiMax_3_Median_x', 'T_FHCC_3_Median_x', 'T_LC_3_Min', 'canthiMax_3_Mean_x', 'T_FHCC_3_Mean_x', 'canthiMax_3_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.6333\n",
      "    MSE: 0.4010\n",
      "    MAE: 0.4668\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.8047\n",
      "    MSE: 0.6475\n",
      "    MAE: 0.6592\n",
      "Round Round 4: Common Columns: ['Age', 'T_LC_4_Min', 'canthiMax_4_Min_x', 'T_FHCC_4_Min_x', 'canthiMax_4_Mean_x', 'T_FHC_Max_4_Max_x', 'T_LC_4_Median', 'T_FHC_Max_4_Min_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.6131\n",
      "    MSE: 0.3758\n",
      "    MAE: 0.4224\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.7074\n",
      "    MSE: 0.5005\n",
      "    MAE: 0.5250\n",
      "\n",
      "Overall Training Metrics:\n",
      "  Overall RMSE: 0.6232\n",
      "  Overall MSE: 0.3884\n",
      "  Overall MAE: 0.4446\n",
      "\n",
      "Overall Testing Metrics:\n",
      "  Overall RMSE: 0.7343\n",
      "  Overall MSE: 0.5393\n",
      "  Overall MAE: 0.5600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize lists to store metrics for each round\n",
    "train_knn_rmse = []\n",
    "train_knn_mse = []\n",
    "train_knn_mae = []\n",
    "\n",
    "test_knn_rmse = []\n",
    "test_knn_mse = []\n",
    "test_knn_mae = []\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Iterate over each round to evaluate the KNN model with K=1 and display common columns\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    test_df = round_dfs1.get(round_name)  # Get the corresponding test set\n",
    "\n",
    "    if test_df is not None:\n",
    "        # Define common columns between train and test sets, excluding 'SubjectID' and the output column\n",
    "        common_columns = list(set(train_df.columns) & set(test_df.columns) - {'SubjectID', output_column})\n",
    "\n",
    "        # Ensure the same features are in both train and test datasets\n",
    "        X_train = train_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_train = pd.to_numeric(train_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        X_test = test_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_test = pd.to_numeric(test_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        # Define and fit KNN with K=1\n",
    "        knn = KNeighborsRegressor(n_neighbors=1)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the training set\n",
    "        y_train_pred = knn.predict(X_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = knn.predict(X_test)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "        # Calculate testing metrics\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Store the metrics for overall calculations\n",
    "        train_knn_rmse.append(train_rmse)\n",
    "        train_knn_mse.append(train_mse)\n",
    "        train_knn_mae.append(train_mae)\n",
    "\n",
    "        test_knn_rmse.append(test_rmse)\n",
    "        test_knn_mse.append(test_mse)\n",
    "        test_knn_mae.append(test_mae)\n",
    "\n",
    "        # Display common columns\n",
    "        print(f\"Round {round_name}: Common Columns:\", common_columns)\n",
    "\n",
    "        # Display metrics for each round\n",
    "        print(f\"  Training Metrics:\")\n",
    "        print(f\"    RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"    MSE: {train_mse:.4f}\")\n",
    "        print(f\"    MAE: {train_mae:.4f}\")\n",
    "\n",
    "        print(f\"  Testing Metrics:\")\n",
    "        print(f\"    RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"    MSE: {test_mse:.4f}\")\n",
    "        print(f\"    MAE: {test_mae:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Test dataset not found for {round_name}.\")\n",
    "\n",
    "# Overall metrics combining all rounds for training and testing\n",
    "overall_train_rmse = sqrt(sum(train_knn_mse) / len(train_knn_mse))\n",
    "overall_train_mse = sum(train_knn_mse) / len(train_knn_mse)\n",
    "overall_train_mae = sum(train_knn_mae) / len(train_knn_mae)\n",
    "\n",
    "overall_test_rmse = sqrt(sum(test_knn_mse) / len(test_knn_mse))\n",
    "overall_test_mse = sum(test_knn_mse) / len(test_knn_mse)\n",
    "overall_test_mae = sum(test_knn_mae) / len(test_knn_mae)\n",
    "\n",
    "print(\"\\nOverall Training Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_train_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_train_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Testing Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_test_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_test_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Common Columns: ['Age', 'T_FHCC_1_Max_x', 'T_FHC_Max_1_Min_x', 'T_FHCC_1_Mean_x', 'canthiMax_1_Min_x', 'T_LC_1_Median', 'T_LC_1_Mean', 'T_FHCC_1_Min_x', 'T_LC_1_Max', 'T_FHC_Max_1_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.5363\n",
      "    MSE: 0.2877\n",
      "    MAE: 0.4097\n",
      "  Testing Metrics:\n",
      "    RMSE: 11116615407241.0215\n",
      "    MSE: 123579138112508441690898432.0000\n",
      "    MAE: 10923138563662.5605\n",
      "Round Round 2: Common Columns: ['Age', 'T_FHCC_2_Min_x', 'canthiMax_2_Max_x', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'T_FHC_Max_2_Max_x', 'T_LC_2_Min', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4861\n",
      "    MSE: 0.2363\n",
      "    MAE: 0.3104\n",
      "  Testing Metrics:\n",
      "    RMSE: 1771046533236.9890\n",
      "    MSE: 3136605822890757079433216.0000\n",
      "    MAE: 1701230408984.8342\n",
      "Round Round 3: Common Columns: ['Age', 'T_FHC_Max_3_Median_x', 'canthiMax_3_Min_x', 'T_LC_3_Max', 'canthiMax_3_Median_x', 'T_FHCC_3_Median_x', 'T_LC_3_Min', 'canthiMax_3_Mean_x', 'T_FHCC_3_Mean_x', 'canthiMax_3_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4849\n",
      "    MSE: 0.2351\n",
      "    MAE: 0.3168\n",
      "  Testing Metrics:\n",
      "    RMSE: 803284772.2894\n",
      "    MSE: 645266425392046976.0000\n",
      "    MAE: 635013859.9029\n",
      "Round Round 4: Common Columns: ['Age', 'T_LC_4_Min', 'canthiMax_4_Min_x', 'T_FHCC_4_Min_x', 'canthiMax_4_Mean_x', 'T_FHC_Max_4_Max_x', 'T_LC_4_Median', 'T_FHC_Max_4_Min_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.5375\n",
      "    MSE: 0.2889\n",
      "    MAE: 0.4145\n",
      "  Testing Metrics:\n",
      "    RMSE: 12741737433811.8223\n",
      "    MSE: 162351872832201494882156544.0000\n",
      "    MAE: 12541008971204.0684\n",
      "\n",
      "Overall Linear Regression Training Metrics:\n",
      "  Overall RMSE: 0.5118\n",
      "  Overall MSE: 0.2620\n",
      "  Overall MAE: 0.3629\n",
      "\n",
      "Overall Linear Regression Testing Metrics:\n",
      "  Overall RMSE: 8500994315561.9609\n",
      "  Overall MSE: 72266904353216780268732416.0000\n",
      "  Overall MAE: 6291503239427.8418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize lists to store metrics for each round\n",
    "lr_train_rmse = []\n",
    "lr_train_mse = []\n",
    "lr_train_mae = []\n",
    "\n",
    "lr_test_rmse = []\n",
    "lr_test_mse = []\n",
    "lr_test_mae = []\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Ensure proper data cleaning and consistency between rounds\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    test_df = round_dfs1.get(round_name)  # Get the corresponding test set\n",
    "\n",
    "    if test_df is not None:\n",
    "        # Define common columns to ensure consistency\n",
    "        common_columns = list(set(train_df.columns) & set(test_df.columns) - {'SubjectID', output_column})\n",
    "\n",
    "        # Ensure the same features are in both train and test datasets\n",
    "        X_train = train_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_train = pd.to_numeric(train_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        X_test = test_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_test = pd.to_numeric(test_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        # Define and fit Linear Regression without regularization\n",
    "        lin_reg = LinearRegression()\n",
    "        lin_reg.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the training set\n",
    "        y_train_pred = lin_reg.predict(X_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = lin_reg.predict(X_test)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "        # Calculate testing metrics\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Store the metrics for overall calculations\n",
    "        lr_train_rmse.append(train_rmse)\n",
    "        lr_train_mse.append(train_mse)\n",
    "        lr_train_mae.append(train_mae)\n",
    "\n",
    "        lr_test_rmse.append(test_rmse)\n",
    "        lr_test_mse.append(test_mse)\n",
    "        lr_test_mae.append(test_mae)\n",
    "\n",
    "        # Display common columns\n",
    "        print(f\"Round {round_name}: Common Columns:\", common_columns)\n",
    "\n",
    "        # Display metrics for each round\n",
    "        print(f\"  Training Metrics:\")\n",
    "        print(f\"    RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"    MSE: {train_mse:.4f}\")\n",
    "        print(f\"    MAE: {train_mae:.4f}\")\n",
    "\n",
    "        print(f\"  Testing Metrics:\")\n",
    "        print(f\"    RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"    MSE: {test_mse:.4f}\")\n",
    "        print(f\"    MAE: {test_mae:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Test dataset not found for {round_name}.\")\n",
    "\n",
    "# Overall metrics combining all rounds for training and testing\n",
    "overall_train_rmse = sqrt(sum(lr_train_mse) / len(lr_train_mse))\n",
    "overall_train_mse = sum(lr_train_mse) / len(lr_train_mse)\n",
    "overall_train_mae = sum(lr_train_mae) / len(lr_train_mae)\n",
    "\n",
    "overall_test_rmse = sqrt(sum(lr_test_mse) / len(lr_test_mse))\n",
    "overall_test_mse = sum(lr_test_mse) / len(lr_test_mse)\n",
    "overall_test_mae = sum(lr_test_mae) / len(lr_test_mae)\n",
    "\n",
    "print(\"\\nOverall Linear Regression Training Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_train_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_train_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Linear Regression Testing Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_test_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_test_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Common Columns: ['Age', 'T_FHCC_1_Max_x', 'T_FHC_Max_1_Min_x', 'T_FHCC_1_Mean_x', 'canthiMax_1_Min_x', 'T_LC_1_Median', 'T_LC_1_Mean', 'T_FHCC_1_Min_x', 'T_LC_1_Max', 'T_FHC_Max_1_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4902\n",
      "    MSE: 0.2403\n",
      "    MAE: 0.3083\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.5771\n",
      "    MSE: 0.3330\n",
      "    MAE: 0.3567\n",
      "Round Round 2: Common Columns: ['Age', 'T_FHCC_2_Min_x', 'canthiMax_2_Max_x', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'T_FHC_Max_2_Max_x', 'T_LC_2_Min', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4891\n",
      "    MSE: 0.2392\n",
      "    MAE: 0.3080\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.5765\n",
      "    MSE: 0.3323\n",
      "    MAE: 0.3559\n",
      "Round Round 3: Common Columns: ['Age', 'T_FHC_Max_3_Median_x', 'canthiMax_3_Min_x', 'T_LC_3_Max', 'canthiMax_3_Median_x', 'T_FHCC_3_Median_x', 'T_LC_3_Min', 'canthiMax_3_Mean_x', 'T_FHCC_3_Mean_x', 'canthiMax_3_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4903\n",
      "    MSE: 0.2404\n",
      "    MAE: 0.3083\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.5733\n",
      "    MSE: 0.3287\n",
      "    MAE: 0.3534\n",
      "Round Round 4: Common Columns: ['Age', 'T_LC_4_Min', 'canthiMax_4_Min_x', 'T_FHCC_4_Min_x', 'canthiMax_4_Mean_x', 'T_FHC_Max_4_Max_x', 'T_LC_4_Median', 'T_FHC_Max_4_Min_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4891\n",
      "    MSE: 0.2392\n",
      "    MAE: 0.3080\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.5777\n",
      "    MSE: 0.3338\n",
      "    MAE: 0.3565\n",
      "\n",
      "Overall Support Vector Regression Training Metrics:\n",
      "  Overall RMSE: 0.4897\n",
      "  Overall MSE: 0.2398\n",
      "  Overall MAE: 0.3081\n",
      "\n",
      "Overall Support Vector Regression Testing Metrics:\n",
      "  Overall RMSE: 0.5762\n",
      "  Overall MSE: 0.3320\n",
      "  Overall MAE: 0.3556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize lists to store metrics for each round\n",
    "svr_train_rmse = []\n",
    "svr_train_mse = []\n",
    "svr_train_mae = []\n",
    "\n",
    "svr_test_rmse = []\n",
    "svr_test_mse = []\n",
    "svr_test_mae = []\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Ensure proper data cleaning and consistency between rounds\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    test_df = round_dfs1.get(round_name)  # Get the corresponding test set\n",
    "\n",
    "    if test_df is not None:\n",
    "        # Define common columns to ensure consistency\n",
    "        common_columns = list(set(train_df.columns) & set(test_df.columns) - {'SubjectID', output_column})\n",
    "\n",
    "        # Ensure the same features are in both train and test datasets\n",
    "        X_train = train_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_train = pd.to_numeric(train_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        X_test = test_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_test = pd.to_numeric(test_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        # Define and fit Support Vector Regression\n",
    "        svr = SVR(kernel='rbf')  # Using RBF kernel, but you can choose others\n",
    "        svr.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the training set\n",
    "        y_train_pred = svr.predict(X_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = svr.predict(X_test)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "        # Calculate testing metrics\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Store the metrics for overall calculations\n",
    "        svr_train_rmse.append(train_rmse)\n",
    "        svr_train_mse.append(train_mse)\n",
    "        svr_train_mae.append(train_mae)\n",
    "\n",
    "        svr_test_rmse.append(test_rmse)\n",
    "        svr_test_mse.append(test_mse)\n",
    "        svr_test_mae.append(test_mae)\n",
    "\n",
    "        # Display common columns\n",
    "        print(f\"Round {round_name}: Common Columns:\", common_columns)\n",
    "\n",
    "        # Display metrics for each round\n",
    "        print(f\"  Training Metrics:\")\n",
    "        print(f\"    RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"    MSE: {train_mse:.4f}\")\n",
    "        print(f\"    MAE: {train_mae:.4f}\")\n",
    "\n",
    "        print(f\"  Testing Metrics:\")\n",
    "        print(f\"    RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"    MSE: {test_mse:.4f}\")\n",
    "        print(f\"    MAE: {test_mae:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Test dataset not found for {round_name}.\")\n",
    "\n",
    "# Overall metrics combining all rounds for training and testing\n",
    "overall_train_rmse = sqrt(sum(svr_train_mse) / len(svr_train_mse))\n",
    "overall_train_mse = sum(svr_train_mse) / len(svr_train_mse)\n",
    "overall_train_mae = sum(svr_train_mae) / len(svr_train_mae)\n",
    "\n",
    "overall_test_rmse = sqrt(sum(svr_test_mse) / len(svr_test_mse))\n",
    "overall_test_mse = sum(svr_test_mse) / len(svr_test_mse)\n",
    "overall_test_mae = sum(svr_test_mae) / len(svr_test_mae)\n",
    "\n",
    "print(\"\\nOverall Support Vector Regression Training Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_train_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_train_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Support Vector Regression Testing Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_test_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_test_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Common Columns: ['Age', 'T_FHCC_1_Max_x', 'T_FHC_Max_1_Min_x', 'T_FHCC_1_Mean_x', 'canthiMax_1_Min_x', 'T_LC_1_Median', 'T_LC_1_Mean', 'T_FHCC_1_Min_x', 'T_LC_1_Max', 'T_FHC_Max_1_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4849\n",
      "    MSE: 0.2351\n",
      "    MAE: 0.3169\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.6618\n",
      "    MSE: 0.4380\n",
      "    MAE: 0.5469\n",
      "Round Round 2: Common Columns: ['Age', 'T_FHCC_2_Min_x', 'canthiMax_2_Max_x', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'T_FHC_Max_2_Max_x', 'T_LC_2_Min', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4849\n",
      "    MSE: 0.2351\n",
      "    MAE: 0.3169\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.5518\n",
      "    MSE: 0.3045\n",
      "    MAE: 0.4101\n",
      "Round Round 3: Common Columns: ['Age', 'T_FHC_Max_3_Median_x', 'canthiMax_3_Min_x', 'T_LC_3_Max', 'canthiMax_3_Median_x', 'T_FHCC_3_Median_x', 'T_LC_3_Min', 'canthiMax_3_Mean_x', 'T_FHCC_3_Mean_x', 'canthiMax_3_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4849\n",
      "    MSE: 0.2351\n",
      "    MAE: 0.3169\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.5526\n",
      "    MSE: 0.3054\n",
      "    MAE: 0.3812\n",
      "Round Round 4: Common Columns: ['Age', 'T_LC_4_Min', 'canthiMax_4_Min_x', 'T_FHCC_4_Min_x', 'canthiMax_4_Mean_x', 'T_FHC_Max_4_Max_x', 'T_LC_4_Median', 'T_FHC_Max_4_Min_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4849\n",
      "    MSE: 0.2351\n",
      "    MAE: 0.3169\n",
      "  Testing Metrics:\n",
      "    RMSE: 0.6030\n",
      "    MSE: 0.3636\n",
      "    MAE: 0.3697\n",
      "\n",
      "Overall Perceptron Training Metrics:\n",
      "  Overall RMSE: 0.4849\n",
      "  Overall MSE: 0.2351\n",
      "  Overall MAE: 0.3169\n",
      "\n",
      "Overall Perceptron Testing Metrics:\n",
      "  Overall RMSE: 0.5940\n",
      "  Overall MSE: 0.3529\n",
      "  Overall MAE: 0.4270\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Initialize lists to store metrics for each round\n",
    "perceptron_train_rmse = []\n",
    "perceptron_train_mse = []\n",
    "perceptron_train_mae = []\n",
    "\n",
    "perceptron_test_rmse = []\n",
    "perceptron_test_mse = []\n",
    "perceptron_test_mae = []\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Ensure proper data cleaning and consistency between rounds\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    test_df = round_dfs1.get(round_name)  # Get the corresponding test set\n",
    "\n",
    "    if test_df is not None:\n",
    "        # Define common columns to ensure consistency\n",
    "        common_columns = list(set(train_df.columns) & set(test_df.columns) - {'SubjectID', output_column})\n",
    "\n",
    "        # Ensure the same features are in both train and test datasets\n",
    "        X_train = train_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_train = pd.to_numeric(train_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        X_test = test_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_test = pd.to_numeric(test_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        # Define and fit a perceptron for regression\n",
    "        perceptron = MLPRegressor(hidden_layer_sizes=(1,), activation='identity', solver='lbfgs')\n",
    "        perceptron.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the training set\n",
    "        y_train_pred = perceptron.predict(X_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "        # Calculate testing metrics\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Store the metrics for overall calculations\n",
    "        perceptron_train_rmse.append(train_rmse)\n",
    "        perceptron_train_mse.append(train_mse)\n",
    "        perceptron_train_mae.append(train_mae)\n",
    "\n",
    "        perceptron_test_rmse.append(test_rmse)\n",
    "        perceptron_test_mse.append(test_mse)\n",
    "        perceptron_test_mae.append(test_mae)\n",
    "\n",
    "        # Display common columns\n",
    "        print(f\"Round {round_name}: Common Columns:\", common_columns)\n",
    "\n",
    "        # Display metrics for each round\n",
    "        print(f\"  Training Metrics:\")\n",
    "        print(f\"    RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"    MSE: {train_mse:.4f}\")\n",
    "        print(f\"    MAE: {train_mae:.4f}\")\n",
    "\n",
    "        print(f\"  Testing Metrics:\")\n",
    "        print(f\"    RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"    MSE: {test_mse:.4f}\")\n",
    "        print(f\"    MAE: {test_mae:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Test dataset not found for {round_name}.\")\n",
    "\n",
    "# Overall metrics combining all rounds for training and testing\n",
    "overall_train_rmse = sqrt(sum(perceptron_train_mse) / len(perceptron_train_mse))\n",
    "overall_train_mse = sum(perceptron_train_mse) / len(perceptron_train_mse)\n",
    "overall_train_mae = sum(perceptron_train_mae) / len(perceptron_train_mae)\n",
    "\n",
    "overall_test_rmse = sqrt(sum(perceptron_test_mse) / len(perceptron_test_mse))\n",
    "overall_test_mse = sum(perceptron_test_mse) / len(perceptron_test_mse)\n",
    "overall_test_mae = sum(perceptron_test_mae) / len(perceptron_test_mae)\n",
    "\n",
    "print(\"\\nOverall Perceptron Training Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_train_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_train_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Perceptron Testing Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_test_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_test_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1: Common Columns: ['Age', 'T_FHCC_1_Max_x', 'T_FHC_Max_1_Min_x', 'T_FHCC_1_Mean_x', 'canthiMax_1_Min_x', 'T_LC_1_Median', 'T_LC_1_Mean', 'T_FHCC_1_Min_x', 'T_LC_1_Max', 'T_FHC_Max_1_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4829\n",
      "    MSE: 0.2332\n",
      "    MAE: 0.3170\n",
      "  Testing Metrics:\n",
      "    RMSE: 2301937969872.3330\n",
      "    MSE: 5298918417139957093105664.0000\n",
      "    MAE: 2191950627668.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 2: Common Columns: ['Age', 'T_FHCC_2_Min_x', 'canthiMax_2_Max_x', 'T_LC_2_Max', 'canthiMax_2_Mean_x', 'canthiMax_2_Min_x', 'T_FHC_Max_2_Max_x', 'T_LC_2_Min', 'T_FHC_Max_2_Mean_x', 'T_FHC_Max_2_Median_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4829\n",
      "    MSE: 0.2332\n",
      "    MAE: 0.3151\n",
      "  Testing Metrics:\n",
      "    RMSE: 227112093425.2059\n",
      "    MSE: 51579902979979434000384.0000\n",
      "    MAE: 170470593342.5972\n",
      "Round Round 3: Common Columns: ['Age', 'T_FHC_Max_3_Median_x', 'canthiMax_3_Min_x', 'T_LC_3_Max', 'canthiMax_3_Median_x', 'T_FHCC_3_Median_x', 'T_LC_3_Min', 'canthiMax_3_Mean_x', 'T_FHCC_3_Mean_x', 'canthiMax_3_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4974\n",
      "    MSE: 0.2474\n",
      "    MAE: 0.3257\n",
      "  Testing Metrics:\n",
      "    RMSE: 9790572288201.7246\n",
      "    MSE: 95855305730503569555062784.0000\n",
      "    MAE: 9006345300612.9023\n",
      "Round Round 4: Common Columns: ['Age', 'T_LC_4_Min', 'canthiMax_4_Min_x', 'T_FHCC_4_Min_x', 'canthiMax_4_Mean_x', 'T_FHC_Max_4_Max_x', 'T_LC_4_Median', 'T_FHC_Max_4_Min_x', 'T_FHCC_4_Mean_x', 'T_FHCC_4_Max_x']\n",
      "  Training Metrics:\n",
      "    RMSE: 0.4828\n",
      "    MSE: 0.2331\n",
      "    MAE: 0.3156\n",
      "  Testing Metrics:\n",
      "    RMSE: 1801949957780.1438\n",
      "    MSE: 3247023650343861635514368.0000\n",
      "    MAE: 1302134365294.8303\n",
      "\n",
      "Overall Quadratic Polynomial Regression Training Metrics:\n",
      "  Overall RMSE: 0.4866\n",
      "  Overall MSE: 0.2367\n",
      "  Overall MAE: 0.3183\n",
      "\n",
      "Overall Quadratic Polynomial Regression Testing Metrics:\n",
      "  Overall RMSE: 5110108308562.7295\n",
      "  Overall MSE: 26113206925241842321588224.0000\n",
      "  Overall MAE: 3167725221729.7935\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "poly_train_rmse = []\n",
    "poly_train_mse = []\n",
    "poly_train_mae = []\n",
    "\n",
    "poly_test_rmse = []\n",
    "poly_test_mse = []\n",
    "poly_test_mae = []\n",
    "\n",
    "output_column = 'aveOralM'\n",
    "\n",
    "# Ensure proper data cleaning and consistency between rounds\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    test_df = round_dfs1.get(round_name)  # Get the corresponding test set\n",
    "\n",
    "    if test_df is not None:\n",
    "        # Define common columns to ensure consistency\n",
    "        common_columns = list(set(train_df.columns) & set(test_df.columns) - {'SubjectID', output_column})\n",
    "\n",
    "        # Ensure the same features are in both train and test datasets\n",
    "        X_train = train_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_train = pd.to_numeric(train_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        X_test = test_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        y_test = pd.to_numeric(test_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        # Apply quadratic polynomial transformation\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        X_train_poly = poly.fit_transform(X_train)\n",
    "        X_test_poly = poly.transform(X_test)\n",
    "\n",
    "        # Define and fit a linear regression model with the quadratic features\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_poly, y_train)\n",
    "\n",
    "        # Predict on the training set\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = sqrt(train_mse)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "        # Calculate testing metrics\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_rmse = sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "        # Store the metrics for overall calculations\n",
    "        poly_train_rmse.append(train_rmse)\n",
    "        poly_train_mse.append(train_mse)\n",
    "        poly_train_mae.append(train_mae)\n",
    "\n",
    "        poly_test_rmse.append(test_rmse)\n",
    "        poly_test_mse.append(test_mse)\n",
    "        poly_test_mae.append(test_mae)\n",
    "\n",
    "        # Display common columns and metrics for each round\n",
    "        print(f\"Round {round_name}: Common Columns:\", common_columns)\n",
    "        print(f\"  Training Metrics:\")\n",
    "        print(f\"    RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"    MSE: {train_mse:.4f}\")\n",
    "        print(f\"    MAE: {train_mae:.4f}\")\n",
    "\n",
    "        print(f\"  Testing Metrics:\")\n",
    "        print(f\"    RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"    MSE: {test_mse:.4f}\")\n",
    "        print(f\"    MAE: {test_mae:.4f}\")\n",
    "    else:\n",
    "        print(f\"Test dataset not found for {round_name}.\")\n",
    "\n",
    "# Overall metrics combining all rounds for training and testing\n",
    "overall_train_rmse = sqrt(sum(poly_train_mse) / len(poly_train_mse))\n",
    "overall_train_mse = sum(poly_train_mse) / len(poly_train_mse)\n",
    "overall_train_mae = sum(poly_train_mae) / len(poly_train_mae)\n",
    "\n",
    "overall_test_rmse = sqrt(sum(poly_test_mse) / len(poly_test_mse))\n",
    "overall_test_mse = sum(poly_test_mse) / len(poly_test_mse)\n",
    "overall_test_mae = sum(poly_test_mae) / len(poly_test_mae)\n",
    "\n",
    "print(\"\\nOverall Quadratic Polynomial Regression Training Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_train_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_train_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Quadratic Polynomial Regression Testing Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_test_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_test_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round Round 1:\n",
      "  RMSE: 58234703736.6017\n",
      "  MSE: 3391280719289771884544.0000\n",
      "  MAE: 58234703736.6017\n",
      "Round Round 2:\n",
      "  RMSE: 13030021806.2483\n",
      "  MSE: 169781468271305392128.0000\n",
      "  MAE: 13030021806.2483\n",
      "Round Round 3:\n",
      "  RMSE: 2500266450872.4165\n",
      "  MSE: 6251332325358150568902656.0000\n",
      "  MAE: 2500266450872.4165\n",
      "Round Round 4:\n",
      "  RMSE: 1019641141168.4969\n",
      "  MSE: 1039668056763394739404800.0000\n",
      "  MAE: 1019641141168.4969\n",
      "Overall Weighted Binning Metrics:\n",
      "  Overall RMSE: 1350422289906.8560\n",
      "  Overall MSE: 1823640361077276472770560.0000\n",
      "  Overall MAE: 897793079395.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "# Define function to apply weighted binning\n",
    "def weighted_binning(data, n_bins, weights):\n",
    "    # Apply discretization (binning) with specified weights\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    data_discretized = discretizer.fit_transform(data)\n",
    "    \n",
    "    # Apply the weights to the discretized data\n",
    "    weighted_data = data_discretized * weights\n",
    "    return weighted_data\n",
    "\n",
    "# Example dataset (assuming 'updated_round_dfs' contains training datasets and 'round_dfs1' contains test datasets)\n",
    "output_column = 'aveOralM'\n",
    "n_bins = 10  # Number of bins\n",
    "bin_weights = np.linspace(1, 100, n_bins)  # Example: linear weights from 1 to 2\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "weighted_rmse = []\n",
    "weighted_mse = []\n",
    "weighted_mae = []\n",
    "\n",
    "# Apply weighted binning and predict with linear regression\n",
    "for round_name, train_df in updated_round_dfs.items():\n",
    "    test_df = round_dfs1.get(round_name)\n",
    "    \n",
    "    if test_df is not None:\n",
    "        # Ensure common columns in both datasets\n",
    "        common_columns = list(set(train_df.columns) & set(test_df.columns) - {'SubjectID', output_column})\n",
    "\n",
    "        # Extract input features and target variable\n",
    "        X_train = train_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        X_test = test_df[common_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "        y_train = pd.to_numeric(train_df[output_column], errors='coerce').fillna(0)\n",
    "        y_test = pd.to_numeric(test_df[output_column], errors='coerce').fillna(0)\n",
    "\n",
    "        # Apply weighted binning to the training and test data\n",
    "        X_train_binned = weighted_binning(X_train, n_bins, bin_weights)\n",
    "        X_test_binned = weighted_binning(X_test, n_bins, bin_weights)\n",
    "\n",
    "        # Use linear regression to predict the target variable\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_binned, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test_binned)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Store the metrics\n",
    "        weighted_rmse.append(rmse)\n",
    "        weighted_mse.append(mse)\n",
    "        weighted_mae.append(mae)\n",
    "\n",
    "        # Display metrics for each round\n",
    "        print(f\"Round {round_name}:\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MSE: {mse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "    else:\n",
    "        print(f\"Test dataset not found for {round_name}.\")\n",
    "\n",
    "# Overall metrics combining all rounds\n",
    "overall_weighted_rmse = sqrt(sum(weighted_mse) / len(weighted_mse))\n",
    "overall_weighted_mse = sum(weighted_mse) / len(weighted_mse)\n",
    "overall_weighted_mae = sum(weighted_mae) / len(weighted_mae)\n",
    "\n",
    "print(\"Overall Weighted Binning Metrics:\")\n",
    "print(f\"  Overall RMSE: {overall_weighted_rmse:.4f}\")\n",
    "print(f\"  Overall MSE: {overall_weighted_mse:.4f}\")\n",
    "print(f\"  Overall MAE: {overall_weighted_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
